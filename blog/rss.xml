<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0">
<channel><title>Johan Renaudie - Blog</title><link>https://plannapus.github.io/blog/index.html</link><description>A little bit of science, a little bit of programming, a little bit of cooking...</description>
<item><title>Sarah's monograph in Zootaxa</title><link>http://plannapus.github.io/blog/2022-07-04.html</link><pubDate>2022-07-04</pubDate><description><div class="blog-content">
				Sarah's monograph on Late Neogene lophophaenids from her thesis is finally <a href="https://mapress.com/zt/article/view/zootaxa.5160.1.1">out</a>! A massive piece of work clocking at 158 pages (including no less than 42 plates).
				<p>In it, <span class="tooltip">we<span class="tooltiptext">but mostly she</span></span> not only described 58 new forms, named 23 of them and revised/commented on 32 previously-described species of those little buggers, we also named and described a new genus, <i>Pelagomanes</i> (the "ghost from the sea", to keep with Sarah's ghost-themed naming scheme for this paper).</p>
				<table class="img_current"><tr><td><img class="current bd" src="img/pelagomanes.png" width="200"/></td>
					<td><img class="current bd" src="img/kozoi.png" width="200"/></td>
					<td><img class="current bd" src="img/stigi.png" width="200"/></td>
					</tr></table><p class="caption">From left to right, <em>Pelagomanes cantharoides</em> (Sugiyama 1992), <em>P. kozoi</em> (Renaudie &amp;amp; Lazarus 2013) and <em>P. stigi</em> (Bjørklund 1976); three previously-described species for which we established this new genus.</p>
				<p>It will never cease to amaze me that this poorly-documented group is not only so diverse in most of the Neogene but also so abundant!</p>
				<h4>Reference:</h4>
				<p class="publications">Trubovitz S., Renaudie J., Lazarus D.B., Noble P. (2022) <a href="https://mapress.com/zt/article/view/zootaxa.5160.1.1">Late Neogene Lophophaenidae (Nassellaria, Radiolaria) from the Eastern Equatorial Pacific</a>. Zootaxa 5160(1), 1-158.</p>
			</div></description><category>Science</category><category>MS-related</category></item>
<item><title>Busy month</title><link>http://plannapus.github.io/blog/2022-05-06.html</link><pubDate>2022-05-06</pubDate><description><div class="blog-content">
				Last month ended up being an incredibly busy month for me, but for all the best reasons. The main one was the return of the work trips after two years of sedentarity. Although i stayed in Germany, it was still greatly appreciated.
				<p>First, I attended the <a href="https://www.paleosynthesis.nat.fau.de/index.php/biodeeptime/">BioDEEPtime workshop</a> in Erlangen, in the context of the <a href="https://www.paleosynthesis.nat.fau.de/">PaleoSynthesis center</a> series of workshops. The goal of that project: integrate ecological timeseries data from various organisms and a wide range of timespans (ranging from annual ecological studies to multi-million year paleontological timeseries) to identify the timescale of community turnover.</p>
				<img class="current" src="img/biodeeptime.jpeg" title="Attending members of BioDEEPtime" width="800"/><p class="caption">from the left to right: Huang Huai Hsuan, Erin Saupe, Amelia Penny, Lee Hsiang Liow, me, Adam Kocsis, Pincelli Hull, Manuel Steinbauer, Marina Costa Rillo, Jansen Smith, Mauro Sugawara, Adam Tomasovych. (Photo: Barbara Seuss)</p>
				<p>Second, as mentioned in <a href="2022-04-21.html">the previous posts</a>, I participated in the "Coding the Column" workshop on stratigraphic databases.</p>
				<p>And finally, I joined the <a href="https://iodp.tamu.edu/scienceops/expeditions/norwegian_continental_margin_magmatism.html">IODP Expedition 396</a> Sampling Party at the MARUM in Bremen, where I was invited as shorebased collaborator, to sample a bunch of Paleocene and Eocene samples to allow me to work on the local radiolarian biostrat!</p>
				<img class="current" src="img/exp396patch.png" title="expedition 396 patch" width="400"/><p class="caption">Patch of IODP Expedition 396.</p>
				<p>More infos on each of those projects will come soon!</p>
				<!--<pre><code class="R">
				</code></pre>-->
			</div></description><category>Science</category></item>
<item><title>Coding the Column: Using Databases to Synthesize Stratigraphy &amp; Geologic Age - Part 2</title><link>http://plannapus.github.io/blog/2022-04-21.html</link><pubDate>2022-04-21</pubDate><description><div class="blog-content">
			<p>Today was the second part of <a href="https://twitter.com/macromicropaleo/status/1485722962548252673">"Coding the Column: Using Databases to Synthesize Stratigraphy &amp;amp; Geologic Age" Symposium</a>, organized by Andy Fraass and Leah LeVay of project <a href="https://eodp.github.io/">eODP</a>. Here are the slides of the talk I gave on <a href="http://nsb.mfn-berlin.de">age modelling in Neptune</a>:</p>
<object class="blog" data="../data/20220421CodingTheColumn.pdf" width="600" height="425" type="application/pdf"><p><a href="../data/20220421CodingTheColumn.pdf">Download.</a></p></object>
			</div></description><category>Science</category></item>
<item><title>Coding the Column: Using Databases to Synthesize Stratigraphy &amp; Geologic Age</title><link>http://plannapus.github.io/blog/2022-02-11.html</link><pubDate>2022-02-11</pubDate><description><div class="blog-content">
			<p>Yesterday was the <a href="https://twitter.com/macromicropaleo/status/1485722962548252673">"Coding the Column: Using Databases to Synthesize Stratigraphy &amp;amp; Geologic Age" Symposium</a>, organized by Andy Fraass and Leah LeVay of project <a href="https://eodp.github.io/">eODP</a>. Here are the slides of the talk I gave on <a href="http://nsb.mfn-berlin.de">Neptune</a>:</p>
<object class="blog" data="../data/20220210CodingTheColumn.pdf" width="600" height="425" type="application/pdf"><p><a href="../data/20220210CodingTheColumn.pdf">Download.</a></p></object>
			<p>Looking forward for the follow-up workshop in April!</p>
			</div></description><category>Science</category></item>
<item><title>Measuring areas in the Southern Ocean with R</title><link>http://plannapus.github.io/blog/2022-02-01.html</link><pubDate>2022-02-01</pubDate><description><div class="blog-content">Recently, for reasons I might explain in a later post, I needed to compute specific areas in the Southern Ocean. I thought I would write down here how to do so.
				<pre><code class="R">library(orsifronts)
library(rgdal)
library(geosphere)
library(rgeos)
library(maptools)
library(rnaturalearthdata) #ropenscilabs/rnaturalearthdata</code></pre>

<p>That's quite a few packages. <code class="inline">orsifronts</code> and <code class="inline">rnaturalearthdata</code> are two convenient packages that holds data from oceanic fronts and continent shapefiles respectively. <code class="inline">rgdal</code> and <code class="inline">maptools</code> are basic packages to deal with geographical shapefiles, while <code class="inline">geosphere</code> is the package that contain the function with which we will actually compute the areas.</p>

				<pre><code class="R">saf &amp;lt;- el(coordinates(parkfronts)[[2]])
pf &amp;lt;- el(coordinates(parkfronts)[[3]])

#Shapefile of the Polar Frontal Zone according to Park et al. 2019
pfz &amp;lt;- rbind(saf,pf[nrow(pf):1,])
pfz.sp &amp;lt;- SpatialPolygonsDataFrame( #Yes creating a SpatialPolygonsDataFrame from scratch is that cumbersome
  SpatialPolygons(
    list(Polygons(
      list(Polygon(pfz)),
      ID=1)),
    proj4string=CRS("+proj=longlat")),
  data=data.frame(n=1))</code></pre>

<p>Here we take the oceanic fronts as defined by <a href="https://doi.org/10.17882/59800">Park and Durant 2019</a>, specifically the Subantarctic front and the Polar fronts. In addition I make a Polygon out of both of them to define the polar frontal zone.</p>

<pre><code class="R">#Shapefile of Antarctica (with 50m accuracy)
antarctica &amp;lt;- countries50[countries50@data$NAME%in%"Antarctica",]

#Polygon of Southern Ocean defined as anything below Subantarctic Front (SAF)
SAF &amp;lt;- rbind(c(-180,-90),saf,c(180,-90)) #Basically a rectangle where the upper side is replaced by the SAF
saf.sp &amp;lt;- SpatialPolygonsDataFrame(
  SpatialPolygons(
    list(Polygons(
      list(Polygon(SAF)),
      ID=1)),
    proj4string=CRS("+proj=longlat")),
  data=data.frame(n=1))
SO_0 &amp;lt;- gDifference(saf.sp,antarctica) 
#as the name implies gDifference make a polygon that is computed 
#as the set difference between the two polygons given as arguments.

#Polygon of Southern Ocean defined as anything below Polar Front (PF)
PF &amp;lt;- rbind(c(-180,-90),pf,c(180,-90))
pf.sp &amp;lt;- SpatialPolygonsDataFrame(
  SpatialPolygons(
    list(Polygons(
      list(Polygon(PF)),
      ID=1)),
    proj4string=CRS("+proj=longlat")),
  data=data.frame(n=1))
SO_1 &amp;lt;- gDifference(pf.sp,antarctica)

#Polygon of Southern Ocean defined as anything below 60°S
SO_etopo1 &amp;lt;- rbind(c(-180,-90),
                   c(-180,-60),
                   c(180,-60),
                   c(180,-90))
SO_etopo1 &amp;lt;- SpatialPolygonsDataFrame(
  SpatialPolygons(
    list(Polygons(
      list(Polygon(SO_etopo1)),
      ID=1)),
    proj4string=CRS("+proj=longlat")),
  data=data.frame(n=1))
SO_2 &amp;lt;- gDifference(SO_etopo1,antarctica)</code></pre>

<p>We create a bunch of polygons corresponding to one or the other way of defining the Southern Ocean, by computing the set difference between the global area below a certain line and the Antarctic continent.</p>

<pre><code class="R">#Polygon of the South Atlantic part of the polar frontal zone
atl &amp;lt;- rbind(cbind(-68,seq(-90,90,1)),
             cbind(20,seq(90,-90,-1)))
ATL &amp;lt;- SpatialPolygonsDataFrame(
  SpatialPolygons(
    list(Polygons(
      list(Polygon(atl)),
      ID=1)),
    proj4string=CRS("+proj=longlat")),
  data=data.frame(n=1))
pfz.satl &amp;lt;- gIntersection(pfz.sp,ATL)</code></pre>

<p>Here just for fun, and to go a bit further, I am making a polygon corresponding to the Atlantic sector of the Polar Frontal Zone by computing the intersection between the Atlantic ocean (crudly defined here) and the PFZ computed above.</p>

<pre><code class="R">areaPolygon(SO_0)/1000000
#[1] 46173015
areaPolygon(SO_1)/1000000
#[1] 35882983
areaPolygon(SO_2)/1000000
[#1] 22058184
areaPolygon(pfz.sp)/1000000
#[1] 10290123
areaPolygon(pfz.satl)/1000000
#[1] 2704735</code></pre>

<p>Computing the areas themselves is fairly easy using <code class="inline">geosphere</code>'s <code class="inline">areaPolygon</code>. The result of <code class="inline">areaPolygon</code> is given in meter squares, so we need to divide by 10^6 to get kilometer squares.</p>

<img class="current" src="img/mapArea.png" width="800"/><p class="caption">Map of various polygons computed here. In red, the 60°S parallel; in blue the Polar Frontal Zone (in dark blue, its Atlantic sector).</p>

			</div></description><category>R</category><category>Programming</category></item>
<item><title>Stats in Strat: a first attempt</title><link>http://plannapus.github.io/blog/2021-10-08.html</link><pubDate>2021-10-08</pubDate><description><div class="blog-content">A year ago, on this <a href="2020-10-28.html">very blog</a>, I brought up to a crowd of absolutely no one the subject of error quantification in marine sediment stratigraphy. I would like today to address the same crowd to bring up a first attempt at characterizing the error in age models such as the ones we use in <span class="tooltip">the<span class="tooltiptext">MY</span></span> <a href="http://nsb-mfn-berlin.de">Neptune database</a>.

			<p>In Neptune, we use a qualitative statement, ranging from "VP" (=very poor) to "E" (=excellent), to give our opinion on the reliability of the age model on a given site. But of course, I am often asked what this correspond to, in term of quantitative age to which I do not usually have an answer, except that I expect that age error to be at most ca. 0.5Ma on most age models – i. e. from age quality "M" (medium) to "VG" (very good) – and up to 1Ma on the worst ones ("VP" and "P"). This number is really just from the top of my head, but somewhat based on experience bootstrapping our data: using age bins lower than 0.5 create hard-to-reproduce results, when filtering the worst age models for instance, but 0.5Ma seem to be fairly robust.</p>

			<p>One way I thought we could measure this is by measuring the scatter of the stratigraphic data used to create the age model around that line-of-correlation and derive a mean discrepancy and a mean standard error on that discrepancy (which would thus be our mean stratigraphic error). Given we keep track of all the needed information in Neptune (unfortunately only since 2014 but, with the help of some student helpers, we have been trying to find back and document how older age models and data were collected), it is not too complicated to do. The result is surprisingly close to what I thought, as "M"-marked age models have a mean stratigraphic error of 0.493Myr (!). Good, Very Good and Excellent age models have thankfully lower error estimated, while the poor age models have an higher one (0.686Myr currently). This way of measuring stratigraphic error however is not well adapted to "very poor" age models which usually are considered as such because they are very poorly constrained, i. e. have a very small number of underlying stratigraphic data, so naturally the discrepancy between those data and the line of correlation is smaller than it probably really is, as they have a disproportionate influence on where that line is drawn.</p>

			<p>An additional issue I have with our quality assessments is that they are made at the site level but an age model could be good in a given section and bad later on. In fact this is fairly common: Neogene sections are usually more tightly calibrated than early Paleogene ones. Also the particularities of Cretaceous magnetostratigraphy (namely the length of chron C34n) make tightly constrained age models very rare in that particular time interval. Hence the idea that maybe we should look at it in a more granular way. Here because of the way we do age models (a series of line segments <span class="tooltip">assuming<span class="tooltiptext">I made clear my opinion about this assumption in a <a href="2020-10-28.html">previous post</a> but for now we will have to work with that</span></span> constant sedimentation rates between each tiepoints), the minimum measuring range is the line segments the line of correlation is made of. Hence the code I wrote to compute age errors on a specific site, for a specific range of depths:</p>

				<pre><code class="R">AgeErrorBySect &amp;lt;- function(conn, hole){
  require(NSBcompanion)
  rev &amp;lt;- dbGetQuery(conn, sprintf("SELECT revision_no FROM neptune_age_model_history WHERE site_hole='%s' AND current_flag='Y';",hole))
  if(length(rev$revision_no)){
    am &amp;lt;- dbGetQuery(conn, sprintf("SELECT age_ma, depth_mbsf FROM neptune_age_model WHERE site_hole='%s' AND revision_no=%i;", hole,rev$revision_no))
    am &amp;lt;- am[order(am$depth_mbsf,am$age_ma),]
    hid &amp;lt;- dbGetQuery(conn, sprintf("SELECT hole_id FROM neptune_hole_summary WHERE site_hole='%s';", hole))
    hid &amp;lt;- hid[1,1]
    ev &amp;lt;- dbGetQuery(conn, sprintf("SELECT a.event_id, top_depth_mbsf, bottom_depth_mbsf, 
                                  calibration_scale, young_age_ma, old_age_ma 
                                  FROM neptune_event a, neptune_event_calibration as b 
                                  WHERE a.top_hole_id='%s' AND a.event_id=b.event_id;",
                                   hid))
    evmag &amp;lt;- dbGetQuery(conn, sprintf("SELECT a.event_id, top_depth_mbsf, bottom_depth_mbsf, 
                                  scale as calibration_scale, age_ma as young_age_ma, NULL as old_age_ma 
                                  FROM neptune_event a, neptune_gpts as b 
                                  WHERE a.top_hole_id='%s' AND a.event_id=b.event_id AND b.scale='Grad12';",
                                   hid))
    ev &amp;lt;- rbind(ev,evmag)
    bycal &amp;lt;- split(ev,ev$calibration_scale)
    if(!length(bycal)){
      return(NULL)
    }
    for(i in seq_along(bycal)){
      scale &amp;lt;- names(bycal)[i]
      bycal[[i]]$young_age_ma &amp;lt;- changeAgeScale(nsb,bycal[[i]]$young_age_ma,scale,'Grad12')
      bycal[[i]]$old_age_ma &amp;lt;- changeAgeScale(nsb,bycal[[i]]$old_age_ma,scale,'Grad12')
#The following two are the age of the datum projected on the LOC
      bycal[[i]]$calc_young_age_ma &amp;lt;- approx(am$depth_mbsf,am$age_ma,bycal[[i]]$top_depth_mbsf,ties="ordered")$y
      bycal[[i]]$calc_old_age_ma &amp;lt;- approx(am$depth_mbsf,am$age_ma,bycal[[i]]$bottom_depth_mbsf,ties="ordered")$y
    }
    new &amp;lt;- do.call(rbind,bycal)
    res &amp;lt;- rep(NA,nrow(new))
    new$calc_old_age_ma[is.na(new$calc_old_age_ma)] &amp;lt;- new$calc_young_age_ma[is.na(new$calc_old_age_ma)]
    new$calc_young_age_ma[is.na(new$calc_young_age_ma)] &amp;lt;- new$calc_old_age_ma[is.na(new$calc_young_age_ma)]
    new$young_age_ma[is.na(new$young_age_ma)] &amp;lt;- new$old_age_ma[is.na(new$young_age_ma)]
    new$old_age_ma[is.na(new$old_age_ma)] &amp;lt;- new$young_age_ma[is.na(new$old_age_ma)]
    for(i in seq_along(res)){
      a &amp;lt;- c() 
#This is a bit clunky, but I wanted to take into account the fact that strat data are not really a set of (depth,age) tuples 
# but have a depth range (the sampling resolution) and an age range (due to the calibration error).
#Here i therefore measure the smallest possible discrepancy between that data and the LOC.
      a[1] &amp;lt;- new$old_age_ma[i]-new$calc_old_age_ma[i]
      a[2] &amp;lt;- new$old_age_ma[i]-new$calc_young_age_ma[i]
      a[3] &amp;lt;- new$young_age_ma[i]-new$calc_young_age_ma[i]
      a[4] &amp;lt;- new$young_age_ma[i]-new$calc_old_age_ma[i]
      if(all(!is.na(a))){
        if(all(a&amp;gt;0)){
          res[i] &amp;lt;- min(a)
        }else if(all(a&amp;lt;0)){
          res[i] &amp;lt;- max(a)
        }else{
          res[i] &amp;lt;- 0
        }
      }else{res[i] &amp;lt;- NA}
    }
    am_sections &amp;lt;- embed(am$depth_mbsf,2)
    am_sections &amp;lt;- am_sections[am_sections[,1]!=am_sections[,2],,drop=FALSE]
    which_section &amp;lt;- sapply(new$top_depth_mbsf,function(x)which(am_sections[,1]&amp;gt;=x&amp;amp;am_sections[,2]&amp;lt;=x))
    which_section &amp;lt;- factor(which_section,levels=seq_len(nrow(am_sections)))
    am_sections &amp;lt;- as.data.frame(am_sections)
    colnames(am_sections) &amp;lt;- c("to","from")
    am_sections$mean &amp;lt;- sapply(split(res,which_section),mean,na.rm=T)
    am_sections$std &amp;lt;- sapply(split(res,which_section),sd,na.rm=T)
    am_sections$ste &amp;lt;- sapply(split(res,which_section),function(x)sd(x,na.rm=T)/sqrt(length(x)))
  }else{
    am_sections &amp;lt;- NULL
  }
  am_sections
}
</code></pre>

<p>Naturally here it's NSB-based but one could very easily adapt this to any other age models, granted the underlying data is known. Does it solve all our issues? Absolutely not. But it's a start I guess.</p>

			</div></description><category>Science</category><category>R</category><category>Programming</category></item>
<item><title>raupShiny</title><link>http://plannapus.github.io/blog/2021-09-01.html</link><pubDate>2021-09-01</pubDate><description><div class="blog-content">
				As a follow-up on the <a href="2021-08-11.html">previous post</a>, I finally got the time to write a <a href="https://github.com/plannapus/raupShiny/blob/master/raupShiny.R">simplified version</a> of the shiny app we made for the <a href="https://www.naturkundemuseum-magdeburg.de/ausstellungen/sonderausstellungen/biominerale/">exhibition on biomineralization at the Museum für Naturkunde in Magdeburg</a>, so that people can built upon it and make a better, more complex version, and so I am going to briefly explain it here.
<pre><code class="R">library(shiny)
library(shinyWidgets)
library(rgl)

make_elliptic_generating_shape &amp;lt;- function(D,S,res=100){
  #Let's define the original ray as 1
  a &amp;lt;- 1
  rc &amp;lt;- (D+1)*a/(1-D)
  t&amp;lt;-seq(0,2*pi,by=pi/res)
  b &amp;lt;- a/S
  circle_0 &amp;lt;- cbind(r=rc + a*cos(t), y= b*sin(t), phi=0)
  return(circle_0)
}

#Function to coil the shape around the axis
coiling &amp;lt;- function(RT,W,generating_shape, turns,steps,dir="dextral"){
  PHI &amp;lt;- seq(0,2*pi*turns,length=steps)
  far_end &amp;lt;- generating_shape[1,1]
  closest_end &amp;lt;- approx(generating_shape[generating_shape[,1]&amp;lt;far_end-1,2],
                        generating_shape[generating_shape[,1]&amp;lt;far_end-1,1],0)$y
  D &amp;lt;- closest_end/far_end
  rc &amp;lt;- (D+1)/(1-D)
  rho &amp;lt;- function(theta, W, r0) r0 * W^(theta/(2*pi))
  y &amp;lt;- function(y0,W,theta,rc,T) y0 * W^(theta/(2*pi)) + rc*T*(W^(theta/(2*pi))-1)
  circle &amp;lt;- apply(generating_shape,1,
                  function(x)lapply(PHI,
                                    function(theta)cbind(r=rho(theta, W, x['r']),
                                                         y=y(x['y'],W,theta, rc,RT),
                                                         phi=theta)))
  circle &amp;lt;- do.call(rbind,lapply(circle,function(x)do.call(rbind,x)))
  
  #To cartesian coordinates
  if(dir=="dextral"){
    XYZ &amp;lt;- list(X = circle[,1] * sin(circle[,3]),
                Y = circle[,1] * cos(circle[,3]),
                Z = circle[,2])
  }else{
    XYZ &amp;lt;- list(X = circle[,1] * cos(circle[,3]),
                Y = circle[,1] * sin(circle[,3]),
                Z = circle[,2])
  }
  XYZ
}

#Function to create the indices of the vertices of the mesh
pt2quad &amp;lt;- function(steps,res){
  pt_n&amp;lt;- rep(1:(res*2+1),each=steps)
  step &amp;lt;- 1:steps
  eg &amp;lt;- expand.grid(1:(2*res),1:(steps-1))
  apply(eg,1,function(x)c(which(step==x[2] &amp;amp; pt_n==x[1]),
                          which(step==x[2]+1 &amp;amp; pt_n==x[1]),
                          which(step==x[2]+1 &amp;amp; pt_n==x[1]+1),
                          which(step==x[2] &amp;amp; pt_n==x[1]+1)))
}</code></pre>

<p>OK that first part is the one I showed last time, with an additional function <code class="inline">pt2quad</code> which takes a dataframe of points (as XYZ coordinates) and return a dataframe of quadrilaterals that will define the 3D shape.</p>
<pre><code class="R">JS.logify &amp;lt;-"function logifySlider (sliderId) {
    // regular number style
    $('#'+sliderId).data('ionRangeSlider').update({
      'prettify': function (num) { return (Math.pow(10, num).toFixed(2)); }
    })
}"
JS.onload &amp;lt;-"$(document).ready(function() {
  setTimeout(function() {
    logifySlider('W')
  }, 5)})"

uirgl &amp;lt;- fluidPage(
  tags$head(tags$script(HTML(JS.logify))),
  tags$head(tags$script(HTML(JS.onload))),
  titlePanel("Raup Coiling model for Mollusks"),
  sidebarLayout(
    sidebarPanel(
      sliderInput(input="W",label="Whorl Expansion Rate (W)",
                  min = 0, max = 5, value = 0.301, step = .0001),
      sliderInput(input="D",label="Umbilicus opening (D)", 
                  min=0, max=0.9,value=0.3,step=0.01),
      sliderInput(input="S",label="Shape of opening (S)", 
                  min=0.1, max=5,value=1,step=0.01),
      sliderInput(input="RT",label="Rate of translation (T)",
                  min=0, max=35,value=2,step=0.1),
    ),
    mainPanel(
      rglwidgetOutput("coilrgl",width=800, height=800)
    )
  )
)</code></pre>

<p><code class="inline">shiny</code> makes it very easy! This is the whole code to create the actual HTML page the user is going to interact with. We just define our variables while creating sliders for variable selection. I already discussed the two javascript functions <a href="2021-05-27.html">before</a>.</p>

<pre><code class="R">serverrgl &amp;lt;- function(input,output){
  mqd &amp;lt;- reactive({ #Compute the shape
    # First we need to grab the variables picked by the user
    RT &amp;lt;- input$RT
    D &amp;lt;- input$D
    res &amp;lt;- 20 #Resolution of a single step is fixed here
    W &amp;lt;- round(10^input$W,2)
    S &amp;lt;- input$S
    # To make the app more functional we're actually computing the number of turns instead of
    # proposing it as a variable. 
    turns &amp;lt;- 5
    if(W&amp;lt;1.5 &amp;amp; RT&amp;gt;2) turns &amp;lt;- 10
    if(D&amp;gt;0.5) turns &amp;lt;- 10
    if(D&amp;lt;0.1 &amp;amp; RT==0) turns &amp;lt;- 2
    if(W&amp;gt;100) turns &amp;lt;- 1
    # We also made the number of steps a function of the number of turns:
    steps &amp;lt;- ifelse(turns&amp;gt;5,150,75)
    # Then we compute the generating shape
    circle &amp;lt;- make_elliptic_generating_shape(D,S,res)
    # We coil it:
    ce &amp;lt;- coiling(RT,W,circle,turns,steps,"dextral")
    # I am a bit ashamed of the next 3 lines, but 3D geometry is hard :)
    CE &amp;lt;- as.data.frame(ce)
    CE &amp;lt;- CE[c(2,3,1)]
    CE[,2] &amp;lt;- -1*CE[,2]
    # We compute the indices of the vertices
    qd &amp;lt;- pt2quad(steps,res)
    # And we make it a 3D mesh
    mesh3d(CE,quads=qd)
  })
  output$coilrgl &amp;lt;- renderRglwidget({ #Render it as 3D plot
    rgl.open(useNULL=TRUE)
    # Having the shape being computed outside this function allows the shape not to be recomputed all the time 
    # (useful in the actual app in which the users modify the angle of the plot as a variable, not so much here):
    m &amp;lt;- mqd()
    rgl.viewpoint(zoom = 1, theta=90, phi=30)
    bg3d(color="grey80")
    material3d(color="lightsalmon",emission="black",alpha=1,specular="white",ambient="black",textype="luminance")
    shade3d(m,override=TRUE) #Apart from this line that actual plot the shape, the rest is pure aesthetic
    light3d(theta=10,phi=10)
    rglwidget() #Launch the rgl widget
  })
}

#The last line finally launch the app:
shinyApp(ui = uirgl, server = serverrgl,options=list("launch.browser"=TRUE))</code></pre>

<p>And this is what happens "server-side". The retrival of the variables, the computation of the shape and its plotting, and that's it. Note the <code class="inline">output$coilrgl</code>: "coilrgl" is the name we gave in the previous code block to the placeholder for the rgl plot in the HTML page; and "output" is the name of the server's output... because we named it that way in the arguments of the function (see first line). Both <code class="inline">reactive</code> and <code class="inline">renderRglwidget</code> are "reactive" in that they will re-run every time they need to (i. e. when the user do something that would change their output).</p>

<img class="current" src="img/raupShiny.png" width="400"/><p class="caption">The result.</p>

			</div></description><category>Programming</category><category>R</category></item>
<item><title>Our Shiny app displaying Raup's Coiling model</title><link>http://plannapus.github.io/blog/2021-08-11.html</link><pubDate>2021-08-11</pubDate><description><div class="blog-content">
			As teased <a href="2021-05-27.html">previously</a>, I made a R shiny app for an <a href="https://www.naturkundemuseum-magdeburg.de/ausstellungen/sonderausstellungen/biominerale/">exhibition on biomineralization at the Museum für Naturkunde in Magdeburg</a>, together with a colleague of mine, <a href="https://www.researchgate.net/profile/David-Ware">David Ware</a>. The app allows users to discover <a href="https://www.jstor.org/stable/1301992">David Raup's mollusc coiling model</a>, a morphometric model able to basically recreate any mollusc form, using a limited number of parameters; and thus allows them to discover the full potential morphospace of molluscs.
			<img class="current" src="img/display.jpeg" width="400"/><p class="caption">The display in the exhibit.</p>
			<p>The way we made it, the app opens 2 windows: one displayed on a touchscreen, showing the controls; the other shows the actual result, i. e. the model. The control panel allows the user to specify the 4 parameters of the model (the whorl expansion rate, the umbilicus opening, the shape of the opening and the rate of translation), the orientation of the model (on a single window display the user would be able to rotate the model in any direction desired but since it is here shown in a separate window we simplified it, and the user can only rotate around the x axis) or select predefined forms (such as nautilus, ammonite, snail, mussel, etc).</p>
<table class="img_current"><tr><td><a href="img/controller.png"><img class="current bd" src="img/controller.png" width="400"/></a></td>
<td><a href="img/display.png"><img class="current bd" src="img/display.png" width="400"/></a></td>
</tr></table><p class="caption">Controller (left) and display (right).</p>
			<p>The code is available in <a href="http://github.com/plannapus/raupShiny">its repository</a>. Currently I only put the code for the app as displayed in the exhibit but I'll add soon a simplified, one-window version, in english, as a skeleton app so that people can build more complex apps from it. It was my first experience with shiny, and I'm frankly impressed at how simple and versatile it is.</p>
			<p>The part of the code creating the actual mollusc form is surprisingly simple (which I guess is the beauty of Raup's concept). The only difficulty (so to speak) is working in cylindrical coordinates instead of cartesian ones.</p>
				<pre><code class="R">make_elliptic_generating_shape &amp;lt;- function(D,S,res=100){
  #Let's define the original ray as 1
  a &amp;lt;- 1
  rc &amp;lt;- (D+1)*a/(1-D) #D is the ratio between the distance from the origin to the distal part of the shape and that from the origin to the proximal part.
  t&amp;lt;-seq(0,2*pi,by=pi/res)
  b &amp;lt;- a/S # Here S is the long axis v short axis ratio of the ellipsis (as we only modelled elliptical/circular opening shapes here)
  circle_0 &amp;lt;- cbind(r=rc + a*cos(t), y= b*sin(t), phi=0) #This is the equation for the shape in cylindrical coordinates.
  return(circle_0)
}

#Function to coil the shape around the axis
coiling &amp;lt;- function(RT,W,generating_shape, turns,steps,dir="dextral"){
  PHI &amp;lt;- seq(0,2*pi*turns,length=steps)
  far_end &amp;lt;- generating_shape[1,1]
  closest_end &amp;lt;- approx(generating_shape[generating_shape[,1]&amp;lt;far_end-1,2],
                        generating_shape[generating_shape[,1]&amp;lt;far_end-1,1],0)$y
  D &amp;lt;- closest_end/far_end
  rc &amp;lt;- (D+1)/(1-D)
  rho &amp;lt;- function(theta, W, r0) r0 * W^(theta/(2*pi)) #See Raup 1966 for the equations.
  y &amp;lt;- function(y0,W,theta,rc,T) y0 * W^(theta/(2*pi)) + rc*T*(W^(theta/(2*pi))-1)
  circle &amp;lt;- apply(generating_shape,1,
                  function(x)lapply(PHI,
                                    function(theta)cbind(r=rho(theta, W, x['r']),
                                                         y=y(x['y'],W,theta, rc,RT),
                                                         phi=theta)))
  circle &amp;lt;- do.call(rbind,lapply(circle,function(x)do.call(rbind,x)))
  
  #To cartesian coordinates
  if(dir=="dextral"){ #For a dextral shell
    XYZ &amp;lt;- list(X = circle[,1] * sin(circle[,3]),
                Y = circle[,1] * cos(circle[,3]),
                Z = circle[,2])
  }else{ #For a sinistral shell
    XYZ &amp;lt;- list(X = circle[,1] * cos(circle[,3]),
                Y = circle[,1] * sin(circle[,3]),
                Z = circle[,2])
  }
  XYZ
}
# [...]
# With D, S, RT and W defined by the user; 
# res and steps and the resolution of the opening shape and the resolution of the coiling
# And turns the number of turns desired (which can be also a parameters for the user to defined if wanted)
circle &amp;lt;- make_elliptic_generating_shape(D,S,res)
ce &amp;lt;- coiling(RT,W,circle,turns,steps,"dextral")
</code></pre>

<h4>References:</h4>
<p class="publications">Raup, D. (1966). <a href="http://www.jstor.org/stable/1301992">Geometric Analysis of Shell Coiling: General Problems</a>. Journal of Paleontology, 40(5), 1178-1190.</p>
			</div></description><category>Science</category><category>Programming</category><category>R</category></item>
<item><title>Connecting to PostgreSQL from R, Python and Julia</title><link>http://plannapus.github.io/blog/2021-07-01.html</link><pubDate>2021-07-01</pubDate><description><div class="blog-content">
				As part of my duties as database manager for the <a href="http://nsb.mfn-berlin.de">Neptune database</a>, I spend a lot of time writing pieces of code that interact with postgreSQL databases, either to access them and retrieve data, or to add or clean data in them. While database managing software nowadays are fairly practical and allow you to do a lot of things, in practice it is still a million time more convenient to connect from whatever programming language one is using at the moment. In particular it allows clever use of metaprogramming to form SQL queries that allows you to add thousands of rows of data with minimal effort.
				<p>Anyway I thought I would here write down some of the tools I use in R, python and julia to connect to and interact with such DBs. As I use R on an everyday basis I'll start with it. There are two basic paths in R to connect to DBs in general: DBI and OCDB. I use DBI as it is the first I discovered, but honestly OCDB is as good. In practice one will load the flavoured-DBI package that match the SQL-flavour of the DB one uses: RMySQL, RSQLite, RPostgreSQL, etc. All of them uses DBI in the background and all of them uses the same grammar. Which is:</p>
				<pre><code class="R">library(RPostgreSQL)
# First we connect to the database
driver &amp;lt;- dbDriver("PostgreSQL")
host &amp;lt;- "212.201.100.111"
database &amp;lt;- "nsb"
username &amp;lt;- "guest"
password &amp;lt;- "arm_aber_sexy"
con &amp;lt;- dbConnect(driver, host = host, port = "5432", dbname = database,
                user = username, password = password)
# Of course one can do that as a oneliner:
con &amp;lt;- dbConnect(dbDriver("PostgreSQL"), host = "212.201.100.111", port = "5432", dbname = "nsb",
                user = "guest", password = "arm_aber_sexy")
# Then we can look at which tables are present:
dbListTables(con)
# Read one of them fully:
nhs &amp;lt;- dbReadTable(con, "neptune_hole_summary")
# Or one can send a query:
dbSendQuery(con, "UPDATE whatever SET field=value WHERE other_field='VALUE';") #Dummy code as guest do not have writing permissions anyway
# Or get data:
dbGetQuery(con, "SELECT hole_id FROM neptune_hole_summary WHERE ocean='ANT';")
# And of course one can do batch operations:
sapply(sprintf("SELECT hole_id FROM neptune_hole_summary WHERE leg=%i;", 1:4), function(x) dbGetQuery(con, x))</code></pre>

<p>This is a very simple paradigm. Python uses a slightly different one though. In psycopg2 for instance one creates an engine, a connexion and a cursor:</p>
<pre><code class="python">import psycopg2 as pg

# First create an engine to connect to the db:
host = "212.201.100.111"
database = "nsb"
username = "guest"
password = "arm_aber_sexy"
engine = "host=%s dbname=%s port=5432 user=%s password=%s" % (host, database, username, password)

# Establish a connection
con = pg.connect(engine)

# Create a "cursor" that will send a query to the db
cur = con.cursor()

# Execute a query in SQL:
cur.execute("SELECT hole_id, sample_depth_mbsf FROM neptune_sample;")

# Fetch the results
cur.execute(query)
res = cur.fetchall()

# Then close the cursor (should be done right away)
cur.close()

# Here res is a list of unnamed tuples, i. e. in that case [("113_689B",23.456), ...]
# which is not always ideal.
# To get a list of dictionaries (i. e. [{'hole_id'; '113_689B', sample_depth_mbsf: 23.456}, ...])
# You need to do the following
cur = con.cursor(cursor_factory=ex.RealDictCursor)
query = "SELECT hole_id, sample_depth_mbsf FROM samples;"
cur.execute(query)
res = cur.fetchall()
cur.close()

# If you want to do an UPDATE or INSERT query, here is a way to do that:
input = [{'site': 511, 'geochemistry': 'not done yet'}, {'site': 703, 'geochemistry': 'not planned'}]
cur = con.cursor()
cur.executemany('UPDATE whatever SET geochemistry=%(geochemistry)s WHERE site=%(site)s;', input) #Dummy code here
con.commit() #Note that it is con.commit and not cur.commit!
cur.close()

#Eventually when everything is finished, close the connection:
con.close()</code></pre>

<p>An alternative is to use sqlalchemy as it is neatly connected to pands:</p>
<pre><code class="python">import pandas as pd
import pandas.io.sql as psql
from sqlalchemy import create_engine

#The process is the same.
#First create an engine to connect:
theEngine = "postgresql://%s:%s@%s/%s" % (username, password, host, database)
engine = create_engine(theEngine, client_encoding='utf8')
engine.connect()

query = "SELECT hole_id, sample_depth_mbsf FROM neptune_sample;"
res = psql.read_sql_query(query, engine)

#Here res is a pandas dataframe with columns named after the columns called from the table.
#I'm not as familiar with sqlalchemy but the way I do UPDATE and INSERT queries are as follow:

input = [{'site': 511, 'geochemistry': 'not done yet'}, {'site': 703, 'geochemistry': 'not planned'}]
with engine.begin() as con: #Here this syntax allow us to open a 'cursor' without having to explicitely close it.
    #Using dictionary comprehension and printf syntax:
    queries = ["UPDATE whatever WITH geochemistry='%s' WHERE site=%s;" % (k['geochemistry'], k['site']) for k in input] #Dummy code of course
    for q in queries: #As far as I know they need to be sent one by one.
        con.execute(q)

#And close the connection:
engine.dispose()</code></pre>

<p>Finally, in Julia. I am very new to Julia so do not expect a very good piece of code here, but I did write a function recently connecting to the DB to compute sample ages, so here we go:</p>

<pre><code class="julia">using LibPQ
using DataFrames
host = "212.201.100.111"
database = "nsb"
username = "guest"
password = "arm_aber_sexy"
con = LibPQ.Connection("dbname=$database host=$host port=5432 user=$username password=$password")
nhs = DataFrame(execute(con, "SELECT hole_id, sample_depth_mbsf FROM neptune_sample;"))
close(con)</code></pre>
			</div></description><category>Programming</category><category>R</category><category>Python</category><category>Julia</category><category>PostgreSQL</category></item>
<item><title>The Triton Database paper in Scientific Data</title><link>http://plannapus.github.io/blog/2021-06-28.html</link><pubDate>2021-06-28</pubDate><description><div class="blog-content">
				An offshoot of our <a href="http://nsb.mfn-berlin.de">Neptune Database</a>, the Triton Database, just got published today in <a href="http://doi.org/10.1038/s41597-021-00942-7"><em>Scientific Data</em></a>. The database focussed on plantonic foraminifera, and is very complete (more than half a million occurrences just for Cenozoic forams!): it not only includes data from NSB but also from the most recent deep sea drilling sites, land sections, and piston cores. It also contains a bunch of new age models for deep sea drilling sites, which were made semi-automatically using GAMs (kudos to the lead author, <a href="https://twitter.com/isabelfenton">Isabel Fenton</a>, for that brilliant idea!). The taxonomy has also been updated as it is based on a soon-to-be-published review of Cenozoic planktonic foram taxonomy by some of our co-authors.

				<img class="current" src="img/triton.png" width="600"/><p class="caption">Figure from the <a href="http://doi.org/10.1038/s41597-021-00942-7">MS</a> showing the data density in Triton (a) vs the one in Neptune (b). Planktonic foram biogeography in the Plio-Pleistocene is impressively well-resolved in Triton for instance.</p>

				<p>Uncharacteristically for a MS I coauthored, most of the work is in front of me as I will be making in the coming months a dedicated page on our website for people to query Triton. I will also start importing back its data into NSB, which will probably take a while as it means creating new structures to accomodate land sections and piston cores, in particular.</p>
				<p>Currently the database is accessible on <a href="https://springernature.figshare.com/collections/Triton_a_new_database_of_Cenozoic_Planktonic_Foraminifera/5242154">FigShare</a>, and I invite you all to explore it! I will probably post a second blogpost soonish with some examples and whatnots.</p>
				<p>Thanks again to Isabel and Adam for inviting us to collaborate with them on that paper!</p>
				<h4>Reference:</h4>
			<p class="publications">Fenton I., Woodhouse A., Aze T., Lazarus D., Renaudie J., Dunhill A., Young J., Saupe E. (2021). <a href="http://doi.org/10.1038/s41597-021-00942-7">Triton, a new species-level database of Cenozoic planktonic foraminiferal occurrences</a>. Scientific Data.</p>
			</div></description><category>Science</category><category>MS-related</category></item>
<item><title>The GPTS switch</title><link>http://plannapus.github.io/blog/2021-06-23.html</link><pubDate>2021-06-23</pubDate><description><div class="blog-content">
				I'm often asked some fairly technical questions on biostratigraphy or micropaleontology by our <a href="http://nsb.mfn.berlin.de">NSB</a> users. One that comes often is how to switch from one GPTS to another. I thought I would reproduce my answers here for reference.
				<p>But first a bit of explanations for people not used to working with geological ages. GPTS stands for geomagnetic polarity time scale. As mentioned in a previous <a href="2020-10-28.html">blogpost</a>, putting an actual numeric age on a sample is not trivial and in most cases we end up building ages models using stratigraphic data calibrated on different sites where they've been compared to the magnetostratigraphic record. That record itself is also something that is being calibrated (in the case of the C part of the scale, based on the Atlantic seafloor), and this calibrations are occasionally updated, most famously in the serie of book "The Geological Timescale" of which the <a href="https://www.sciencedirect.com/book/9780128243602/geologic-time-scale-2020">2020 edition</a> is the latest update.</p>
				<p>As biostratigraphic events calibrated on the GPTS are expressed linearly on it, switching from one scale to another is just a matter of linear interpolation (see here my answer on <a href="https://www.researchgate.net/post/Age_Interpolation_of_marker_microfossils_in_new_GPTS">ResearchGate</a>):</p>
				<blockquote cite="https://www.researchgate.net/post/Age_Interpolation_of_marker_microfossils_in_new_GPTS">With an example: in Agnini et al 2014, the LAD of <i>Discoaster lodoensis</i> was calibrated at 48.37Ma on the 1995 GPTS. It is thus between the bottom of C21n (47.906Ma) and the top of C22n (49.037Ma). On the 2020 GPTS, those chron boundaries are at 47.760 and 48.878Ma respectively. The mapped age of the LAD of Discoaster lodoensis on the 2020 GPTS is thus (by linear interpolation):
				47.760 + (48.37-47.906)*(47.906-47.760)/(49.037-48.878) = 48.18606 Ma</blockquote>
				<p>In practice however this is very tedious to do on a large amount of data. Hence this function I wrote some time ago in my package <a href="https://github.com/plannapus/NSBcompanion">NSBcompanion</a>, called <code class="inline">changeAgeScale</code>. It does require a user account to NSB (though in practice people can use the guest account mentioned in the help pages of the package).</p>
				<pre><code class="R">library(NSBcompanion)
nsb &amp;lt;- nsbConnect("guest","arm_aber_sexy") #Connect to the database in order to access the various GPTS data
old_ages &amp;lt;- c(0,1,2,3,4,5)
changeAgeScale(nsb, old_ages, from="CK95", to="GTS2020")</code></pre>
				<p>For the list of available GPTS and their abbreviations, see the <a href="https://nsb.mfn-berlin.de/help/">help page</a> of the NSB website.</p>
			</div></description><category>Science</category><category>R</category></item>
<item><title>Shiny app: logarithmic slider and updateSliderInput</title><link>http://plannapus.github.io/blog/2021-05-27.html</link><pubDate>2021-05-27</pubDate><description><div class="blog-content">
I am currently working on a R shiny app for an <a href="https://www.naturkundemuseum-magdeburg.de/ausstellungen/vorschau/sonderausstellung-biominerale/">exposition by a colleague from the Museum für Naturkunde in Magdeburg</a> (more on that later when it'll be finished). I stumbled onto a very specific issue that took me a while to solve so I thought I'll share my solution here, for future reference.
<p>The app allows the user to fiddle with the parameters of <a href="https://www.jstor.org/stable/1301992">Raup's mollusk model</a>, with the result being shown as a rgl 3D plot. One of the parameter (W, the Whorl Expansion Rate) needed to be represented as a logarithmic scale (as values range from 1 to 100 000, with most of the changes happening between 1 and 10). As this is not a native option in shiny, I had to use some javascript to sort it out, as per <a href="https://stackoverflow.com/a/39028280/1451109">this StackOverflow answer</a>:</p>
<pre><code class="R">library(shiny)
library(mwshiny)
JS.logify &amp;lt;-"function logifySlider (sliderId) {
    // regular number style
    $('#'+sliderId).data('ionRangeSlider').update({
      'prettify': function (num) { return (Math.pow(10, num).toFixed(2)); }
    })
}"
JS.onload &amp;lt;-"$(document).ready(function() {
  setTimeout(function() {
    logifySlider('W')
  }, 5)})"
ui_win &amp;lt;- list() #Here we are using mwshiny, as we need the controller and the plot on two different screens.
ui_win[["Controller"]] &amp;lt;- fluidPage(
	  tags$head(tags$script(HTML(JS.logify))),
	  tags$head(tags$script(HTML(JS.onload))),
	  sidebarLayout(
	    sidebarPanel(
	      sliderInput(input="W",
	                  label="Whorl Expansion Rate",
	                  min = 0, max = 5, value = 0.301, step = .0001),width=8)))
#[...]</code></pre>

<p>Overall it's a good solution and work as expected. However problems arise when using, server-side, the <code class="inline">updateSliderInput</code>. In this case, we have a series of preselected forms: when clicking on one of them, the values of that form replace the current values on the various sliders.</p>
<pre><code class="R">ui_win[["Controller"]] &amp;lt;- fluidPage(
  tags$head(tags$script(HTML(JS.logify))),
  tags$head(tags$script(HTML(JS.onload))),
  sidebarLayout(
    sidebarPanel(
      sliderInput(input="W",
                  label="Whorl Expansion Rate",
                  min = 0, max = 5, value = 0.301, step = .0001),
      width=8
    ),
    mainPanel(
      radioButtons(inputId="preselect", label="Preselection:",
                   choiceNames=list("None","Nautilus","Ammonite","Tower shell","Roman snail","Mussel","Tusk shell"),
                   choiceValues=list("nul","nau","amm","tow","rom","mus","tus")),
      width=4)
  )
)

serv_calc &amp;lt;- list()
presel &amp;lt;- data.frame(species=c("nau","amm","tow","rom","mus","tus"),
                     RT=c(0,0,12,2,0.2,0),
                     D=c(0,0.5,0,0,0,0.9),
                     W=c(3.2,1.9,1.2,2,10000,10000),
                     S=c(1.3,1,0.8,0.9,2,1),
                     turns=c(2,5,10,5,1,1))

serv_calc[[1]] &amp;lt;- function(input,session){
  observeEvent(input$preselect,{
    x &amp;lt;- input$preselect
    if(!is.null(x)){
      if(x!="nul"){
        ps &amp;lt;- presel[presel$species==x,]
        updateSliderInput(session, "W", value=round(log(ps$W,10),4))
        # [...]
      }
    }
  }
  )
}
# [...]</code></pre>
<p>And so, when it does, the slide becomes "de-logified" (i.e., ranging from 0 to 5 instead of 10^1 to 10^5). I knew from the start that the issue was that, while the JS script was run on page load, it was not rerun on update. The solution, which took me a while to find, was to use shinyjs's <code class="inline">runjs</code> together with the shiny server's <code class="inline">onFlushed</code>:</p>
<pre><code class="R">library(shiny)
library(mwshiny)
library(shinyjs)
JS.logify &amp;lt;-"function logifySlider (sliderId) {
    // regular number style
    $('#'+sliderId).data('ionRangeSlider').update({
      'prettify': function (num) { return (Math.pow(10, num).toFixed(2)); }
    })
}"
JS.onload &amp;lt;-"$(document).ready(function() {
  setTimeout(function() {
    logifySlider('W')
  }, 5)})"
ui_win &amp;lt;- list()
ui_win[["Controller"]] &amp;lt;- fluidPage(
  useShinyjs(), # &amp;lt;- Note that useShinyjs is needed to instantiate shinyJS before its use.
  tags$head(tags$script(HTML(JS.logify))),
  tags$head(tags$script(HTML(JS.onload))),
  sidebarLayout(
    sidebarPanel(
      sliderInput(input="W",
                  label="Whorl Expansion Rate",
                  min = 0, max = 5, value = 0.301, step = .0001),
      width=8
    ),
    mainPanel(
      radioButtons(inputId="preselect", label="Preselection:",
                   choiceNames=list("None","Nautilus","Ammonite","Tower shell","Roman snail","Mussel","Tusk shell"),
                   choiceValues=list("nul","nau","amm","tow","rom","mus","tus")),
      width=4)
  )
)

serv_calc &amp;lt;- list()
presel &amp;lt;- data.frame(species=c("nau","amm","tow","rom","mus","tus"),
                     RT=c(0,0,12,2,0.2,0),
                     D=c(0,0.5,0,0,0,0.9),
                     W=c(3.2,1.9,1.2,2,10000,10000),
                     S=c(1.3,1,0.8,0.9,2,1),
                     turns=c(2,5,10,5,1,1))

serv_calc[[1]] &amp;lt;- function(input,session){
  session$onFlushed(function()runjs("logifySlider('W');"),once=FALSE) # &amp;lt;- The key line
  observeEvent(input$preselect,{
    x &amp;lt;- input$preselect
    if(!is.null(x)){
      if(x!="nul"){
        ps &amp;lt;- presel[presel$species==x,]
        updateSliderInput(session, "W", value=round(log(ps$W,10),4))
        # [...]
      }
    }
  }
  )
}
# [...]</code></pre>
			</div></description><category>Programming</category><category>R</category></item>
<item><title>Micha and Momo's paper in Frontiers in Ecology &amp; Evolution</title><link>http://plannapus.github.io/blog/2021-05-07.html</link><pubDate>2021-05-07</pubDate><description><div class="blog-content">
Our paper, lead by Michael Buchwitz and Maren Jansen, came out today in <i><a href="http://doi.org/10.3389/fevo.2021.674779">Frontiers in Ecology &amp;amp; Evolution</a></i>, in the context of a special issue on the <a href="https://www.frontiersin.org/research-topics/14947/origin-and-early-evolution-of-amniotes#articles">Origin and Early Evolution of Amniotes</a>. This is a study I have been contributing to since 2018 (if I remember correctly), so I am glad to see it finally published. The idea of the paper was to use a fairly impressive database of measurements on early tetrapod trackways, and current hypotheses on track-trackmaker relationships, to test some of those hypotheses and characterize the mode of locomotion of the last common ancestors of amniotes. As it is quite outside of my usual expertise I am not here going to delve into the details of the study, given I was, after all, involved only as the number cruncher.
<p>I don't have a lot of opportunities to work with phylogenies in my own line of work, though my training in Paris was very heavily phylogenetically-oriented, so it was fun to finaly make use of it.</p>
<p>One particular thing I did for this paper was to try taking into account track polymorphism into the ancestral state reconstruction, given the data we used had, for some species, a large amount of specimens measured, and the distribution of the measurements showed quite a spread (ranging from juveniles to adult forms) and the occasional bimodal set-up. The idea I had to account for this was simply to bootstrap these data, meaning that instead of simply using the mean measurement, or median measurement, for each ichnospecies, I picked a single species randomly representing each species, and reiterated the operation a large amount of time. Here the code in R (full code for the entire study is available <a href="https://github.com/plannapus/tracks_asr/">here</a>):</p>
<pre><code class="r">#Bootstrap based on polymorphism
n_trials &amp;lt;- 1e4
n_char &amp;lt;- length(all_char) #all_char contains the list of characters for which we want to run the ASR
# This is fairly time consuming so to make things smoother I used doSNOW to parallelize it.
# The following 7 lines are only there to set it up.
library(doSNOW)
cl &amp;lt;- parallel::makeCluster(2)
registerDoSNOW(cl)
pb &amp;lt;- txtProgressBar(max = n_trials, style = 3)
progress &amp;lt;- function(n) setTxtProgressBar(pb, n)
opts &amp;lt;- list(progress = progress)
set.seed(20180822)
aas &amp;lt;- foreach(i = seq_len(n_trials), .options.snow = opts) %dopar% {
    R &amp;lt;- list() # will contain the results of the 1e4 x n_char x n_tree ASR.
    for(j in 1:n_char){
      R[[j]]&amp;lt;-list()
      # Pick a random specimen for each run and get the value for the specific measurement.
			# First get the list of specimens for which that particular character was measured:
      sp &amp;lt;- dat$specimens[!is.na(dat$specimens[,colnames(dat$specimens)==all_char[j]]),]
			# Then select randomly a single specimen for each ichnospecies:
      specimen_set &amp;lt;- sapply(split(sp$`No Specimen`,sp$`No Species`),function(x)ifelse(length(x)&amp;gt;1,sample(x,1),x))
			# Then pick its corresponding measurement:
      values &amp;lt;- sapply(specimen_set,function(x)sp[sp$`No Specimen`==x,colnames(sp)==all_char[j]])
			# We tried several phylogenetic hypotheses here
      for(k in 1:n_tree){
				 # Maximum likelihood ASR
        R[[j]][[k]] &amp;lt;- ape::reconstruct(values,dat$trees[[k]],method="ML")
      }
    }
    R
  }
close(pb)
stopCluster(cl)
</code></pre>
<p>It gives a nice idea of the incertitude linked to the set of specimens measured that is introduced by ASR.</p>
<img class="current" src="img/distrib1.png" width="400"/><p class="caption">An example of the distribution of the ancestral values for a given character for each node, when picking which specimen represent the species at random over 10 000 runs.</p>
<p>Anyway, the paper is naturally about way more than just this (in fact it is a very minor point of that paper) but I thought it was a cool solution to that particular issue.</p>
<h4>Reference:</h4>
<p class="publications">Buchwitz M., Jansen M., Renaudie J., Marchetti L., Voigt S. (2021). <a href="https://doi.org/10.3389/fevo.2021.674779">Evolutionary Change in Locomotion Close to the Origin of Amniotes Inferred From Trackway Data in an Ancestral State Reconstruction Approach</a>. Frontiers in Ecology and Evolution, 9:674779.</p>
			</div></description><category>Science</category><category>Programming</category><category>R</category><category>MS-related</category></item>
<item><title>Using R code in Julia and Python</title><link>http://plannapus.github.io/blog/2021-03-13.html</link><pubDate>2021-03-13</pubDate><description><div class="blog-content">
As we are moving toward the data analysis part of <a href="http://ehrenberglab.github.io/pages/mopga.html">our project</a>, I am trying to teach my working group how to use the stack of R functions I wrote during the past decade. The issue is however that R does not seem to be a popular option among my working group and that they prefer using either Python or Julia. As translating a decade-worth of code into those two languages is a bit of a bummer, I've been looking at how to read and use R functions in Julia and Python instead.
<p>Julia has, in particular, a very intuitive and well-done package to achieve this: <a href="https://github.com/JuliaInterop/RCall.jl">RCall.jl</a>:</p>

<pre><code class="julia">using RCall
R""" #Macro to make operations in an R environment
x &amp;lt;- c(0,1,2)
a &amp;lt;- 2
"""
X = @rget x # moving object from the R environemnt to Julia
y = X .+ randn(3)
@rput y # moving object from julia to the R environment
R"z &amp;lt;- y + rpois(3,2)"
@rget z

# Skipping the @rput step using the dollar sign notation
x = randn(10)
R"y &amp;lt;- mean($x)"
@rget y

# Concrete example using a function I wrote in R to compute Chao1 estimator and its Confidence Interval
R"""
chao1 &amp;lt;- function(mat){
  a &amp;lt;- sum(mat==1) #Number of singletons
  b &amp;lt;- sum(mat==2) #Number of doubletons
  S &amp;lt;- sum(mat&amp;gt;0)  #Number of species
  chao1 &amp;lt;- S+(a^2)/(2*b)
  d &amp;lt;- a/b
  varS1 &amp;lt;- b*((d/4)^4+d^3+(d/2)^2)
  t &amp;lt;- chao1-S
  k &amp;lt;- exp(1.96*sqrt(log(1+varS1/t^2)))
  c(S+t/k, chao1, S+t*k)
}
"""
#Alternatively
R"source('path/to/chao1.R')"

sample = [1, 42, 0, 3, 23, 2, 1, 1, 2, 5, 6, 0, 123, 9] #Made-up sample
res = R"chao1($sample)"
</code></pre>

<p>It's very easy to use, and the conversion between Julia and R objects is a no-brainer, even with dataframes. In the following example I take a species-accumulation curve (<a href="../data/sac.csv">here</a>) and fit a curve (using <a href="https://doi.org/10.1007/BF01042995">de Caprariis <emph>et al.</emph></a> formula) to estimate its asymptote (the results of the function is a list containing the fitted curve as a data.frame and a named vector containing the parameters of the function along with fit quality measures).</p>

<pre><code class="julia">using CSV
using DataFrames
sac = CSV.read("sac.csv", DataFrame, header=1, delim="\t")
R"""
caprariis &amp;lt;- function(SAC){
         caprariis.misfit &amp;lt;- function(parametres,x){
           Smax &amp;lt;- parametres[1]
           b &amp;lt;- parametres[2]
           FIT &amp;lt;- c()
           misfit &amp;lt;- 0
           for (i in 1:nrow(x)){
             FIT[i] &amp;lt;- (Smax*i)/(b+i)
             misfit &amp;lt;- sum(abs(FIT[i]-x[i,2]))+misfit
           }
           return(misfit)
         }
         OPT &amp;lt;- optim(c(50,10),caprariis.misfit,method="BFGS",x=SAC,control=list(trace=1))
         Smax &amp;lt;- OPT$par[1]
         b &amp;lt;- OPT$par[2]
         FIT &amp;lt;- c()
         caprar &amp;lt;- list()
         misfit &amp;lt;- OPT$value
         for (i in 1:nrow(SAC)) FIT[i] &amp;lt;- (Smax*i)/(b+i)
         caprar$Curve.fitting &amp;lt;- cbind(SAC,FIT)
         colnames(caprar$Curve.fitting) &amp;lt;- c("N","SAC","Fitting")
         pearson &amp;lt;- cor(FIT,SAC[,2])
         pearson.squared &amp;lt;- pearson^2
         all &amp;lt;- c(Smax,b,misfit,pearson,pearson.squared)
         names(all) &amp;lt;- c("Smax","b","Misfit","Pearson","Pearson squared")
         caprar$Summary &amp;lt;- all
         return(caprar)
       }
"""
R"caprariis($sac)"</code></pre>

<p>In Python, using <a href="https://pypi.org/project/rpy2/">rpy2</a>, it's still very easy (though a bit less elegant than with Julia I must say):</p>

<pre><code class="python">import rpy2.robjects as robjects

# As in Julia the object rpy2.robjects.r contains a queryable R environment
robjects.r('''
chao1 &amp;lt;- function(mat){
  a &amp;lt;- sum(mat==1) #Number of singletons
  b &amp;lt;- sum(mat==2) #Number of doubletons
  S &amp;lt;- sum(mat&amp;gt;0)  #Number of species
  chao1 &amp;lt;- S+(a^2)/(2*b)
  d &amp;lt;- a/b
  varS1 &amp;lt;- b*((d/4)^4+d^3+(d/2)^2)
  t &amp;lt;- chao1-S
  k &amp;lt;- exp(1.96*sqrt(log(1+varS1/t^2)))
  c(S+t/k,chao1,S+t*k)
}
''')
#Or read directly from the script
robjects.r('source(/path/to/chao1.R)')
#Or:
robjects.r.source("/path/to/chao1.R")

sample = [1, 42, 0, 3, 23, 2, 1, 1, 2, 5, 6, 0, 123, 9]
r_sample = robjects.IntVector(sample) #The conversion in python needs to be explicit
chao1 = robjects.r['chao1']
chao1(r_sample)</code></pre>

<p>The explicit conversion is a bit annoying, however when using pandas and dataframes, one can bypass that and make it way easier:</p>

<pre><code class="python">import pandas
from rpy2.robjects import pandas2ri
pandas2ri.activate()
robjects.r.source("/path/to/caprariis.R")
cap = robjects.r['caprariis']
sac = pandas.read_csv("sac.csv", delimiter="\t", header=1)
res = cap(sac)</code></pre>

<p>In the other direction, I have been using package <a href="">reticulate</a> to use Python code in R. Maybe I'll also write a post one day about it. Anyway I'm glad to see all my legacy code can still be used even by non-R-programmers.</p>
			</div></description><category>Programming</category><category>R</category><category>Python</category><category>Julia</category></item>
<item><title>A look back at IODP Expedition 379</title><link>http://plannapus.github.io/blog/2021-02-25.html</link><pubDate>2021-02-25</pubDate><description><div class="blog-content">
This week, the <a href="http://publications.iodp.org/proceedings/379/379title.html">Proceedings from IODP Expedition 379</a> <span class="tooltip">finally<span class="tooltiptext">They were supposed to come out six months ago, but were delayed because of COVID.</span></span> came out! The occasion to reminisce about the expedition, while being able to actually talk about what we found (as the moratorium on the preliminary results is over).<br/><img class="current" src="img/exp379.png" width="600"/><p class="caption">Credit: Tim Fulton, IODP.</p>

<p>First, let's state the obvious: it was one of the most exciting and fantastic experience I had in my life. Working on a mythical ship such as the JOIDES Resolution with the best, most qualified scientists in the world, at the top of their game, in one of the most remote area on Earth, so rarely visited that we actually got to <span class="tooltip">name<span class="tooltiptext">The Resolution Drift!</span></span> one of the oceanographic feature we spend our time on, surrounded by nothing but a constant flow of icebergs (700+ of them were counted and monitored during our trip) and a few curious humpback whales, all that to work on one of the most pressing question science has to answer right now (how stable is the West Antarctic Ice Sheet, and how fast is it able to restore itself): i can't fathom what experience, as a scientist, could top that for me.</p>

<img class="current" src="img/jrlab.png" width="600"/><img class="current" src="img/labview.png" width="600"/><p class="caption">The micropaleontology lab onboard the JOIDES Resolution has quite some view.</p>

<p>One downside of the constant flow of icebergs is that we spent a lot of time dodging them and thus we had a lot of <span class="tooltip">downtime<span class="tooltiptext">which did give me the time to read Lovecraft's <em>At the Mountains of Madness</em> and Geoff's <em>The Tao of Pooh</em> :)</span></span> workwise. But when we did drill, it was quite frantic: with a new core on deck every hour or so, processing core catchers for foraminifera and radiolarians (as Margot Courtillat and I were sharing samples) as well as the occasional in-core sample, and then scanning countless (mostly empty) slides to find the one or two rare rads we could use for figuring out the core age range was quite a thrill. Two years later, I can still smell the fresh core catchers, that mostly barren sticky grey clay we got hundreds of meters of.</p>

<img class="current" src="img/discuss.png" width="600"/><img class="current" src="img/green.png" width="600"/><p class="caption">The day crew in discussion over the significance of our first "bioturbated green interval". Picture credit: Tim Fulton, IODP.</p>

<p>While the endless laminated grey clay seems to correspond to glacial deposits, that small greener interval pictured here might correspond to retreat/collapse of the icesheet (the jury is still out on what it means exactly, but whatever it means, it is what we went in the Amundsen Sea for). Also: while the laminated grey intervals were mostly barren of radiolarians, those greener intervals were generally more prone to contain radiolarians and were thus packed with informations for me.</p>

<table class="img_current"><tr><td><img class="current" src="img/barrenness.png" width="400"/></td>
<td><img class="current" src="img/vema.png" width="500"/></td></tr></table><p class="caption">This was the desperate look of the micropaleontology report board during those long, barren grey laminated intervals. But then, when radiolarians are present one stumbles immediately on something that can pinpoint the age fairly precisely: here <em>Helotholus vema</em> Hays 1965.</p>

<p>While most of our research plans were disrupted because of COVID-19, we're still working at it diligently. Stay tune for great science to come out of the material we drilled during Expedition 379!</p>
			</div></description><category>Science</category><category>MS-related</category></item>
<item><title>Unagi Onigirazu</title><link>http://plannapus.github.io/blog/2021-01-15.html</link><pubDate>2021-01-15</pubDate><description><div class="blog-content">
			Yes, I reached that stage of the lockdown when I like to pretend I am still going to work by preparing myself a bento :)
			<p>Today, in my <a href="https://www.instagram.com/p/B8Gj_-Ni2Ei/">ongoing attempt</a> at mastering <a href="https://www.justonecookbook.com/search/?q=onigirazu">onigirazu</a>, coupled by my ongoing attempt at mastering Unagi Kobayashi, I combined both. The concept of onigirazu is fairly simple: one only has to cook rice the way you would cook it for onigiris (minus the surplus of salt), or for sushis (minus the rice vinegar), place a square of nori algae, make a square of rice in the middle (leave a large margin on all sides), place some ingredient on top, a sauce (personnally I like adding some sriracha mayo), some more rice, and then fold the nori algae around the ingredients like some kind of gift wrap. Then put it in a box or wrap it in foil and place in the fridge overnight, the next day slice in half, and voilà: a rice sandwich!</p>

<img class="current" src="img/onigirazu.png" width="400"/><p>So my main filling today was eel. I defrosted one yesterday, took the head and tail off and sliced it in flat squares. Separately I reduced on the stove a mixture of soy sauce, mirin, sugar and broth (normally fish broth but yesterday I only had beef broth) until it thickens. Then I placed the eel skin-side on aluminium foil in the oven and brushed some of the sauce on both side. Every 5~10min i would open the oven to brush some of the sauce on top of the eel again. The whole process lasted ca. 40min (I flipped the eel 10min before the end). Naturally, proper Unagi Kobayashi is made on a barbecue, but, well, it's winter and I live in a flat.</p>
			<p>In the past, I made onigirazus with salmon teriyaki, with cucumbers, beetroots, etc. People also make them with spam. It's a very versatile dish.</p>
			</div></description><category>Cooking</category></item>
<item><title>Wrap-up on AoC2020 and my conky setup</title><link>http://plannapus.github.io/blog/2021-01-03.html</link><pubDate>2021-01-03</pubDate><description><div class="blog-content">
				First thing first: the final day of AoC2020. <a href="http://adventofcode.com/2020/day/25">The puzzle for the 25th</a>, as usual, was only a one-parter (the second part is awarded if and only if all of the other 49 puzzles were successfully completed). It was a fairly simple cryptography problem. Two numbers have gone through a mathematically operation (multiplication/modulo) an indeterminate amount of times, i.e. number x is the result of a number going through n iterations and y the result of the same number going through m iterations, and the problem is to find n and m such as when x goes through m iterations and y through n iterations it results in an identical number (which becomes the solution of the puzzle). My simple solution:
				<pre><code class="r">input &amp;lt;- scan("input25.txt")
cardpk &amp;lt;- input[1]
doorpk &amp;lt;- input[2]
n_loops &amp;lt;- function(n,to){ #Find the number of loop, knowing the operation, the key number (here 7) and the resulting number
  start &amp;lt;- 1
  i &amp;lt;- 0
  while(start!=to){
   i &amp;lt;- i+1
   start &amp;lt;- (start*n)%%20201227
  }
  i
}
cardloop &amp;lt;- n_loops(7,cardpk) #Number n of loops for first input
doorloop &amp;lt;- n_loops(7,doorpk) #Number m of loops for second input

transform &amp;lt;- function(n,n_loop){ #Reverse operations: knowing the number of loops, the key number (the first and second input), find the resulting number
  start &amp;lt;- 1
  for(i in 1:n_loop){
    start &amp;lt;- (start*n)%%20201227
  }
  start
}
ek1 &amp;lt;- transform(cardpk,doorloop)
ek2 &amp;lt;- transform(doorpk,cardloop)
ek1==ek2 #Check that they are indeed identical
#TRUE
ek1
#9177528</code></pre>

<p>As mentioned before, the puzzles this year where significantly easier than last year (though some, like day 20, were still insanely hard); I am nonetheless very happy to have been able this year to complete all puzzles!</p>

<img class="current" src="https://github.com/plannapus/Advent_of_Code/raw/master/2020/screenshot.png" width="800" title="Screenshot of final stats for year 2020"/><p>In other programming news, I have been fiddling with a <a href="https://github.com/brndnmtthws/conky">conky theme</a> in the past few days. For those who do not know conky, it is, similarly to Mac's <a href="https://www.reddit.com/r/GeekTool/">Geektools</a> and <a href="https://tracesof.net/uebersicht/">Übersicht</a>, a scripting tool allowing you to integrate self-updating elements to your desktop background. I have been using Geektools, and then later Übersicht, on my work Mac since years to integrate date/time, weather updates, calendar updates, and various climatological/geological realtime data (quakes, current state of the arctic/antarctic icesheets, etc.). Now that I work at home on my own Ubuntu laptop, I started craving for something similar, hence my journey into conky. While Geektools used a fancy GUI and bash scripts, and Übersicht uses a mixture of bash,  coffeescript and CSS (!), conky is pure bash which is actually not a bad thing.</p>

<p>My setup is a mixture of simple, standard conky things (in fact part of it is kept from the default configuration file) and more complex data representations computed asynchronously by R scripts and just read by the conky setup. Let's have a look at the config file. The first part is not the content but the options configuring the overall aspect and behaviour of the whole setup:</p>

<pre><code class="bash">conky.config = {
    alignment = 'top_left',
    background = true,
    border_width = 0,
    cpu_avg_samples = 2,
	default_color = 'white',
    default_outline_color = 'white',
    default_shade_color = 'white',
    draw_borders = false,
    draw_graph_borders = false,
    draw_outline = false,
    draw_shades = false,
    use_xft = true,
    font = 'DejaVu Sans Mono:size=12',
    gap_x = 70,
    gap_y = 30,
    minimum_height = 5,
	minimum_width = 385,
    net_avg_samples = 2,
    no_buffers = true,
    out_to_console = false,
    out_to_stderr = false,
    extra_newline = false,
    own_window = true,
    own_window_class = 'Conky',
    own_window_type = 'desktop',
    own_window_argb_visual = true,
    own_window_colour = '#435366',
    own_window_transparent = false,
    stippled_borders = 0,
    update_interval = 1.0,
    uppercase = false,
    use_spacer = 'none',
    show_graph_scale = false,
    show_graph_range = false
}</code></pre>

<p>As said before, most of it is from the default setup, except from a few things: the <code class="inline">own_window_colour</code> which I set to the color of the rest of the background, <code class="inline">gap_x, gap_y, minimum_width</code> which I set up so that the "window" is just glued to the dash column. The second and most important part is what actually set the content of the "window/widget":</p>

<pre><code class="none">conky.text = [[
${font DejaVu Sans Mono:size=18}${exec date +"%A %d %B, %H:%M"}$font
$hr
${color grey}Uptime:$color $uptime_short
${color grey}RAM Usage:$color $mem/$memmax - $memperc% ${membar 4}
${color grey}CPU Usage:$color $cpu% ${cpubar 4}
${color grey}Processes:$color $processes  ${color grey}Running:$color $running_processes
$hr
${color grey}File systems:
 / $color${fs_used /}/${fs_size /} ${fs_bar 6 /}
$hr
${color grey}Core 0:$color ${exec sensors | awk '/Core 0/ {print $3}'} ${execbar sensors | awk '/Core 0/ {print $3}'}
${color grey}Core 1:$color ${exec sensors | awk '/Core 1/ {print $3}'} ${execbar sensors | awk '/Core 1/ {print $3}'}
${color grey}Core 2:$color ${exec sensors | awk '/Core 2/ {print $3}'} ${execbar sensors | awk '/Core 2/ {print $3}'}
${color grey}Core 3:$color ${exec sensors | awk '/Core 3/ {print $3}'} ${execbar sensors | awk '/Core 3/ {print $3}'}
$hr
${color grey}Name              PID   CPU%   MEM%
${color lightgrey} ${top name 1} ${top pid 1} ${top cpu 1} ${top mem 1}
${color lightgrey} ${top name 2} ${top pid 2} ${top cpu 2} ${top mem 2}
${color lightgrey} ${top name 3} ${top pid 3} ${top cpu 3} ${top mem 3}
${color lightgrey} ${top name 4} ${top pid 4} ${top cpu 4} ${top mem 4}$color
$hr
${color grey}Coronavirus in Berlin
${image ~/.config/conky/corona_berlin.png -p 0,435 -s 380x150}








${color grey}Total cases:$color ${exec cut -d ',' -f1 ~/.config/conky/coronaberlin.txt} (${exec cut -d ',' -f2 ~/.config/conky/coronaberlin.txt} active)
${color grey}New:$color ${exec cut -d ',' -f3 ~/.config/conky/coronaberlin.txt} ${color grey}- R:$color ${exec cut -d ',' -f4 ~/.config/conky/coronaberlin.txt} ${color grey}- ICU beds:$color ${exec cut -d ',' -f5 ~/.config/conky/coronaberlin.txt}%
$hr
${color grey}Current pCO2:$color ${exec cut -d ',' -f1 ~/.config/conky/pco2.txt | head -n 1} ${color grey}ppm on$color ${exec cut -d ',' -f2 ~/.config/conky/pco2.txt | head -n 1}
${color grey}Max pCO2    :$color ${exec cut -d ',' -f1 ~/.config/conky/pco2.txt | tail -n 1} ${color grey}ppm on$color ${exec cut -d ',' -f2 ~/.config/conky/pco2.txt | tail -n 1}
$hr
Calendar events for the week:
${font DejaVu Sans Mono:size=10}${color lightgrey}${exec head -n 7 ~/.config/conky/events.txt}
${if_updatenr 1}${image ~/.config/conky/plots/01-535.png -p 0,860 -s 380x190}${endif}
${if_updatenr 2}${image ~/.config/conky/plots/02-525.png -p 0,860 -s 380x190}${endif}
${if_updatenr 3}${image ~/.config/conky/plots/03-515.png -p 0,860 -s 380x190}${endif}
${if_updatenr 4}${image ~/.config/conky/plots/04-510.png -p 0,860 -s 380x190}${endif}
${if_updatenr 5}${image ~/.config/conky/plots/05-505.png -p 0,860 -s 380x190}${endif}
${if_updatenr 6}${image ~/.config/conky/plots/06-500.png -p 0,860 -s 380x190}${endif}
${if_updatenr 7}${image ~/.config/conky/plots/07-495.png -p 0,860 -s 380x190}${endif}
${if_updatenr 8}${image ~/.config/conky/plots/08-490.png -p 0,860 -s 380x190}${endif}
${if_updatenr 9}${image ~/.config/conky/plots/09-485.png -p 0,860 -s 380x190}${endif}
${if_updatenr 10}${image ~/.config/conky/plots/10-480.png -p 0,860 -s 380x190}${endif}
${if_updatenr 11}${image ~/.config/conky/plots/11-475.png -p 0,860 -s 380x190}${endif}
${if_updatenr 12}${image ~/.config/conky/plots/12-470.png -p 0,860 -s 380x190}${endif}
${if_updatenr 13}${image ~/.config/conky/plots/13-465.png -p 0,860 -s 380x190}${endif}
${if_updatenr 14}${image ~/.config/conky/plots/14-455.png -p 0,860 -s 380x190}${endif}
${if_updatenr 15}${image ~/.config/conky/plots/15-450.png -p 0,860 -s 380x190}${endif}
${if_updatenr 16}${image ~/.config/conky/plots/16-445.png -p 0,860 -s 380x190}${endif}
${if_updatenr 17}${image ~/.config/conky/plots/17-440.png -p 0,860 -s 380x190}${endif}
${if_updatenr 18}${image ~/.config/conky/plots/18-435.png -p 0,860 -s 380x190}${endif}
${if_updatenr 19}${image ~/.config/conky/plots/19-430.png -p 0,860 -s 380x190}${endif}
${if_updatenr 20}${image ~/.config/conky/plots/20-425.png -p 0,860 -s 380x190}${endif}
${if_updatenr 21}${image ~/.config/conky/plots/21-420.png -p 0,860 -s 380x190}${endif}
${if_updatenr 22}${image ~/.config/conky/plots/22-415.png -p 0,860 -s 380x190}${endif}
${if_updatenr 23}${image ~/.config/conky/plots/23-410.png -p 0,860 -s 380x190}${endif}
${if_updatenr 24}${image ~/.config/conky/plots/24-400.png -p 0,860 -s 380x190}${endif}
${if_updatenr 25}${image ~/.config/conky/plots/25-390.png -p 0,860 -s 380x190}${endif}
${if_updatenr 26}${image ~/.config/conky/plots/26-385.png -p 0,860 -s 380x190}${endif}
${if_updatenr 27}${image ~/.config/conky/plots/27-375.png -p 0,860 -s 380x190}${endif}
${if_updatenr 28}${image ~/.config/conky/plots/28-365.png -p 0,860 -s 380x190}${endif}
${if_updatenr 29}${image ~/.config/conky/plots/29-355.png -p 0,860 -s 380x190}${endif}
${if_updatenr 30}${image ~/.config/conky/plots/30-340.png -p 0,860 -s 380x190}${endif}
${if_updatenr 31}${image ~/.config/conky/plots/31-325.png -p 0,860 -s 380x190}${endif}
${if_updatenr 32}${image ~/.config/conky/plots/32-320.png -p 0,860 -s 380x190}${endif}
${if_updatenr 33}${image ~/.config/conky/plots/33-310.png -p 0,860 -s 380x190}${endif}
${if_updatenr 34}${image ~/.config/conky/plots/34-305.png -p 0,860 -s 380x190}${endif}
${if_updatenr 35}${image ~/.config/conky/plots/35-300.png -p 0,860 -s 380x190}${endif}
${if_updatenr 36}${image ~/.config/conky/plots/36-295.png -p 0,860 -s 380x190}${endif}
${if_updatenr 37}${image ~/.config/conky/plots/37-285.png -p 0,860 -s 380x190}${endif}
${if_updatenr 38}${image ~/.config/conky/plots/38-275.png -p 0,860 -s 380x190}${endif}
${if_updatenr 39}${image ~/.config/conky/plots/39-270.png -p 0,860 -s 380x190}${endif}
${if_updatenr 40}${image ~/.config/conky/plots/40-265.png -p 0,860 -s 380x190}${endif}
${if_updatenr 41}${image ~/.config/conky/plots/41-260.png -p 0,860 -s 380x190}${endif}
${if_updatenr 42}${image ~/.config/conky/plots/42-255.png -p 0,860 -s 380x190}${endif}
${if_updatenr 43}${image ~/.config/conky/plots/43-250.png -p 0,860 -s 380x190}${endif}
${if_updatenr 44}${image ~/.config/conky/plots/44-245.png -p 0,860 -s 380x190}${endif}
${if_updatenr 45}${image ~/.config/conky/plots/45-240.png -p 0,860 -s 380x190}${endif}
${if_updatenr 46}${image ~/.config/conky/plots/46-230.png -p 0,860 -s 380x190}${endif}
${if_updatenr 47}${image ~/.config/conky/plots/47-220.png -p 0,860 -s 380x190}${endif}
${if_updatenr 48}${image ~/.config/conky/plots/48-205.png -p 0,860 -s 380x190}${endif}
${if_updatenr 49}${image ~/.config/conky/plots/49-200.png -p 0,860 -s 380x190}${endif}
${if_updatenr 50}${image ~/.config/conky/plots/50-195.png -p 0,860 -s 380x190}${endif}
${if_updatenr 51}${image ~/.config/conky/plots/51-185.png -p 0,860 -s 380x190}${endif}
${if_updatenr 52}${image ~/.config/conky/plots/52-180.png -p 0,860 -s 380x190}${endif}
${if_updatenr 53}${image ~/.config/conky/plots/53-170.png -p 0,860 -s 380x190}${endif}
${if_updatenr 54}${image ~/.config/conky/plots/54-165.png -p 0,860 -s 380x190}${endif}
${if_updatenr 55}${image ~/.config/conky/plots/55-160.png -p 0,860 -s 380x190}${endif}
${if_updatenr 56}${image ~/.config/conky/plots/56-155.png -p 0,860 -s 380x190}${endif}
${if_updatenr 57}${image ~/.config/conky/plots/57-150.png -p 0,860 -s 380x190}${endif}
${if_updatenr 58}${image ~/.config/conky/plots/58-140.png -p 0,860 -s 380x190}${endif}
${if_updatenr 59}${image ~/.config/conky/plots/59-135.png -p 0,860 -s 380x190}${endif}
${if_updatenr 60}${image ~/.config/conky/plots/60-130.png -p 0,860 -s 380x190}${endif}
${if_updatenr 61}${image ~/.config/conky/plots/61-125.png -p 0,860 -s 380x190}${endif}
${if_updatenr 62}${image ~/.config/conky/plots/62-120.png -p 0,860 -s 380x190}${endif}
${if_updatenr 63}${image ~/.config/conky/plots/63-105.png -p 0,860 -s 380x190}${endif}
${if_updatenr 64}${image ~/.config/conky/plots/64-95.png -p 0,860 -s 380x190}${endif}
${if_updatenr 65}${image ~/.config/conky/plots/65-90.png -p 0,860 -s 380x190}${endif}
${if_updatenr 66}${image ~/.config/conky/plots/66-85.png -p 0,860 -s 380x190}${endif}
${if_updatenr 67}${image ~/.config/conky/plots/67-80.png -p 0,860 -s 380x190}${endif}
${if_updatenr 68}${image ~/.config/conky/plots/68-70.png -p 0,860 -s 380x190}${endif}
${if_updatenr 69}${image ~/.config/conky/plots/69-65.png -p 0,860 -s 380x190}${endif}
${if_updatenr 70}${image ~/.config/conky/plots/70-60.png -p 0,860 -s 380x190}${endif}
${if_updatenr 71}${image ~/.config/conky/plots/71-50.png -p 0,860 -s 380x190}${endif}
${if_updatenr 72}${image ~/.config/conky/plots/72-45.png -p 0,860 -s 380x190}${endif}
${if_updatenr 73}${image ~/.config/conky/plots/73-40.png -p 0,860 -s 380x190}${endif}
${if_updatenr 74}${image ~/.config/conky/plots/74-35.png -p 0,860 -s 380x190}${endif}
${if_updatenr 75}${image ~/.config/conky/plots/75-30.png -p 0,860 -s 380x190}${endif}
${if_updatenr 76}${image ~/.config/conky/plots/76-25.png -p 0,860 -s 380x190}${endif}
${if_updatenr 77}${image ~/.config/conky/plots/77-20.png -p 0,860 -s 380x190}${endif}
${if_updatenr 78}${image ~/.config/conky/plots/78-15.png -p 0,860 -s 380x190}${endif}
${if_updatenr 79}${image ~/.config/conky/plots/79-10.png -p 0,860 -s 380x190}${endif}
${if_updatenr 80}${image ~/.config/conky/plots/80-5.png -p 0,860 -s 380x190}${endif}
${if_updatenr 81}${image ~/.config/conky/plots/81-0.png -p 0,860 -s 380x190}${endif}
]]</code></pre>

<p>OK that's a lot, but basically there are 6 sections to it. The first one is almost untouched from the basic config file:</p>

<pre><code class="bash">${font DejaVu Sans Mono:size=18}${exec date +"%A %d %B, %H:%M"}$font
$hr
${color grey}Uptime:$color $uptime_short
${color grey}RAM Usage:$color $mem/$memmax - $memperc% ${membar 4}
${color grey}CPU Usage:$color $cpu% ${cpubar 4}
${color grey}Processes:$color $processes  ${color grey}Running:$color $running_processes
$hr
${color grey}File systems:
 / $color${fs_used /}/${fs_size /} ${fs_bar 6 /}
$hr
${color grey}Name              PID   CPU%   MEM%
${color lightgrey} ${top name 1} ${top pid 1} ${top cpu 1} ${top mem 1}
${color lightgrey} ${top name 2} ${top pid 2} ${top cpu 2} ${top mem 2}
${color lightgrey} ${top name 3} ${top pid 3} ${top cpu 3} ${top mem 3}
${color lightgrey} ${top name 4} ${top pid 4} ${top cpu 4} ${top mem 4}$color
$hr</code></pre>

<p>There is a basic color (here white) used overall, but when one needs to momentarily use another color, one uses those <code class="inline">${color ...}</code> statements followed by <code class="inline">$color</code> to go back to the main color. Same with fonts. The first line uses something that I'll be using everywhere afterwards: <code class="inline">${exec ...}</code>, which just execute a shell command and return the standard output. Here just the date and time. All the rest are standard conky parameters: <code class="inline">top</code> gives the current processes using the most CPU, <code class="inline">$cpu%</code> returns the percentage of CPU used, <code class="inline">${cpubar ...}</code> the same but as a bar chart, etc. Now the second segment (that is in fact intercalated between the file system size subsection and the "top" subsection):</p>

<pre><code class="none">${color grey}Core 0:$color ${exec sensors | awk '/Core 0/ {print $3}'} ${execbar sensors | awk '/Core 0/ {print $3}'}
${color grey}Core 1:$color ${exec sensors | awk '/Core 1/ {print $3}'} ${execbar sensors | awk '/Core 1/ {print $3}'}
${color grey}Core 2:$color ${exec sensors | awk '/Core 2/ {print $3}'} ${execbar sensors | awk '/Core 2/ {print $3}'}
${color grey}Core 3:$color ${exec sensors | awk '/Core 3/ {print $3}'} ${execbar sensors | awk '/Core 3/ {print $3}'}</code></pre>

<p>This uses <a href="https://wiki.archlinux.org/index.php/lm_sensors"><code class="inline">sensors</code></a> to check the temperature of the cores. Unfortunately what sensors returns is a much too complicated text, but I just want the actually numbers here, so a bit of "awk" regex is necessary. What <code class="inline">${execbar ...}</code> does, is that it executes a shell command but returns a bar chart (the output of the command is treated as an integer between 0 and 100). The third section starts using R scripts in the background:</p>

<pre><code class="none">$hr
${color grey}Coronavirus in Berlin
${image ~/.config/conky/corona_berlin.png -p 0,435 -s 380x150}








${color grey}Total cases:$color ${exec cut -d ',' -f1 ~/.config/conky/coronaberlin.txt} (${exec cut -d ',' -f2 ~/.config/conky/coronaberlin.txt} active)
${color grey}New:$color ${exec cut -d ',' -f3 ~/.config/conky/coronaberlin.txt} ${color grey}- R:$color ${exec cut -d ',' -f4 ~/.config/conky/coronaberlin.txt} ${color grey}- ICU beds:$color ${exec cut -d ',' -f5 ~/.config/conky/coronaberlin.txt}%
$hr</code></pre>

<p>OK,a lot to go through here. As one can see in the "config" part of the setup, the whole thing refreshes every 1s. Bash is fast so that's fine, but running a R script, particularly one that grabs data online and produces a chart as a result is not. So I chose to do that asynchronously. There is a separate R script running once every 3 hours that grabs the latest Coronavirus report from the Berlin <a href="https://www.berlin.de/corona/">official website</a>, and outputs on one hand a simple text file with the latest data, and on the other a full plot showing the complete local history. The plot is shown with <code class="inline">${image ...}</code>, argument "p" specify the position (relative to the overall window/widget). As you can see, I left some empty lines for space for the plot. Then with <code class="inline">${exec ...}</code> i run simple bash commands that reads the data file, and pick the necessary fields (with <code class="inline"><a href="https://man7.org/linux/man-pages/man1/cut.1.html">cut</a></code>).
Here is the associated R script:</p>

<pre><code class="R">library(rjson)
a &amp;lt;- fromJSON(readLines("https://www.berlin.de/lageso/gesundheit/infektionsepidemiologie-infektionsschutz/corona/tabelle-indikatoren-gesamtuebersicht/index.php/index/all.json?q=4",warn=FALSE)) #Grabs json file online
date &amp;lt;- as.Date(sapply(a$index,function(x)x$datum)) #Date
cases &amp;lt;- as.integer(sapply(a$index,function(x)x$fallzahl)) #Total cases
gueri &amp;lt;- as.integer(sapply(a$index,function(x)x$genesene)) #Total recovered
dead &amp;lt;- as.integer(sapply(a$index,function(x)x$todesfaelle)) #Total deaths
r0 &amp;lt;- as.numeric(sapply(a$index,function(x)x$`4_tage_r_wert_berlin_rki`)) #R
its &amp;lt;- as.numeric(sapply(a$index,function(x)x$its_belegung)) #Percentage of ICU beds used
daily &amp;lt;- diff(cases) #Daily new cases
d7ave &amp;lt;- sapply(7:length(daily),function(x)mean(daily[x-6:0])) #Running weekly average of the latter
cat(tail(cases,1), tail(cases,1)-tail(dead,1)-tail(gueri,1), tail(diff(cases),1),r0[length(r0)-5], tail(its,1),sep=",",file="~/.config/conky/coronaberlin.txt") #Save the last data in a file as comma separated values
cat(sprintf("Total cases: %i (%i active)\nNew: %i - R: %.2f - ICU beds: %.0f%%", tail(cases,1), tail(cases,1)-tail(dead,1)-tail(gueri,1), tail(diff(cases),1),r0[length(r0)-5], tail(its,1)))
png("~/.config/conky/corona_berlin.png",bg="transparent",w=395,h=150) #Plot the historical data
par(mar=c(0,0,0,0))
plot(date,c(cases[1],daily),type="n",yaxs="i",ax=F,ann=F,ylim=c(0,max(round(diff(cases)+500,-3))),xlim=c(date[1],tail(date,1)+1),xaxs="i")
segments(date,0,
         date,c(cases[1],daily),col="grey50")
lines(head(date[-(1:4)],-3),d7ave,lwd=2,col="white")
box(lwd=2,col="white")
invisible(dev.off()) #invisible is needed here only if the script is directly executed by conky as conky prints all stdout.</code></pre>

The fourth section is very similar, in that it grabs data online (of <a href="https://en.wikipedia.org/wiki/Mauna_Loa_Observatory">Mauna Loa weekly CO2 concentrations</a>) with an R script asynchronously, saves them in a data file and then conky reads them and prints them:

<pre><code class="none">$hr
${color grey}Current pCO2:$color ${exec cut -d ',' -f1 ~/.config/conky/pco2.txt | head -n 1} ${color grey}ppm on$color ${exec cut -d ',' -f2 ~/.config/conky/pco2.txt | head -n 1}
${color grey}Max pCO2    :$color ${exec cut -d ',' -f1 ~/.config/conky/pco2.txt | tail -n 1} ${color grey}ppm on$color ${exec cut -d ',' -f2 ~/.config/conky/pco2.txt | tail -n 1}
$hr</code></pre>

And the corresponding R script:

<pre><code class="none">r &amp;lt;- readLines("ftp://aftp.cmdl.noaa.gov/products/trends/co2/co2_weekly_mlo.txt")
d &amp;lt;- do.call(rbind,strsplit(r[!grepl("^#",r)]," +"))
a &amp;lt;- apply(d,2,as.numeric)
a &amp;lt;- a[,-1]
a[a== -999.99] &amp;lt;- NA
maunalau &amp;lt;- data.frame(date=as.Date(paste(a[,1],a[,2],a[,3],sep="-")),co2=a[,5])
with(tail(maunalau,1),
     cat(sprintf("%.2f,%s\n",round(co2,2),as.character(date)),file="~/.config/conky/pco2.txt"))
with(maunalau[which.max(maunalau$co2),],
     cat(sprintf("%.2f,%s",round(co2,2),as.character(date)),append=TRUE,file="~/.config/conky/pco2.txt"))</code></pre>

<p>The 5th section grabs calendar data and prints them to conky. Honestly it's still very clunky so I'm not going to explain it here until I'll figure out a proper to do it. The 6th however is interesting: it shows an animated earth with plate tectonics for the full Phanerozoic (using the latest reconstructions of continental margins/shorelines by <a href="https://www.sciencedirect.com/science/article/abs/pii/S0012825220305092">Kocsis &amp;amp; Scotese</a>). This time the way I did it is to first produce the maps using R, save them in an accessible folder and then just loop through those pictures with conky at each refresh (i. e. one map every second). This is what the 6th section thus look like in the conky setting file:</p>

<pre><code class="none">${if_updatenr 1}${image ~/.config/conky/plots/01-535.png -p 0,860 -s 380x190}${endif}
${if_updatenr 2}${image ~/.config/conky/plots/02-525.png -p 0,860 -s 380x190}${endif}
${if_updatenr 3}${image ~/.config/conky/plots/03-515.png -p 0,860 -s 380x190}${endif}
${if_updatenr 4}${image ~/.config/conky/plots/04-510.png -p 0,860 -s 380x190}${endif}
${if_updatenr 5}${image ~/.config/conky/plots/05-505.png -p 0,860 -s 380x190}${endif}
${if_updatenr 6}${image ~/.config/conky/plots/06-500.png -p 0,860 -s 380x190}${endif}
${if_updatenr 7}${image ~/.config/conky/plots/07-495.png -p 0,860 -s 380x190}${endif}
${if_updatenr 8}${image ~/.config/conky/plots/08-490.png -p 0,860 -s 380x190}${endif}
${if_updatenr 9}${image ~/.config/conky/plots/09-485.png -p 0,860 -s 380x190}${endif}
${if_updatenr 10}${image ~/.config/conky/plots/10-480.png -p 0,860 -s 380x190}${endif}
${if_updatenr 11}${image ~/.config/conky/plots/11-475.png -p 0,860 -s 380x190}${endif}
${if_updatenr 12}${image ~/.config/conky/plots/12-470.png -p 0,860 -s 380x190}${endif}
${if_updatenr 13}${image ~/.config/conky/plots/13-465.png -p 0,860 -s 380x190}${endif}
${if_updatenr 14}${image ~/.config/conky/plots/14-455.png -p 0,860 -s 380x190}${endif}
${if_updatenr 15}${image ~/.config/conky/plots/15-450.png -p 0,860 -s 380x190}${endif}
${if_updatenr 16}${image ~/.config/conky/plots/16-445.png -p 0,860 -s 380x190}${endif}
${if_updatenr 17}${image ~/.config/conky/plots/17-440.png -p 0,860 -s 380x190}${endif}
${if_updatenr 18}${image ~/.config/conky/plots/18-435.png -p 0,860 -s 380x190}${endif}
${if_updatenr 19}${image ~/.config/conky/plots/19-430.png -p 0,860 -s 380x190}${endif}
${if_updatenr 20}${image ~/.config/conky/plots/20-425.png -p 0,860 -s 380x190}${endif}
${if_updatenr 21}${image ~/.config/conky/plots/21-420.png -p 0,860 -s 380x190}${endif}
${if_updatenr 22}${image ~/.config/conky/plots/22-415.png -p 0,860 -s 380x190}${endif}
${if_updatenr 23}${image ~/.config/conky/plots/23-410.png -p 0,860 -s 380x190}${endif}
${if_updatenr 24}${image ~/.config/conky/plots/24-400.png -p 0,860 -s 380x190}${endif}
${if_updatenr 25}${image ~/.config/conky/plots/25-390.png -p 0,860 -s 380x190}${endif}
${if_updatenr 26}${image ~/.config/conky/plots/26-385.png -p 0,860 -s 380x190}${endif}
${if_updatenr 27}${image ~/.config/conky/plots/27-375.png -p 0,860 -s 380x190}${endif}
${if_updatenr 28}${image ~/.config/conky/plots/28-365.png -p 0,860 -s 380x190}${endif}
${if_updatenr 29}${image ~/.config/conky/plots/29-355.png -p 0,860 -s 380x190}${endif}
${if_updatenr 30}${image ~/.config/conky/plots/30-340.png -p 0,860 -s 380x190}${endif}
${if_updatenr 31}${image ~/.config/conky/plots/31-325.png -p 0,860 -s 380x190}${endif}
${if_updatenr 32}${image ~/.config/conky/plots/32-320.png -p 0,860 -s 380x190}${endif}
${if_updatenr 33}${image ~/.config/conky/plots/33-310.png -p 0,860 -s 380x190}${endif}
${if_updatenr 34}${image ~/.config/conky/plots/34-305.png -p 0,860 -s 380x190}${endif}
${if_updatenr 35}${image ~/.config/conky/plots/35-300.png -p 0,860 -s 380x190}${endif}
${if_updatenr 36}${image ~/.config/conky/plots/36-295.png -p 0,860 -s 380x190}${endif}
${if_updatenr 37}${image ~/.config/conky/plots/37-285.png -p 0,860 -s 380x190}${endif}
${if_updatenr 38}${image ~/.config/conky/plots/38-275.png -p 0,860 -s 380x190}${endif}
${if_updatenr 39}${image ~/.config/conky/plots/39-270.png -p 0,860 -s 380x190}${endif}
${if_updatenr 40}${image ~/.config/conky/plots/40-265.png -p 0,860 -s 380x190}${endif}
${if_updatenr 41}${image ~/.config/conky/plots/41-260.png -p 0,860 -s 380x190}${endif}
${if_updatenr 42}${image ~/.config/conky/plots/42-255.png -p 0,860 -s 380x190}${endif}
${if_updatenr 43}${image ~/.config/conky/plots/43-250.png -p 0,860 -s 380x190}${endif}
${if_updatenr 44}${image ~/.config/conky/plots/44-245.png -p 0,860 -s 380x190}${endif}
${if_updatenr 45}${image ~/.config/conky/plots/45-240.png -p 0,860 -s 380x190}${endif}
${if_updatenr 46}${image ~/.config/conky/plots/46-230.png -p 0,860 -s 380x190}${endif}
${if_updatenr 47}${image ~/.config/conky/plots/47-220.png -p 0,860 -s 380x190}${endif}
${if_updatenr 48}${image ~/.config/conky/plots/48-205.png -p 0,860 -s 380x190}${endif}
${if_updatenr 49}${image ~/.config/conky/plots/49-200.png -p 0,860 -s 380x190}${endif}
${if_updatenr 50}${image ~/.config/conky/plots/50-195.png -p 0,860 -s 380x190}${endif}
${if_updatenr 51}${image ~/.config/conky/plots/51-185.png -p 0,860 -s 380x190}${endif}
${if_updatenr 52}${image ~/.config/conky/plots/52-180.png -p 0,860 -s 380x190}${endif}
${if_updatenr 53}${image ~/.config/conky/plots/53-170.png -p 0,860 -s 380x190}${endif}
${if_updatenr 54}${image ~/.config/conky/plots/54-165.png -p 0,860 -s 380x190}${endif}
${if_updatenr 55}${image ~/.config/conky/plots/55-160.png -p 0,860 -s 380x190}${endif}
${if_updatenr 56}${image ~/.config/conky/plots/56-155.png -p 0,860 -s 380x190}${endif}
${if_updatenr 57}${image ~/.config/conky/plots/57-150.png -p 0,860 -s 380x190}${endif}
${if_updatenr 58}${image ~/.config/conky/plots/58-140.png -p 0,860 -s 380x190}${endif}
${if_updatenr 59}${image ~/.config/conky/plots/59-135.png -p 0,860 -s 380x190}${endif}
${if_updatenr 60}${image ~/.config/conky/plots/60-130.png -p 0,860 -s 380x190}${endif}
${if_updatenr 61}${image ~/.config/conky/plots/61-125.png -p 0,860 -s 380x190}${endif}
${if_updatenr 62}${image ~/.config/conky/plots/62-120.png -p 0,860 -s 380x190}${endif}
${if_updatenr 63}${image ~/.config/conky/plots/63-105.png -p 0,860 -s 380x190}${endif}
${if_updatenr 64}${image ~/.config/conky/plots/64-95.png -p 0,860 -s 380x190}${endif}
${if_updatenr 65}${image ~/.config/conky/plots/65-90.png -p 0,860 -s 380x190}${endif}
${if_updatenr 66}${image ~/.config/conky/plots/66-85.png -p 0,860 -s 380x190}${endif}
${if_updatenr 67}${image ~/.config/conky/plots/67-80.png -p 0,860 -s 380x190}${endif}
${if_updatenr 68}${image ~/.config/conky/plots/68-70.png -p 0,860 -s 380x190}${endif}
${if_updatenr 69}${image ~/.config/conky/plots/69-65.png -p 0,860 -s 380x190}${endif}
${if_updatenr 70}${image ~/.config/conky/plots/70-60.png -p 0,860 -s 380x190}${endif}
${if_updatenr 71}${image ~/.config/conky/plots/71-50.png -p 0,860 -s 380x190}${endif}
${if_updatenr 72}${image ~/.config/conky/plots/72-45.png -p 0,860 -s 380x190}${endif}
${if_updatenr 73}${image ~/.config/conky/plots/73-40.png -p 0,860 -s 380x190}${endif}
${if_updatenr 74}${image ~/.config/conky/plots/74-35.png -p 0,860 -s 380x190}${endif}
${if_updatenr 75}${image ~/.config/conky/plots/75-30.png -p 0,860 -s 380x190}${endif}
${if_updatenr 76}${image ~/.config/conky/plots/76-25.png -p 0,860 -s 380x190}${endif}
${if_updatenr 77}${image ~/.config/conky/plots/77-20.png -p 0,860 -s 380x190}${endif}
${if_updatenr 78}${image ~/.config/conky/plots/78-15.png -p 0,860 -s 380x190}${endif}
${if_updatenr 79}${image ~/.config/conky/plots/79-10.png -p 0,860 -s 380x190}${endif}
${if_updatenr 80}${image ~/.config/conky/plots/80-5.png -p 0,860 -s 380x190}${endif}
${if_updatenr 81}${image ~/.config/conky/plots/81-0.png -p 0,860 -s 380x190}${endif}</code></pre>

<p>Here <code class="inline">${if_updatenr ...} ... ${endif}</code> does all the heavy lifting: "updatenr" increments at each refresh until it reaches the highest value used in the file (here 81) and then starts again at 1. Simple enough.</p>

<p>Eventually all R scripts are bundled in a single bash script (i. e. <code class="inline">/usr/bin/Rscript ~/.config/conky/coronaberlin.R; /usr/bin/Rscript ~/.config/conky/maunaloa.R; ...</code>) that I run every 3 hours with <a href="https://en.wikipedia.org/wiki/Cron">cron</a> (i. e. <code class="inline">0 */3 * * * ~/.config/conky/rscripts.sh</code>). And so here is the (very humble) result (captured with <a href="https://linuxecke.volkoh.de/vokoscreen/vokoscreen.html">VokoscreenNG</a>):</p>

<video width="1000" controls=""><source src="vid/conky.mp4" type="video/mp4"/></video><br/></div></description><category>Programming</category><category>R</category></item>
<item><title>AoC2020 Days 19 to 24</title><link>http://plannapus.github.io/blog/2020-12-24.html</link><pubDate>2020-12-24</pubDate><description><div class="blog-content">
				Today was the penultimate day of the <a href="http://adventofcode.com/2020">Advent of Code 2020</a>. As the difficulty increased significantly in the last week I ended up spending a larger amount of time solving the puzzles, thus leaving me less time to talk about them here.
				<p>First of all: I am very happy with myself as I managed so far to solve every puzzle, thus beating my record from last year. Only one sub-puzzle took me more than a day to solve (and it turned out it was because of a silly mistake, as i actually had the correct algorithm on the first day).</p>
				<p>To recap quickly (click on the numbers to jump to the adequate section):
				</p><ul><li>Day <a href="#day19">19</a> was a regular expression eldritch nightmare (including recursion).</li>
					<li>Day <a href="#day20">20</a> was in my opinion this year's toughest challenge (so far) consisting on solving a jigsaw puzzle where pieces can be flipped and rotated in any direction, and when solved finding how many times a pattern arised in the resulting picture.</li>
					<li>Day 21 was fairly easy and reminiscent of the puzzle from day 16 (in fact I recycled part of my code).</li>
					<li>Day <a href="#day22">22</a> was a game of <a href="https://en.wikipedia.org/wiki/War_(card_game)">"Bataille"</a> where recursive subgames were needed to solved situation in which the card values are lower than the number of cards in the stack.</li>
					<li>Day <a href="#day23">23</a> was another toughie where the naive solution (i.e. the one I used for part 1) would run for weeks to solve part 2.</li>
					<li>Day <a href="#day24">24</a> finally was an hexagonal game of life! Very fun.</li>
				</ul><p id="day19">In day <a href="http://adventofcode.com/2020/day/19">19</a> challenge we add a series of self-referencing rules like rule 0 is 1 2, rule 1 is 2 | 3, rule 2 "a" and rule 3 "b", which means rule 1 is actually "a" or "b" and rule 0 "aa" or "ba". Check the original website for a better and more complete explanation. The easy way to solve it was to recursively replace the "rules" by their regex version. But in part 2, two of the rules were self-referecing e. g. rule 8 is 11 | 8 11 for instance. I ended up doing the lazy variation to arbitrarily set a maximum recursion level and just write it by hand, something like:
</p><pre><code>(42 | 42 (42 | 42 (42 | 42 (42 | 42 (42 | 42 (42 | 42 (42 | 42 (42 | 42 (42 | 42 (42 | 42 (42 | 42 (42)*)*)*)*)*)*)*)*)*)*)*)</code></pre>
				<p>But I did dscover that there is indeed a possibility of recursion in perl-style regex: <code class="inline">(?R)</code> or <code class="inline">(?n)</code> where n is the parentheses level or the recursive section. Here it was too tough to implement though is the recursive part of a small section in a 1000+ character regex.</p>
				<p id="day20">Day <a href="http://adventofcode.com/2020/day/20">20</a> was super tough. It took me an extra day to figure out because after 6h of working at it it still didn t return the correct value so I gave up, and the next day when I came back at it, I just printed the in-between steps and saw where it failed: I just flipped to expression around. Once resolved it went smoothly and I found the correct solution. Here, first, the part where we solve the jigsaw puzzle (each tile has a border in common with another one, we merge them and then get rid of their border):</p>

<pre><code class="r">options(digits=22)
input &amp;lt;- readLines("input20.txt")
tiles &amp;lt;- list()
n &amp;lt;- 1
for(i in 1:144){
  tiles[[i]] &amp;lt;- list()
  tiles[[i]]$tile &amp;lt;- do.call(rbind,strsplit(input[n+1:10],""))
  tiles[[i]]$nb &amp;lt;- as.integer(gsub("^Tile ([0-9]+):$","\\1",input[n]))
  n &amp;lt;- n+12
}

sides &amp;lt;- lapply(tiles,function(x)c(paste(x$tile[1,],collapse=""),
                                   paste(x$tile[,10],collapse=""),
                                   paste(x$tile[10,],collapse=""),
                                   paste(x$tile[,1],collapse="")))

for(i in 1:144){
# Super messy but basically for each tile create a list with the tile pattern, its ID,
# its borders as character strings, and a matrix of matching tiles
# (with tile ID, which side of current tile match and which side of target tile match)
  tiles[[i]]$sides &amp;lt;- sides[[i]]
  tiles[[i]]$matching_sides &amp;lt;- shttps://raw.githubusercontent.com/plannapus/Advent_of_Code/master/2020/visualisations/map20.pngapply(sides[[i]],function(x)x%in%unlist(sides[-i])|intToUtf8(rev(utf8ToInt(x)))%in%unlist(sides[-i]))
  tiles[[i]]$n_matching &amp;lt;- sum(sapply(sides[[i]],function(x)x%in%unlist(sides[-i])|intToUtf8(rev(utf8ToInt(x)))%in%unlist(sides[-i])))
  y &amp;lt;- sides[[i]]
  mat &amp;lt;- lapply(tiles[-i],function(X){
    m &amp;lt;-sapply(X$sides,function(x)x==y|intToUtf8(rev(utf8ToInt(x)))==y)
    if(any(m)){
      return(cbind(X$nb,which(m,arr.ind=T)))
    }else{return(NULL)}
  })
  tiles[[i]]$matched &amp;lt;- do.call(rbind,mat)
}

reorient &amp;lt;- function(tile,side,previous,from="left"){
# The most important part of the code: rotate and flip the tile to fit in the jigsaw
  if(from=="left"){
    w &amp;lt;- which(c(tile$side[side]==previous$side[2],intToUtf8(rev(utf8ToInt(tile$side[side])))==previous$side[2]))
    if(side==4){
      if(w!=1){
        tile$tile &amp;lt;- tile$tile[10:1,]
      }
    }
    if(side==2){
      if(w==1){
        tile$tile &amp;lt;- tile$tile[,10:1]
      }else{
        tile$tile &amp;lt;- tile$tile[10:1,10:1]
      }
    }
    if(side==1){
      if(w==1){
        tile$tile &amp;lt;- t(tile$tile)
      }else{
        tile$tile &amp;lt;- t(tile$tile)[10:1,]
      }
    }
    if(side==3){
      if(w==1){
        tile$tile &amp;lt;- t(tile$tile)[,10:1]
      }else{
        tile$tile &amp;lt;- t(tile$tile)[10:1,10:1]
      }
    }
  }
  if(from=="top"){
    w &amp;lt;- which(c(tile$side[side]==previous$side[3],intToUtf8(rev(utf8ToInt(tile$side[side])))==previous$side[3]))
    if(side==1){
      if(w!=1){
        tile$tile &amp;lt;- tile$tile[,10:1]
      }
    }
    if(side==2){
      if(w==1){
        tile$tile &amp;lt;- t(tile$tile)[10:1,]
      }else{
        tile$tile &amp;lt;- t(tile$tile)[10:1,10:1]
      }
    }
    if(side==3){
      if(w==1){
        tile$tile &amp;lt;- tile$tile[10:1,]
      }else{
        tile$tile &amp;lt;- tile$tile[10:1,10:1]
      }
    }
    if(side==4){
      if(w==1){
        tile$tile &amp;lt;- t(tile$tile)
      }else{
        tile$tile &amp;lt;- t(tile$tile)[,10:1]
      }
    }
  }
  # Recompute sides and matching tiles:
  tile$sides &amp;lt;- c(paste(tile$tile[1,],collapse=""),
                  paste(tile$tile[,10],collapse=""),
                  paste(tile$tile[10,],collapse=""),
                  paste(tile$tile[,1],collapse=""))
  mat &amp;lt;- lapply(tiles,function(X){
    m&amp;lt;-sapply(X$sides,function(x)x==tile$sides|intToUtf8(rev(utf8ToInt(x)))==tile$sides)
    if(any(m)){
      return(cbind(X$nb,which(m,arr.ind=T)))
    }else{return(NULL)}
  })
  mat &amp;lt;- do.call(rbind,mat)
  tile$matched &amp;lt;- mat[mat[,1]!=tile$nb,]
  tile
}

map &amp;lt;- matrix(nrow=8*12,ncol=8*12)
nbs &amp;lt;- sapply(tiles,function(x)x$nb)
used &amp;lt;- matrix(nr=12,nc=12)
modtiles &amp;lt;- tiles
for(j in 1:12){
  if(j==1){ #First tile is a corner tile (only has 2 matching sides)
    ind &amp;lt;- tiles[which(n_matching_sides==2)][[1]]$nb
    first_tile &amp;lt;- tiles[nbs==ind][[1]]
    first_tile$tile &amp;lt;- first_tile$tile[10:1,10:1]
    first_tile$sides &amp;lt;- c(paste(first_tile$tile[1,],collapse=""),
                          paste(first_tile$tile[,10],collapse=""),
                          paste(first_tile$tile[10,],collapse=""),
                          paste(first_tile$tile[,1],collapse=""))
    mat &amp;lt;- lapply(tiles,function(X){
      m &amp;lt;-sapply(X$sides,function(x)x==first_tile$sides|intToUtf8(rev(utf8ToInt(x)))==first_tile$sides)
      if(any(m)){
        return(cbind(X$nb,which(m,arr.ind=T)))
      }else{return(NULL)}
    })
    mat &amp;lt;- do.call(rbind,mat)
    first_tile$matched &amp;lt;- mat[mat[,1]!=first_tile$nb,]
    map[1:8,1:8]&amp;lt;-first_tile$tile[2:9,2:9]
    used[1,1]&amp;lt;-ind
    lasttile &amp;lt;-first_tile
    modtiles[nbs==lasttile$nb][[1]] &amp;lt;- lasttile
    for(i in 1:11){ #First row.
      m &amp;lt;- lasttile$matched
      M &amp;lt;- m[m[,2]==2,,drop=FALSE]
      M &amp;lt;- M[!M[,1]%in%used,]
      nexttile &amp;lt;- tiles[nbs==M[1]][[1]]
      nexttile &amp;lt;- reorient(nexttile,M[3],lasttile,"left")
      map[(j-1)*8+1:8,i*8+1:8]&amp;lt;-nexttile$tile[2:9,2:9]
      used[j,i+1] &amp;lt;- nexttile$nb
      lasttile &amp;lt;- nexttile
      modtiles[nbs==lasttile$nb][[1]] &amp;lt;- lasttile
    }
  }else{ # Next rows, first tile: check the top one
    top &amp;lt;- used[j-1,1]
    lasttile &amp;lt;- modtiles[nbs==top][[1]]
    m &amp;lt;- lasttile$matched
    M &amp;lt;- m[m[,2]==3,,drop=FALSE]
    M &amp;lt;- M[!M[,1]%in%used,]
    nexttile &amp;lt;- tiles[nbs==M[1]][[1]]
    nexttile &amp;lt;- reorient(nexttile,M[3],lasttile,"top")
    map[(j-1)*8+1:8,1:8]&amp;lt;-nexttile$tile[2:9,2:9]
    used[j,1] &amp;lt;- nexttile$nb
    lasttile &amp;lt;- nexttile
    modtiles[nbs==lasttile$nb][[1]] &amp;lt;- lasttile
    for(i in 1:11){ #Other tiles, check left and top tiles.
      m &amp;lt;- lasttile$matched
      top &amp;lt;- used[j-1,i+1]
      m2 &amp;lt;- modtiles[nbs==top][[1]]$matched
      M &amp;lt;- m[m[,1]%in%m2[,1],,drop=FALSE]
      M &amp;lt;- M[!M[,1]%in%used,]
      nexttile &amp;lt;- tiles[nbs==M[1]][[1]]
      nexttile &amp;lt;- reorient(nexttile,M[3],lasttile,"left")
      map[(j-1)*8+1:8,i*8+1:8]&amp;lt;-nexttile$tile[2:9,2:9]
      used[j,i+1] &amp;lt;- nexttile$nb
      lasttile &amp;lt;- nexttile
      modtiles[nbs==lasttile$nb][[1]] &amp;lt;- lashttps://raw.githubusercontent.com/plannapus/Advent_of_Code/master/2020/visualisations/map20.pngttile
    }
  }
}</code></pre>

<p>The pattern to find in the final reconstructed map is a "sea dragon":</p>

<pre><code>                  #
#    ##    ##    ###
 #  #  #  #  #  #   </code></pre>

<p>As he could be in any direction in the map, here is the code to find it:</p>

 <pre><code class="r">monster &amp;lt;- "                  # \n#    ##    ##    ###\n #  #  #  #  #  #   "
monster &amp;lt;- do.call(rbind,strsplit(el(strsplit(monster,"\n")),""))
mask &amp;lt;- monster=="#"
map2 &amp;lt;- map
for(i in 1:94){
  for(j in 1:77){
    if(all(map[i+0:2,j+0:19][mask]=="#")) map2[i+0:2,j+0:19][mask]&amp;lt;-"O"
    if(all(map[i+0:2,j+0:19][mask[,20:1]]=="#")) map2[i+0:2,j+0:19][mask[,20:1]]&amp;lt;-"O"
    if(all(map[i+0:2,j+0:19][mask[3:1,]]=="#")) map2[i+0:2,j+0:19][mask[3:1,]]&amp;lt;-"O"
    if(all(map[i+0:2,j+0:19][mask[3:1,20:1]]=="#")) map2[i+0:2,j+0:19][mask[3:1,20:1]]&amp;lt;-"O"
    if(all(map[j+0:19,i+0:2][t(mask)]=="#")) map2[j+0:19,i+0:2][t(mask)]&amp;lt;-"O"
    if(all(map[j+0:19,i+0:2][t(mask)[20:1,]]=="#")) map2[j+0:19,i+0:2][t(mask)[20:1,]]&amp;lt;-"O"
    if(all(map[j+0:19,i+0:2][t(mask)[,3:1]]=="#")) map2[j+0:19,i+0:2][t(mask)[,3:1]]&amp;lt;-"O"
    if(all(map[j+0:19,i+0:2][t(mask)[20:1,3:1]]=="#")) map2[j+0:19,i+0:2][t(mask)[20:1,3:1]]&amp;lt;-"O"
  }
}
</code></pre>

<p>For reference, here is a visualization of the result:</p>

<img class="current" src="https://raw.githubusercontent.com/plannapus/Advent_of_Code/master/2020/visualisations/map20.png" width="800"/><p id="day22">As mentioned above, day <a href="http://adventofcode.com/2020/day/22">22</a> is a card game, basically "War", but with slightly different rules: if a game is in an identical situation as a previous one, the game is forfeited and player 1 wins (to avoid infinite loops), if one of the two cards played is larger than the size of that player card stack, whomever as the larger value wins the fold and place both cards at the bottom of their stack (winning card first) as in War, but if both card values are at most equal to the size of their respective card stack, a subgame starts (with same conditions) where each player plays with a card stack of length n where n was the value of the played card. Whomever wins that subgame, wins the fold and place both cards at the bottom of their stack (winning card first). My code works but is clearly suboptimal, i. e. very slow. Here it is nonetheless:</p>

<pre><code class="r">input &amp;lt;- readLines("input22.txt")
w &amp;lt;- grep("Player",input)
p1 &amp;lt;- scan(text=paste(input[(w[1]+1):(w[2]-2)],collapse="\n"))
p2 &amp;lt;- scan(text=paste(input[(w[2]+1):length(input)],collapse="\n"))
game &amp;lt;- function(p1,p2){
  history = list()
  n = 1
  history[[n]]&amp;lt;-c(paste(p1,collapse=","),paste(p2,collapse=","))
  while(length(p1)&amp;amp;length(p2)){
    n = n+1
    x = p1[1]
    y = p2[1]
    p1 = p1[-1]
    p2 = p2[-1]
    if(length(p1)&amp;gt;=x&amp;amp;length(p2)&amp;gt;=y){
      subg = game(p1[1:x],p2[1:y]) #Recursion!
      if(subg$winner==1){
        p1 = c(p1,x,y)
      }else{
        p2 = c(p2,y,x)
      }
    }else{
      if(x&amp;gt;y){
        p1 = c(p1,x,y)
      }else{
        p2 = c(p2,y,x)
      }
    }
	#Check if situation existed before. Thanks to scoping it is limited to the current game or subgame.
    history[[n]] = c(paste(p1,collapse=","),paste(p2,collapse=","))
    check = sapply(history[-n],function(x)identical(x,history[[n]])|identical(rev(x),history[[n]]))
    if(any(check)){
      return(list(winner=1, p1=p1, p2=p2))
    }
  }
  return(list(winner=ifelse(length(p1),1,2), p1=p1, p2=p2))
}
game(p1,p2)</code></pre>


				<p id="day23">In day <a href="http://adventofcode.com/2020/day/23">23</a> challenge, we have a cyclic serie of integer. Each turns n, the integers in position n+1 to n+3 are taken out, and then place after the integer in the remaining serie that is immediately lower than the integer at place n. Simple enough. Part 1 consisted of doing that on a serie of 9 integers, 100 times. But then in part 2 we had to do that ten million times on a serie of a million integer. Needless to say the naive solution I used in part 1 (i. e. actually having a vector representing the serie and indexing it, splitting in, and rebuilding it every turn) was not good for part 2 (it would have needed 6 days to run). The trick was to make a vector where the index is the integer values, and the actual cell content list the direct clockwise neighbour. Thus we only index that vector, and replace a couple values every turn (the neighbour of the value at n become the value at n+4, the neighbour of the value that is immediately lower than the value at n become n+1 and the neighbour of n+3 become the former neighbour of the value "immediately lower than the value at n"). Here is what it looks like:</p>

<pre><code class="r">input &amp;lt;- 157623984
input &amp;lt;- c(1,5,7,6,4,3,9,8,4)
input &amp;lt;- c(input,10:1000000)
#Original neighbour list:
neighbours &amp;lt;- c(sapply(1:10,function(x)input[which(input==x)+1]),12:1000000,input[1])
start &amp;lt;- input[1]
for(i in 1:10000000){
  s1 &amp;lt;- neighbours[start]
  s2 &amp;lt;- neighbours[s1]
  s3 &amp;lt;- neighbours[s2]
  s4 &amp;lt;- neighbours[s3]
  neighbours[start]&amp;lt;-s4
  r &amp;lt;- ifelse(start-1,start-1,1000000)
  while(r%in%c(s1,s2,s3)) r &amp;lt;- ifelse(r-1,r-1,1000000)
  r1 &amp;lt;- neighbours[r]
  neighbours[r] &amp;lt;- s1
  neighbours[s3] &amp;lt;- r1
  start &amp;lt;- s4
}</code></pre>

				<p id="day24"> Day <a href="http://adventofcode.com/2020/day/24">24</a> as mentioned was an hexagonal Game of Life. As we already did a 3D Game of Life on <a href="2020-12-18.html">day 17</a>, I actually was able to recycle that code, because I used cubic coordinates to identify the hexagonal cells, thanks to a neat <a href="https://www.redblobgames.com/grids/hexagons/#coordinates-cube">website teaching about hexagonal grids</a> (for video game designers).</p>
				<img class="current" src="img/cubic.png" width="400"/><p class="caption">Concept of cubic coordinates for hexagonal grids, from <a href="https://www.redblobgames.com/grids/hexagons/#coordinates-cube">RedBlobGames.com</a></p>
				<p>Part 1 was giving a list of direction instructions to find the first batch of black tiles (all tiles start white and a set of directions lead to a tile that needs to be "flipped", i. e. turned black if white or white if black):</p>
				<pre><code class="r">input &amp;lt;- readLines("input24.txt")
input &amp;lt;- gsub("([ew])","\\1 ",input) #Add a space after w or e to separate the directions
input &amp;lt;- strsplit(input," ")
dif &amp;lt;- as.data.frame(do.call(rbind,list(e = c(1,-1,0), #Coordinates increment depending on direction
                   w = c(-1,1,0),
                   sw = c(-1,0,1),
                   nw = c(0,1,-1),
                   se = c(0,-1,1),
                   ne = c(1,0,-1))))
for(i in seq_along(input)){
  tab &amp;lt;- table(factor(input[[i]],c("e","w","sw","nw","se","ne")))
  tile &amp;lt;- colSums(tab*dif) #Just have to multiply the number of times we are going east, southeast, etc. with the direction increment.
  if(i!=1){
    if(paste(tile,collapse="_")%in%apply(flipped,1,paste,collapse="_")){
      flipped &amp;lt;- flipped[apply(flipped,1,paste,collapse="_")!=paste(tile,collapse="_"),] #If already in, take out
    }else{
      flipped &amp;lt;- rbind(flipped,tile) #If not, add in
    }
  }else{
    flipped &amp;lt;- matrix(tile,ncol=3)
  }
}</code></pre>

<p>Afterwards we reuse day 17's code but with a different way to compute the neighbouring cells based on the cubic coordinate system. Luckily, it's already there, in object <code class="inline">dif</code>:</p>
<pre><code class="r">colnames(flipped) &amp;lt;- c("x","y","z")
tiles &amp;lt;- as.data.frame(flipped)
tiles$value &amp;lt;- 1
neighbours &amp;lt;- function(m){ #Returns a list of neighbours for a given point or a full region
  M &amp;lt;- as.matrix(m[,1:3])
  # Instead of adding the expand.grid matrix from day 17
  # we directly add dif, the matrix of coordinates increment per direction
  # in the cubic coordinate system for hexagonal grid
  l &amp;lt;- do.call(rbind,lapply(seq_len(nrow(m)),function(i)t(M[i,]+t(dif))))
  l &amp;lt;- l[!duplicated(l),]
  data.frame(x=l[,1],y=l[,2],z=l[,3])
}
cat("Day 0: ",nrow(tiles),"\n",sep="")
for(i in 1:100){ #From now on, literally the same code as day 17
  p &amp;lt;- neighbours(tiles)
  p$value &amp;lt;- 0
  coords &amp;lt;- apply(tiles[,1:3],1,paste,collapse=",")
  for(j in 1:nrow(p)){
    content &amp;lt;- table(sapply(apply(neighbours(p[j,]),1,paste,collapse=","),function(x)factor(ifelse(x%in%coords,tiles$value[coords==x],"."),levels=c(0,1))))
    this_cube &amp;lt;- paste(p[j,1:3],collapse=",")
    here &amp;lt;- ifelse(this_cube%in%coords,tiles$value[coords==this_cube],0)
    if(here==1&amp;amp;content["1"]%in%1:2){ # Conditions here are different from day 17 and closer to actual GoL
      p$value[j] &amp;lt;- 1
    }else if(here==0&amp;amp;content["1"]==2){
      p$value[j] &amp;lt;- 1
    }
  }
  tiles &amp;lt;- p[p$value==1,]
  cat("Day ",i,": ",nrow(tiles),"\n",sep="\n")
}</code></pre>

			</div></description><category>Programming</category><category>R</category></item>
<item><title>AoC2020 Days 17 and 18</title><link>http://plannapus.github.io/blog/2020-12-18.html</link><pubDate>2020-12-18</pubDate><description><div class="blog-content">
				OK this time, the difficulity level went up a notch! To summarize briefly, day <a href="http://adventofcode.com/2020/day/17">17</a> was a 3 and 4D extension on <a href="https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life">Conway's Game of Life</a> while day <a href="http://adventofcode.com/2020/day/18">18</a> consisted in performing basic arithmetic... but with different precedence order: in part 1 there was no precedence order, i. e. 1+2*3 resolves to 3*3=9; and in part 2 addition took precedence over multiplication so 3*2+1 would also resolve to 9; but in both cases parentheses still enforce precedence, meaning 2 + (3 * 4 +5) * 6 would resolve to 2 + (3 * 9) *6 =  2 + 27 * 6 = 29 * 6 = 174. The way I proceeded for day 18 was to use regex to identify sections of the input equations that needed to be solved first and then evaluate them separately and replacing the original bit by its local solution.

				<p>The workhouses of that solution are two functions, one to evaluate and the second to replace the original bits:</p>
				<pre><code class="r">input &amp;lt;- readLines("input18.txt") #Read in the equations
input &amp;lt;- paste0("(",input,")") #Add parentheses around the full equations (probably not necessary but I prefer)
evaluate_unit &amp;lt;- function(x){ 
# x is the list of unit blocks for a given equations
# where the "precedence" is just the order of operation, 
# i. e. 3 * 5 * 2 or 4 + 3 + 6 + 7, etc. in part 2 
# or 2 + 3 * 4 etc. in part 1.
  if(length(x)){ #In some rounds of the loop there won t be any new units
    y &amp;lt;- strsplit(x," ") #Split by spaces
    for(i in seq_along(y)){ #For each units
      while(length(y[[i]])&amp;gt;1){ #while there is still elements left
        z &amp;lt;- eval(parse(text=paste(y[[i]][1:3],collapse=""))) #Collapse by groups of 3 (nb, op, nb) and evaluate
        y[[i]] &amp;lt;- c(z,y[[i]][-(1:3)]) #replace the evaluated bits by their solution
      }
    }
    return(y)
  }else{
    return(NA)
  }
}
replace_unit &amp;lt;- function(j, input, innermost, w){
# j is the index
# input the list of equations in their current state
# innermost is the list of solved unit blocks
# w is the result of gregexpr i. e. the position in the equation string of the blocks
    inp &amp;lt;- input[j]
    inn &amp;lt;- innermost[[j]]
    W &amp;lt;- w[[j]] #Positions of the first character of the blocks
    l &amp;lt;- attr(W,"match.length") # Length of the blocks
    L &amp;lt;- W+l # Positions of the last character of the blocks
    for(i in seq_along(inn)){ #For each block
      if(!is.na(inn[i])){ #If non empty
        inp &amp;lt;- paste0(substr(inp,1,W[i]-1),inn[i],substr(inp,L[i],nchar(inp))) #Replace
        W &amp;lt;- W - l[i] + nchar(inn[i]) #Change the position of next block accordingly
        L &amp;lt;- L - l[i] + nchar(inn[i])
      }
    }
    inp&amp;lt;-gsub("\\((\\d+)\\)","\\1",inp) #Rare case in which it results in things like "(549)"
    inp
}
while(!all(grepl("^\\d+$",input))){ #The actual loop (for part 2), run until all equations are replaced by a single integer
  p &amp;lt;- gregexpr("[0-9][0-9 +]+[0-9]",input) #Grabs addition blocks
  additions &amp;lt;- regmatches(input,p)
  additions &amp;lt;- lapply(additions,evaluate_unit) #Evaluate them
  input &amp;lt;- sapply(seq_along(input),function(j)replace_unit(j,input,additions,p)) #Replace them
  input &amp;lt;- gsub("\\(([0-9]+)\\)","\\1",input) #Make sure you dont have single integers stuck between parentheses
  w&amp;lt;-gregexpr("\\(([^()+]+)\\)",input) #Grab parenthese blocks
  innermost &amp;lt;- regmatches(input,w)
  innermost &amp;lt;- lapply(innermost,function(x)gsub("[()]","",x)) #Get rid of parentheses
  innermost &amp;lt;- lapply(innermost,evaluate_unit) #Evaluate
  input &amp;lt;- sapply(seq_along(input),function(j)replace_unit(j,input,innermost,w)) #Replace
}</code></pre>

As for day 17, i decided, instead of making a N-dimensional array actually representing the problem, to just stores the "visited" coordinates and their state. The advantage was that the code didn t need a lot of changes to go from 3d to 4d, however my code is SUPER slow (it resolved part 2 in a staggering two hours!). I'm sure there is a better solution but I couldn't figure it out. So here is my suboptimal solution (for the 4d part):

<pre><code class="r">library(reshape)
input &amp;lt;- readLines("input17.txt")
map &amp;lt;- do.call(rbind,strsplit(input,""))
m &amp;lt;- melt(map)
m &amp;lt;- data.frame(x=m$X1,y=m$X2,z=0,w=0,value=m$value) #Contains coordinates of points visited and their state
neighbours &amp;lt;- function(m){ #Returns a list of neighbours for a given point or a full region
  mask &amp;lt;- expand.grid(-1:1,-1:1,-1:1,-1:1)
  M &amp;lt;- as.matrix(m[,1:4])
  l &amp;lt;- do.call(rbind,lapply(seq_len(nrow(m)),function(i)t(M[i,]+t(mask))))
  l &amp;lt;- l[!duplicated(l),]
  data.frame(x=l[,1],y=l[,2],z=l[,3],w=l[,4])
}
for(i in 1:6){
  p &amp;lt;- neighbours(m) #Grabs list of neighbours of the currently visited region
  p$value &amp;lt;- NA #No value at start
  coords &amp;lt;- apply(m[,1:4],1,paste,collapse=",") #Coordinates as string for each points that already have a value
  for(j in 1:nrow(p)){
    #For each new coordinates check content of known neighbours
    content &amp;lt;- table(sapply(apply(neighbours(p[j,]),1,paste,collapse=","),function(x)factor(ifelse(x%in%coords,m$value[coords==x],"."),levels=c(".","#"))))
    #Coords of point of interest
    this_cube &amp;lt;- paste(p[j,1:4],collapse=",")
    #Current value (if known check, if not blank)
    here &amp;lt;- ifelse(this_cube%in%coords,m$value[coords==this_cube],".")
    if(here=="#"&amp;amp;content["#"]%in%3:4){ #Apply rules, i. e. if ON and 2 or 3 neighbours are ON, keep ON
      p$value[j] &amp;lt;- "#"
    }else if(here=="."&amp;amp;content["#"]==3){ # If OFF and exactly 3 neighbours are ON, turn ON 
      p$value[j] &amp;lt;- "#"
    }else{ # Else turn OFF
      p$value[j] &amp;lt;- "."
    }
  }
  m&amp;lt;-p #Replace old map with new one
}</code></pre>

<p>As usual you can check the full code <a href="http://github.com/plannapus/Advent_of_Code/2020">in my repository</a>.</p>
			</div></description><category>Programming</category><category>R</category></item>
<item><title>AoC2020 Days 13 to 16</title><link>http://plannapus.github.io/blog/2020-12-16.html</link><pubDate>2020-12-16</pubDate><description><div class="blog-content">Here we are in lockdown again. Luckily the advent of code does brighten the day somewhat. And some challenges were noticeably harder this week (though still not remotely as hard as last year I must say). It started with day <a href="http://adventofcode.com/2020/day/13">13</a>: the given input was a list of buses (whose names correspond to their periodicities), and find out a time t from which each bus arrives at t+n where n is their place in the list. As someone pointed out on <a href="http://reddit.com/r/adventofcode">reddit</a>, this problem is known as the Chinese Remainder Problem. Though one (like me) didn't need to know that to solve it thankfully. Brute-focing it (by trying all integers until we find one) was clearly not the best idea (as you will see with the solution) so one had to find a way to solve it faster. And the solution ended up fairly trivial: finding an integer i at which the first bus arrive, and then incrementing only by the periodicity of that bus until we end up on a time at which the second one arrive, then incrementing by the product of both, etc.
				<pre><code class="r">options(digits=22)
input &amp;lt;- readLines("input13.txt")
bus &amp;lt;- as.integer(el(strsplit(input[2],",")))
spot &amp;lt;- which(!is.na(bus))-1 # -1 one as R vectors are 1-based but we need 0-based spots
bus &amp;lt;- bus[!is.na(bus)]
t &amp;lt;-0
pace &amp;lt;- 1
for(i in seq_along(bus)){
  while((t+spot[i])%%bus[i]) t &amp;lt;- t+pace
  pace &amp;lt;- pace*bus[i]
}
t
#471793476184394</code></pre>

<p>Note that <code class="inline">options(digits=22)</code> again that I end up using on every odd challenge those days. Day <a href="http://adventofcode.com/2020/day/14">14</a> was again fairly difficult. The first part required reading a series of integers, transforming to a 36 bit length binary number, then using masks to force some positions into 1s or 0s, reconverting it to a base-10 integer and placing it in a specific place in "memory" (i. e. a master vector). Easy enough, but the second part required that operation to be done on the memory index rather than the value, and the mask rule changes to forcing 1s and Xs with Xs being both 1 and 0 (i. e. sending the value to several place in the master vector). As those numbers are fairly high it prohibits actually using a proper vector: the maximum length of a vector in R is 2^31 while here we can have indices as high as 2^35.</p>

<pre><code class="r">options(digits=22) #If it wasn't for reproducibility sake I would put it in my Rprofile at that point
input &amp;lt;- readLines("input14.txt")
mem &amp;lt;- c()
intTo36Bits &amp;lt;- function(x){ #Homemade binary transformation
  res &amp;lt;- c()
  for(i in 35:0){
    res &amp;lt;- c(res,x%/%(2^i))
    x &amp;lt;- x%%(2^i)
  }
  res
}

bitsToInt&amp;lt;- function(x)sum(x*2^(35:0)) #And the reverse

for(i in seq_along(input)){
  if(grepl("^mask",input[i])){
    mask &amp;lt;- el(strsplit(gsub("mask = ","",input[i]),"")) #Parsing the input
    w1 &amp;lt;- which(mask%in%"1")
    wx &amp;lt;- which(mask%in%"X")
  }else{
    dig &amp;lt;- as.integer(el(regmatches(input[i],gregexpr("\\d+",input[i])))) #Parsing the input
    index &amp;lt;- dig[1]
    value &amp;lt;- dig[2]
    b &amp;lt;- as.matrix(intTo36Bits(index))
    for(k in w1) b[k,]&amp;lt;-1
    for(k in wx){ #Duplicate the binary number and make one side 0s and the other 1s
      b &amp;lt;- cbind(b,b)
      b[k,] &amp;lt;- c(rep(0,ncol(b)/2),rep(1,ncol(b)/2)) #Put 0s in half of them and 1s in the other half
    }
    #Instead of a mostly-empty vector, a key-value matrix:
    mem &amp;lt;- rbind(mem,cbind(apply(b,2,bitsToInt),value))
    #We "overwrite" values by getting rid of older duplicates
    mem &amp;lt;- mem[!duplicated(mem[,1],fromLast=TRUE),]
  }
}
sum(mem[,2])
#3705162613854</code></pre>

<p>Funnily, day <a href="http://adventofcode.com/2020/day/15">15</a> required to do an opposite trick. We have a sequence of number where the u(n+1) is 0 if u(n) appears for the first time in the list or its age if it appeared before (i. e. n-m where m was the previous time it appeared). The standard one starting with 0, 0 is referred to as the <a href="https://oeis.org/A181391">Van Eck's sequence</a>, but here we each got a unique starter. The problem was to figure out which will be the 30 000 000nth integer in that sequence. Actually creating a sequence that long would take forever (increasing a vector's length in R is time consuming). What I ended up doing though was having a 30 000 000-long vector of "ages", that I updated each iteration with the most recent age for a given integer (given by the index), as looking up into a vector by its index is almost costless timewise, whereas checking the full stack of values for a condition is time-consuming.</p>
<pre><code class="r">input &amp;lt;- c(13,16,0,12,15,1)
last &amp;lt;- 0
current &amp;lt;- 7
maxn &amp;lt;- 30000000
age &amp;lt;- rep(NA,maxn)
age[1+input]&amp;lt;-seq_along(input) #The additional 1 is because we need a space for O
while(current&amp;lt;=maxn){
  x &amp;lt;- age[1+last]
  age[1+last] &amp;lt;- current
  if(is.na(x)){
    last &amp;lt;- 0
  }else{
    last &amp;lt;- current-x
  }
  current &amp;lt;- current + 1
}
which(age%in%maxn)-1
#2424</code></pre>

<p>Day <a href="http://adventofcode.com/2020/day/16">16</a> finally was way simpler and thus not really worth explaining here (all operations involved were trivial in R: checking whether numbers belongs to a list, checking which row only has a single TRUE value, etc.), but you can always check it out on <a href="http://github.com/plannapus/Advent_of_Code">my repository</a>.</p>

			</div></description><category>Programming</category><category>R</category></item>
<item><title>AoC2020 Days 8 to 12</title><link>http://plannapus.github.io/blog/2020-12-12.html</link><pubDate>2020-12-12</pubDate><description><div class="blog-content">
				Unfortunately I did not have much time to report on the last batch of puzzles. In part, because they became somewhat harder and thus needed more of my free time to be resovled :)
				<p>Day <a href="http://adventofcode.com/2020/day/8">8</a> required building an interpreter for a simple, 3-operations code (and finding the one instruction that needed to be modified for it not to loop indefinitely: interesting!); day <a href="http://adventofcode.com/2020/day/9">9</a> was fairly simple, as it was purely integer manipulation, though I did need to use package <code class="inline">bit64</code> as the integers were too large for base R which thus relied on floating-point integer (which is not precise enough naturally).</p>
				<p>Day <a href="http://adventofcode.com/2020/day/10">10</a> was the first one I wasn't able to "brute-force" and thus was a bit more interesting! Basically we had a chain of integer were each integer could only be separated by 1, 2 or 3; and we needed to figure out the number of possible paths between the smaller and the bigger one. At first i tried to solve that using <code class="inline">igraph</code> by turning the problem into a graph and calculating the number of possible paths but it crashed my laptop! As it turned out, it was solvable algebraically. None of the integer were separated by 2 meaning we had a serie of numbers separated by 1 or 3. The ones separated by 3 are not skippable given we can't connect to number separated by more than 3, so the problem in the hand was subsettable into figuring out the number of paths that was possible in each serie of numbers separated by only a single unit. There were no series longer than 5, as it turned out. So the number of possibilities were 7 for a series of 5 consecutive integers:</p>
				<pre><code>0 - 3 - 4 - 5 - 6 - 7 - 10
0 - 3 - 4 - 5 - 7 - 10
0 - 3 - 4 - 6 - 7 - 10
0 - 3 - 5 - 6 - 7 - 10
0 - 3 - 4 - 7 - 10
0 - 3 - 5 - 7 - 10
0 - 3 - 6 - 7 - 10</code></pre>

<p>4 for a series of 4 integers:</p>

				<pre><code>0 - 3 - 4 - 5 - 6 - 9
0 - 3 - 4 - 6 - 9
0 - 3 - 5 - 6 - 9
0 - 3 - 6 - 9</code></pre>

<p>2 for a series of 3:</p>

				<pre><code>0 - 3 - 4 - 5 - 8
0 - 3 - 5 - 8</code></pre>

<p>And that's all! Meaning the number of paths was 7^(# of series of 5)*4^(# of series of 4)*2^(# of series of 3). Or in R:</p>
				<pre><code class="r">options(digits=22) #Prevents R from displaying integers in scientific notation.
input &amp;lt;- scan("input10.txt")
chain &amp;lt;- c(0,sort(input),max(input)+3) #Minimum was 0 and Max was the max of the list plus 3
r &amp;lt;- rle(diff(chain)) #Run Length Encoding: returns a list of lengths and values, i. e. 1 1 1 1 becomes l=4, v=1
7^sum(r$l==4&amp;amp;r$v==1)*4^sum(r$l==3&amp;amp;r$v==1)*2^sum(r$l==2&amp;amp;r$v==1) #r$l==4 as we are working on differences, so 4 means 5 consecutive numbers</code></pre>

<p>The option to display a large amount of digits was necessary as the result (in my case) was a staggerring 193 434 623 148 032. No wonder the graph solution crashed!</p>
<p>Day <a href="http://adventofcode.com/2020/day/11">11</a> was frankly the hardest: a form of <a href="https://en.wikipedia.org/wiki/Conway%27s_Game_of_Life">game of life</a> mixed with "line-of-sight" computations on a 2D map. It took me quite a while (I had to refactor entirely the code of part 1 to be able to process part 2), but transforming the "line-of-sight" (a row, a column or a matrix diagonal in that case) into a character string and then using regex to analyse it turned out to be the easiest solution. The solution is a bit too tedious to share here however, but feel free to have a look in <a href="https://github.com/plannapus/Advent_of_Code/blob/master/2020/aoc11.R#L25">my repository</a>. Day <a href="http://adventofcode.com/2020/day/12">12</a> (i. e. today) finally was simpler: just take a series of instructions and translates that into coordinates. Not especially hard though the instructions on part 2 were not super clear (the rotation was ill defined I think).</p>
			</div></description><category>Programming</category><category>R</category></item>
<item><title>AoC2020 Days 5 to 7</title><link>http://plannapus.github.io/blog/2020-12-07.html</link><pubDate>2020-12-07</pubDate><description><div class="blog-content">
				Days <a href="http://adventofcode.com/2020/day/5">5</a> and <a href="http://adventofcode.com/2020/day/6">6</a> were pretty uneventful, still relatively easy; the first one solvable trivially by transforming the input into a binary sequence and translating it into an integer, the second one needed to be carefully parsed but once done was solved just by applying <code class="inline">table</code> to it, so nothing unsurmontable. Day <a href="http://adventofcode.com/2020/day/7">7</a> however was a notch more complicated, in that it needed recursion, which is always an head-scratcher.
				<p>Technically it was reminiscent from <a href="http://adventofcode.com/2019/day/14">day 14 of last year</a> but simpler: the input consisted of sentences like "light red bags contain 1 bright white bag, 2 muted yellow bags." and "faded blue bags contain no other bags." and one needed to figure out how many bags a "shiny gold" bag contained, which necessited to go through the whole "reaction" path (i. e. <code class="inline">a=2b+4c; c=3d+4b; etc.</code>). <a href="https://github.com/plannapus/Advent_of_Code/blob/master/2020/aoc07.R">For the parser</a> I used the same method as for <a href="2020-12-02.html">day 2</a> (i. e. <code class="inline">parse.one</code>) and then, lazily, use an operator that is normally a no-go in R: <code class="inline">repeat</code>. I am sure there would have been more elegant ways to do the job, but at least it worked and was fairly fast.</p>
				<pre><code class="r">input &amp;lt;- readLines("input07.txt")
res &amp;lt;- lapply(input,parse_reac) #parse_reac is the parser I made, check out in the github repo to read it. It's fairly tedious though.
# res is the parsed input, each element contains:
# an element "a" that is the containing color,
# "b" a dataframe of what is contained in "a", where n is the number of bags and col their color.
# If the bag contains no other bags, b is NULL.
step &amp;lt;- res[sapply(res,function(x)x$a=="shiny gold")][[1]]$b # I know it's ugly: go through all elements and return dataframe b if a is he color we need
step$n &amp;lt;- as.integer(step$n) # Why didn't I made the parser converts that to integer directly, i'll never know
step$end &amp;lt;- FALSE # This vector is where we'll say if we reached the end of a path (i. e. "contains no bags")
repeat{ # The dreaded repeat
  replacement &amp;lt;- data.frame(n=NULL, col=NULL, end=NULL) # Empty dataframe in which we will put the result
  for(i in 1:nrow(step)){
    if(!step$end[i]){ #If we didn't already reach the end of that particular path
      sub &amp;lt;- res[sapply(res,function(x)x$a==step$col[i])][[1]]$b
      if(!is.null(sub)){
        sub$n &amp;lt;- as.integer(sub$n)*step$n[i]
        sub$end &amp;lt;- FALSE
        step$end[i] &amp;lt;- TRUE
        #One needs to keep the step that is done because we need to count those bags too.
        replacement &amp;lt;- rbind(replacement, sub, step[i,])
      }else{
        step$end[i]&amp;lt;-TRUE
        replacement &amp;lt;- rbind(replacement, step[i,])
      }
    }else{
      replacement &amp;lt;- rbind(replacement, step[i,])
    }
  }
  step &amp;lt;- replacement
  if(all(step$end)){break} #If every path reached its end, we can stop
}
sum(step$n)</code></pre>
			
			</div></description><category>Programming</category><category>R</category></item>
<item><title>AoC2020 Days 3-4 and Quinces</title><link>http://plannapus.github.io/blog/2020-12-04.html</link><pubDate>2020-12-04</pubDate><description><div class="blog-content">
				Days 3 and 4 of the Advent of Code were still relatively trivial to solve. Day 3 was just playing with coordinates on a 2D discrete map while day 4 entailed more adventures in input parsing :)

				<p>This time the input was fairly randomly organised. Each set of data was on a random number of lines but separated from the next one by an empty line, and each set was composed of values in the form <code class="inline">key:value</code> and separated from the others by a space. I'm pretty sure it could have been entirely sparsed by regex but I was <span class="tooltip">feeling lazy<span class="tooltiptext">having woken up way earlier than usual</span></span>, so I used instead the standard tools (<code class="inline">scan, readLines, strsplit</code>) to create a list of named vectors. Named vectors are frankly rarely used nowadays in R but, maybe it is because I have been programming a lot in python in the last few years, I like to use them as they are R equivalent to python's dictionaries.</p>

<pre><code class="r">input &amp;lt;- readLines("input04.txt")
input[input==""]&amp;lt;-"\n" #First, replace the blank lines separating entries with a newline symbol
passports &amp;lt;- gsub("^ | $","",scan(text=paste(input,collapse=" "),what="",sep="\n")) #collapse all together so that the only newlines are the one separating entries, and reads it in again (plus clean the trailing whitespaces).
pass &amp;lt;- list() #Yes, I know, a loop, yawn.
for(i in seq_along(passports)){
  step &amp;lt;- do.call(rbind,strsplit(scan(text=passports[i],what="",sep=" "),":")) #Separate by spaces, then by colons
  pass[[i]]&amp;lt;-step[,2] #keep the value
  names(pass[[i]])&amp;lt;-step[,1] #and use the key as name
  }</code></pre>

				<p>Once this is done, figuring out the rest of the challenge was fairly trivial. The first part for instance was a one-liner:</p>

<pre><code class="R">#Count the number of passports having all the required fields:
sum(sapply(pass,function(x)all(c("byr","iyr","eyr","hgt","hcl","ecl","pid")%in%names(x))))</code></pre>

				<p>In culinary news, I made a quince pie the other day, "american-style". It wasn't that much of a success, so instead here's a recipe for my best quince-based dessert: my quince cheesecake. I have several variations on that theme but here is the one I made in October 2019 for <a href="https://volkanozen.github.io/">our student's</a> birthday.</p>
				<p>For the crunchy base: broken down biscuits, with butter and a bit of sugar (the biscuits are usually too sweet to begin with), as usual. Then a layer of quince compote (i. e. quinces cooked in a water and a tiny bit of sugar until they are mashed), and then the 'cheese' matrix: 4 eggs, a pot of crême fraîche, some mascarpone and some Rahmjoghurt (in France I would have used Faisselles and/or Petits-Suisses), with sugar (including vanillin sugar). Cooked 45min at around 180°.</p>

				<img class="current" src="img/quince_cheesecake.png" width="800"/><p class="caption">What remained after my working group ate the cake. It's a great frustration of mine to have a lab big enough to make cooking a cake worthwhile but so small that I still have tons of leftovers :)</p>
			</div></description><category>Programming</category><category>R</category><category>Cooking</category></item>
<item><title>AoC2020 Days 1-2 and Buckwheat Pancakes</title><link>http://plannapus.github.io/blog/2020-12-02.html</link><pubDate>2020-12-02</pubDate><description><div class="blog-content">
			As mentioned in the <a href="2020-11-30.html">previous post</a>, the Advent of Code 2020 started yesterday. As expected the first two days were quite simple (the challenges tend to ramp up pretty quickly though). <a href="https://adventofcode.com/2020/day/1">Day 1</a> consisted on a bit of integer manipulation, while <a href="https://adventofcode.com/2020/day/2">day 2</a> was about string manipulation. You can find my solutions in R in <a href="http://github.com/plannapus/Advent_of_Code/tree/master/2020">my code repository</a>.

			<p>One interesting bit is that, as often, most of the code on those simple challenges is spent on input parsing. In this case, in day 2 challenge, the input had a form as follows: </p>

				<pre><code>1-2 a: ahgtnkjz
3-12 j: asjdladajsdsjdaslj</code></pre>

			<p>Obviously, given the inconsistent column separator, the easiest here was to use a tailored regex. So I whipped up that ol' function from the help file of <a href="https://stat.ethz.ch/R-manual/R-devel/library/base/html/grep.html"><code class="inline">?regexpr</code></a> that I remembered from an old <a href="https://stackoverflow.com/a/29669403/1451109">StackOverflow answer of mine</a>:</p>
				<pre><code class="r">input &amp;lt;- readLines("input02.txt")
parse.one &amp;lt;- function(res, result) {
  m &amp;lt;- do.call(rbind, lapply(seq_along(res), function(i) {
    if(result[i] == -1) return("")
    st &amp;lt;- attr(result, "capture.start")[i, ]
    substring(res[i], st, st + attr(result, "capture.length")[i, ] - 1)
  }))
  colnames(m) &amp;lt;- attr(result, "capture.names")
  m
}
parsed &amp;lt;- regexpr("^(?&amp;lt;lb&amp;gt;[0-9]+)-(?&amp;lt;ub&amp;gt;[0-9]+) (?&amp;lt;let&amp;gt;[a-z]+): (?&amp;lt;p&amp;gt;[a-z]+)$", input, perl=TRUE)
tab &amp;lt;- parse.one(input,parsed)
tab &amp;lt;- as.data.frame(tab)</code></pre>

			<p>What <code class="inline">parse.one</code> does is just a modification of <code class="inline">regmatches</code> that allows the capture of "named" group, PERL-style; i. e. it allows me to use that weird regex trick of <code class="inline">(?&amp;lt;name&amp;gt;group)</code>. Once done and transformed into a data.frame, the columns are already named correctly and are instantly usable.</p>

			<img class="current" src="img/buckwheat_pancake.png" width="400"/><p class="caption">From my experience in the last 3 days, it tastes brilliant, either alone, or with some spicy sauce (sriracha) if in a savoury mood, or some Pflaumenmuss (if in a sweet mood).</p>

			<p>In other news, I've been cooking buckwheat pancakes for breakfast/lunch recently, and I am quite happy with the result. To 100g of buckwheat flour, I added an egg, 20cL of milk, 10cL of water, salt, pepper and a bit of dill. Once whipped up to a flowing but thick mixture, a small laddleful of it is placed at the center of a very hot pan. When bubbles appeared on the surface, flip and let cook another 30s/1min. </p>
			</div></description><category>Programming</category><category>R</category><category>Cooking</category></item>
<item><title>Advent of Code 2020</title><link>http://plannapus.github.io/blog/2020-11-30.html</link><pubDate>2020-11-30</pubDate><description><div class="blog-content">
				Tomorrow starts the best advent event of the year: the <a href="https://adventofcode.com/2020">Advent of Code</a>!
				<p>For those not familiar with the event, it is a programming contest, put together by Eric Wastl, where everyday a new, or in fact two new (solving the first one unlocks the second one), programming puzzles are proposed (at midnight UTC-5). They are language-agnostic, in fact they could in theory be solved manually (though in a very large amount of time). According to the leaderboard, at least 110&amp;amp;nbsp000 programmers around the world managed to solve the first puzzle last year. I participated <a href="https://github.com/plannapus/Advent_of_Code/tree/master/2019">last year</a> and really thought it was a great and fun way to train my problem-solving skills.</p>

				<img class="current" src="https://github.com/plannapus/Advent_of_Code/raw/master/2019/screenshot.png" width="800"/><p class="caption">Screenshot of my personal stats from last year's contest.</p>

				<p>All puzzles take an input file that is unique to the player so no possibilities to cheat! Naturally I am solving everything in R, though I'm thinking of using python eventually. As you can see on the screenshot they are <span class="tooltip">only 5 of the 49 puzzles<span class="tooltiptext">The 50th "star" is unlocked if the 49 preceeding puzzles are solved</span></span> I didn't manage to solve so I'm pretty happy about that. Also my ranking (given I play in general 3 hours after the contest is posted because of timezone differences) is not too bad either, I think.</p>
				<p>Last year contest revolved heavily around an <a href="https://github.com/plannapus/Advent_of_Code/blob/master/2019/intcode_fast_but_dirty.R">interpreter</a> for a made-up, self-modifying programming language and a lot of path-finding problems. The latter actually gave me some great insight of an issue I had in my professional life.</p>
				<p>I'm quite looking forward to this year's puzzles! I might even document here some of them, otherwise the code and all material necessary to make it reproducible will be on <a href="https://github.com/plannapus/Advent_of_Code/tree/master/2020">my github repo</a> as usual.</p>
			</div></description><category>Programming</category></item>
<item><title>Some fried rice recipe I started making during the first lockdown</title><link>http://plannapus.github.io/blog/2020-11-22.html</link><pubDate>2020-11-22</pubDate><description><div class="blog-content">
				As most people who ended home and aimless during the first lockdown, I cooked a bit more than usual and attempted new recipes. Now that we are in a second lockdown (albeit a so-called 'lite lockdown'), after almost a year of not leaving Berlin and not seeing my family I tend to crave for more rustic french cuisine, but I do occasionally cook those recipes I came up with during the first lockdown. Three of them were good enough for me to write them down: one is a fried rice recipe, while the two others are spring recipes so not necessarily interesting for the time being (a recipe of pasta with squids and another of an eggplant-based sauce for pasta as well).

				<img class="current" src="img/cookbook2.jpg" width="400"/><p class="caption">Ye olde <em>encyclopedia culinaria</em> page with the three recipes. If you have good eyes and understand french you can see the <span class="tooltip">two other recipes<span class="tooltiptext">or go to the last paragraph of this post</span></span>.</p>

				<p>The fried rice recipe is more a template of a recipe really, as I don't think I ever used twice the same set of ingredients despite having made myself this dish ten to fifteen times since last March. </p>

				<p>When I moved to Toulouse, I lived in a flat on top of a chinese restaurant (the <em>Phoenix</em>, which closed later and was replaced by a japanese tea house, the <em>Okini</em>, both were excellent and very welcoming). Among the many delicious meals they were making was the so-called cantonese rice, a dish of fried rice with eggs, peas, pork, etc. At the time I tried to reproduce it at home with various successes. But nowadays that my tastes changed a little, I wanted to revisit it. So here is what I came up with, with my apologies to China for bastardizing one of their excellent regional recipe.</p>

				<p>The idea is on one hand, in a frying pan, to sauté garlic, an onion and some vegetables (yesterday I used turnips, but it works well with cauliflowers, bell-peppers, carrots, broadbeans, peas, etc. As I said it's really just a template of a dish) and/or mushrooms, and eventually some meat, fish or seafood (yesterday I did it with shrimps but I tried monkfish and chicken in the past), all that in butter with ginger powder, salt, pepper and eventually a bunch of chilies. On the other hand, cook some rice in salted water as usual (preferentially a 'round' rice like arborio). When the vegetables soften, add a spoonful of soy sauce, and soon after crack an egg or two in it. Scramble the egg so that it coats the vegetables. When the rice is done and the liquid evaporated in the frying pan, add the rice to the vegetables, stir and let cook for 5-10min while stirring so that the rice doesn't stick too much. I then serve that in a bowl with a swirl of kecap manis and a swirl of sriracha.</p>

				<p>Writing this recipe down I realize that it is in fact a mix between the vague memory I have of the <em>Phoenix</em>'s cantonese rice and my sister-in-law's nasi goreng. The result is maybe not as good as any of those two, but I like it and it matches my current taste somehow.</p>

				<img class="current" src="img/friedrice.jpg" width="400"/><p class="caption">Yesterday's dish (shrimp, turnip and mushrooms). Not a good picture by any definition of it. Plus I made the mistake of cooking it with basmati rice this time, which I won't make again :)</p>

				<p>As for the two other recipes, here they are briefly:</p>
				<p>For the orrechiette with squids, cut the squids and some jalapeños in thin stripes. Sauté them with some garlic and a handful of black olives (pitted), a tomato and some basil. Season with black pepper, chili and eventually a dash of cumin. Mix with the cooked orrechiette. Simple enough.</p>

				<p>And finally the eggplant sauce: cut an eggplant in half, grate it until you reach the <span class="tooltip">rind<span class="tooltiptext">is it how it's called?</span></span>. Sauté an onion, some garlic, the eggplant 'flesh', some bellpeppers in stripes and half a zucchini cut thinly with a mandolin. Season with salt, pepper and chili. When the vegetables soften, add a tomato or two (skinned and/or crushed). Let cook for 5 to 10 minutes, and add a good dose of freshly grated parmesan. Halfway through the cooking process, add another clove of garlic, grated as well.</p>

			</div></description><category>Cooking</category></item>
<item><title>A look at the code behing the blog</title><link>http://plannapus.github.io/blog/2020-11-20.html</link><pubDate>2020-11-20</pubDate><description><div class="blog-content">
				This is going to be very meta, I'm afraid. In the <a href="2020-09-27.html">first post of this blog</a> I said that I wasn't sure why I was doing this. Two months later, I am still not entirely sure of my motivation (though being fairly obsessive it allows me to put all my thoughts somewhere instead of rehashing them constantly in my mind, so it does help with that) but I know that one of the small reason was to see if I was able to write down a very simple code to write a functional blog.

				<p>As you may have noticed reading this, this website is fairly minimalistic. The first reason is of course that I am not a designer, but the second and main reason is that my biggest issue with Web 2.0 is that it is bloated. If you are looking at the source code of most pages, it is way bigger than it should be, between all the unnecessary javascript calls, the opaque automatically built CSS (typical of blogging environment such as wordpress) , the google analytics and whatnots... It does annoy me primarily because all that extraneous content has a carbon footprint, but also because it obfuscates the web. So I have been doing simple, easily maintainable, functional pages using pure HTML and CSS (and a very tiny bit of javascript, when MathJax or a syntax highlighter is needed). But blogs are a tad more complex to maintain (because of the index, the categories etc.).</p>

				<p>So here is what I came up with.</p>

				<p>First the template for a blog page (that you can see in full <a href="https://github.com/plannapus/plannapus.github.io/blob/master/blog/template.html">here</a>) has two parts: the first one is the navigation part common to the rest of the website (the thing you see on the left) and the rest is the blogpost itself:</p>

				<pre><code class="html">&amp;lt;div class="main"&amp;gt;
&amp;lt;p class="blog-title"&amp;gt;&amp;lt;/p&amp;gt;
&amp;lt;p class="blog-date"&amp;gt;&amp;lt;/p&amp;gt;
&amp;lt;p class="blog-content"&amp;gt;
  &amp;lt;!-- &amp;lt;pre&amp;amp;glt;&amp;lt;code class="r"&amp;gt;
  &amp;lt;/code&amp;gt;&amp;lt;/pre&amp;gt; --&amp;gt;
&amp;lt;/p&amp;gt;
&amp;lt;hr/&amp;gt;
  &amp;lt;div class="footer"&amp;gt;
    &amp;lt;a href="2020-11-13.html"&amp;gt;&amp;amp;lt; Previous entry&amp;lt;/a&amp;gt; | &amp;lt;a href="index.html"&amp;gt;Back to Index&amp;lt;/a&amp;gt; &amp;lt;!--| &amp;lt;a href=""&amp;gt;Next entry &amp;amp;gt;&amp;lt;/a&amp;gt;--&amp;gt;
  &amp;lt;/div&amp;gt;
&amp;lt;/div&amp;gt;</code></pre>

				<p>Basically the idea is just to fill in the title, date and content. I do not touch the rest at first. You will notice that the link to the previously published page is already entered (the script does that). Only in the <code class="inline">&amp;lt;head&amp;gt;</code> part of the html code I modify the following meta tags:</p>

				<pre><code class="html">&amp;lt;meta property="og:title" content="" /&amp;gt;
&amp;lt;meta property="og:image" content="" /&amp;gt;
&amp;lt;meta name="category" content=""&amp;gt;</code></pre>

				<p>The first three will define the "overview" of the webpage if shared on facebook, twitter, reddit etc. The fourth one will be used by my code to define in which category the page will be displayed.</p>

				<p>And so, the <a href="http://github.com/plannapus/plannapus.github.io/blob/master/blog/process_blog.R">code itself</a> (in R, naturally), to process the blog:</p>

				<pre><code class="R">library(XML) #Only uses package XML
process &amp;lt;- function(html_file){
# This function will retrieve the content of each blogpost as a dataframe
# containing the url, the date, the title and the actual content.
  h &amp;lt;-htmlParse(html_file,encoding="utf-8") #Parse the file
  tit &amp;lt;- xpathSApply(h,"//p[@class='blog-title']",xmlValue) #Extract the title
  dat &amp;lt;- xpathSApply(h,"//p[@class='blog-date']",xmlValue) #The date
  desc &amp;lt;- xpathSApply(h,"//p[@class='blog-content']",xmlValue) #the content
  data.frame(url=html_file,title=tit,date=as.Date(dat),content=desc)
}
categories &amp;lt;- function(html_file){
#This one grabs the categories I blogpost were entered under
  h &amp;lt;-htmlParse(html_file,encoding="utf-8")
  x &amp;lt;- as.data.frame(do.call(rbind,xpathApply(h,"//meta[@name='category']",xmlAttrs)))[['content']]
  `if`(length(x),x,NA) #If no category, returns NA
}

setwd("plannapus.github.io/blog") #The path to the blog folder
entries &amp;lt;- dir(pattern="[0-9].html") #takes all the blogpost files

all &amp;lt;- do.call(rbind,lapply(entries,process)) #Apply the process function to each file and turn into a single dataframe
catg &amp;lt;- sapply(entries,categories) #Grab categories
catg &amp;lt;- catg[order(all$date,decreasing=TRUE)] #Reorder both per date (in reverse chronological order)
all &amp;lt;- all[order(all$date,decreasing=TRUE),]
cats &amp;lt;- names(sort(table(unlist(catg)),decreasing=TRUE)) #Retrieve unique categories and sort by number of posts</code></pre>

				<p>From that point on it's a lot of metaprogramming where I have html strings that I fill with the appropriate content. First the index page. This is really the most important part, that makes the blog readable. The only thing changing in that page is the table containing all the post titles+dates, and the list of categories:</p>

				<pre><code class="R">index &amp;lt;- readLines("index.html",encoding="utf-8") #Reads in the index page
#The following line create each entries in the table
j &amp;lt;- sprintf("\t\t\t&amp;lt;tr&amp;gt;&amp;lt;td class=\"date\"&amp;gt;%s&amp;lt;/td&amp;gt;&amp;lt;td class=\"title\"&amp;gt;&amp;lt;a href=\"%s\"&amp;gt;%s&amp;lt;/a&amp;gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;",all$date, all$url, all$title)
#Then we replace the former table with the new one:
index &amp;lt;- c(index[1:grep("&amp;lt;table",index)],j,index[grep("&amp;lt;/table",index):length(index)])
#And the same with the categories:
index[grep("&amp;gt;Categories:",index)] &amp;lt;- sprintf("\t\t\t&amp;lt;div class=\"footer\"&amp;gt;Categories: %s&amp;lt;br/&amp;gt;",
                                             paste(sprintf("&amp;lt;a href=\"categories/%1$s.html\"&amp;gt;%1$s&amp;lt;/a&amp;gt;",cats),collapse=" - "))
#We then print it back to the file (overriding the previous content):
cat(index,file="index.html",sep="\n")</code></pre>

<p>Then for each category, a specific page:</p>

<pre><code class="R">for(i in seq_along(cats)){
  #Here each page will be built on the same template, which is similar to the index page, with an additional title
  index &amp;lt;- readLines("categories/template.html",encoding="utf-8") #Reads in the template
  w &amp;lt;-sapply(catg,function(x)cats[i]%in%x)
  sub &amp;lt;- all[w,] #Pick the subset having the category currently processed
  #Makes the table, as previously:
  j &amp;lt;- sprintf("\t\t\t&amp;lt;tr&amp;gt;&amp;lt;td class=\"date\"&amp;gt;%s&amp;lt;/td&amp;gt;&amp;lt;td class=\"title\"&amp;gt;&amp;lt;a href=\"../%s\"&amp;gt;%s&amp;lt;/a&amp;gt;&amp;lt;/td&amp;gt;&amp;lt;/tr&amp;gt;",sub$date, sub$url, sub$title)
  index &amp;lt;- c(index[1:grep("&amp;lt;table",index)],j,index[grep("&amp;lt;/table",index):length(index)])
  #Adds the category name as title:
  index[grep("&amp;lt;h3&amp;gt;",index)] &amp;lt;- sprintf("&amp;lt;div class=\"footer\"&amp;gt;&amp;lt;h3&amp;gt;Category: %s&amp;lt;/h3&amp;gt;&amp;lt;/div&amp;gt;",cats[i])
  #Save in an html page named as the category for simplicity:
  cat(index,file=sprintf("categories/%s.html",cats[i]),sep="\n")
}</code></pre>

				<p>Then there is a section creating the RSS feed: it is very similar to the previous bit. You can <a href="http://github.com/plannapus/plannapus.github.io/blob/master/blog/process_blog.R">check it out in the repository</a> but I don't see the point of explaining it here. After that, there is a couple of lines of code to replace the link to the last blogpost in the template, followed by another few lines to put the link to the new post in the penultimate one:</p>

				<pre><code class="R">template &amp;lt;- readLines("template.html") #Reads in template
template &amp;lt;- gsub(as.character(all$date[2]),as.character(all$date[1]),template) #Replace the link to the second-to-last post by the last one
cat(template,file="template.html",sep="\n") #Prints it back

last_html &amp;lt;- entries[grepl(as.character(all$date[2]),entries)] #Finds the second-to-last post
last &amp;lt;- readLines(last_html) #Reads it in
w &amp;lt;- grep("Back to Index",last) #Finds the line with the links
last[w] &amp;lt;- gsub("&amp;lt;!--\\| &amp;amp;lt;a href=\"\"&amp;gt;Next entry &amp;amp;gt;&amp;lt;/a&amp;gt;--&amp;gt;",
                sprintf("| &amp;lt;a href=\"%s.html\"&amp;gt;Next entry &amp;amp;gt;&amp;lt;/a&amp;gt;",all$date[1]),
                last[w]) #And add the new one.
cat(last,sep="\n",file=last_html) #Prints it back</code></pre>

				<p>And this is literaly all there is to it. I might add some new parts to it as I am figuring things out (such as the categories pages that I really just added in the last week) but I think the core will stay the same. I don't think I need anything fancier.</p>

			</div></description><category>Programming</category><category>R</category></item>
<item><title>TMS General Meeting 2020 Online</title><link>http://plannapus.github.io/blog/2020-11-13.html</link><pubDate>2020-11-13</pubDate><description><div class="blog-content">
				This week was the annual meeting of <a href="http://tmsoc.org/">The Micropalaeontological Society</a>, in fact it doubled as its 50th anniversary. Of course given the current <s>nightmare</s> circumstances, it happened online and I must say that of all the many, many, MANY meetings and conferences I attended online this year, this might have been one of the most well done, together with the <a href="https://www.egu2020.eu/">EGU online meeting</a> which was really cool and original as it was really free-form, whereas TMS did manage to stick successfully to the classic talks + posters setting. 

				<p>Our students had <a href="https://www.tmsoc.org/conf2020/postergallery.php">two posters</a> there <a href="http://ehrenberglab.github.io/pages/mopga.html">on our current project</a> and got interesting comments/questions (in part as comments on the poster page which was a simple but nifty solution).</p>

				<p>The <a href="https://www.tmsoc.org/conf2020/programme.php">talks</a>, and in particular the keynote talks, were (more than usual) brilliant I must say. It's nice to see micropaleontology finally starting to live up to its potential. Some random thoughts/comments on the talks:
				</p><ul><li>Mixotrophs are in, this year! Two of the keynote talks revolved more or less around mixotrophy (one by Samantha Gibbs on <a href="https://advances.sciencemag.org/content/6/44/eabc9123">K/Pg nannofossils</a>, one by Celli Hull on a more general point of consideration vis-à-vis questions around planktonic protists and carbon pump). More generally people really moved from considering microfossils as piece of rocks useful to date things to actually considering them for the complex eukaryots they were/are. It helps that molecular biologists started working hand in hand with micropaleontologists in the last decade.</li>
				<li>Radiolarian taxonomy is going through a massive transformation (which might end up with us having to throw away half of the <span class="tooltip">Haeckelian genera names<span class="tooltiptext">good riddance!</span></span> in particular) and I for one cannot be happier.</li>
				<li>People really finally embrace databases and online taxonomical catalog, which is a win for everyone.</li>
				</ul><img class="current" src="img/tms_screenshot.png" width="400"/><p class="caption">Screenshot of the <a href="https://www.tmsoc.org/conf2020">meeting website</a>.</p>

				<p>Together with our colleagues at the GFZ in Potsdam, we are thinking about putting on a bid to host next year's meeting in Berlin (<span class="tooltip">or, you know, online, if needed<span class="tooltiptext">Hat tip to all the people out there acting as if this was over and thanks to whom this nightmare is gonna last for the next 2 to 5 years! If it's OK with you I would like to be able to go see my family again.</span></span>). We'll see how it goes!</p>
			</div></description><category>Science</category></item>
<item><title>Calculating paleocoordinates with gplates and R</title><link>http://plannapus.github.io/blog/2020-11-09.html</link><pubDate>2020-11-09</pubDate><description><div class="blog-content">
As promised in the <a href="2020-10-15.html">first post of this series</a>, I am going to show here how to use gplates to calculate the paleocoordinates of specific sites given a specific rotation model, using GPlates. To make it worth it, I'll use here the <a href="https://doi.org/10.1016/j.gloplacha.2016.10.002">Matthews et al. 2016</a> rotation model as it is an update on the one we are using in the Neptune database. Also, and just for fun, we are going to compute the paleocoordinates of the two sites we drilled during <a href="https://iodp.tamu.edu/scienceops/expeditions/amundsen_sea_ice_sheet_history.html">Expedition 379</a> to the Amundsen Sea, an expedition I was part of. We only drilled until the latest Miocene but here I am going to compute the coordinates in the Turonian (as a tribute to the <a href="https://www.nature.com/articles/s41586-020-2148-5">Klages et al. 2020 paper</a>).

The first step, as usual, is to get the sites current coordinates from Neptune (using my <a href="https://github.com/plannapus/NSBcompanion">NSBcompanion</a> package). I know that I mentioned in <a href="2020-11-03.html">the previous post</a> that <code class="inline">rgdal/sp</code> packages were obsolete and needed to be replaced with package <code class="inline">sf</code>, but I am not yet familiar enough with <code class="inline">sf</code> so here it is with <code class="inline">rgdal/sp</code>:

				<pre><code class="r">library(rgdal)
setwd("~/Desktop") #For code simplicity I am explicitely setting a working directory in which every file will be.
nsb &amp;lt;- NSBcompanion::nsbConnect("guest","arm_aber_sexy")
exp379 &amp;lt;- dbGetQuery(nsb,"SELECT hole_id, longitude, latitude FROM neptune_hole_summary WHERE leg=379 and hole='A';")
dbDisconnect(nsb)
exp379sp &amp;lt;- SpatialPointsDataFrame(exp379[,-1],data=exp379, proj4string=CRS("+proj=longlat"))
writeOGR(exp379sp,".","exp379",driver="ESRI Shapefile") #"." because we save it in the current folder</code></pre>

Then we'll download the data from the Matthews et al. 2016 paper from the EarthByte website (the makers of GPlates).

<pre><code class="r">download.file("https://www.earthbyte.org/webdav/ftp/Data_Collections/Matthews_etal_2016_Global_Plate_Model_GPC.zip","Matthews_etal_2016.zip") #download
unzip("Matthews_etal_2016.zip") #unzip</code></pre>

Here we determine on which tectonic plates the sites we want to backtrack are:

<pre><code class="r">system("gplates assign-plate-ids -p Matthews_etal_2016_Global_Plate_Model_GPC/Global_EarthByte_Mesozoic-Cenozoic_plate_boundaries_Matthews_etal.gpml\\
       -l exp379.shp -t 0 -s shapefile")
# -p is the plate boundary file that will be used to partition the sites
# -l is the feature we want to partition
# -t is the time at which the features are taken
# -s is the format we want to save the result as

# Just to check it worked, read in the file and check it has plate IDs
e3 &amp;lt;- readOGR(".","exp379")
e3$PLATEID1 #Should show 802 for both sites.
rm(e3)</code></pre>

Then we use the same method as previously to reconstruct the coastlines and the sites at the given time (here 90Ma):

<pre><code class="r">system("gplates reconstruct -t 90 -l exp379.shp\\
       -r Matthews_etal_2016_Global_Plate_Model_GPC/Global_EB_250-0Ma_GK07_Matthews_etal.rot\\
       -o exp379_cretaceous -e shapefile")

system("gplates reconstruct -t 90 -l Matthews_etal_2016_Global_Plate_Model_GPC/StaticGeometries/Coastlines/Global_coastlines_2015_v1_low_res.shp \\
       -r Matthews_etal_2016_Global_Plate_Model_GPC/Global_EB_250-0Ma_GK07_Matthews_etal.rot\\
       -o coast_cretaceous -e shapefile")</code></pre>

And finally we plot them the exact same way as previously:

<pre><code class="r">exp379cret &amp;lt;- readOGR(".","exp379_cretaceous")
coast &amp;lt;- readOGR(".","coast_cretaceous")

dest_projection &amp;lt;- CRS("+proj=laea +lat_0=-90 +lon_0=0")
polar &amp;lt;- spTransform(coast,dest_projection)
polar_379 &amp;lt;- spTransform(exp379cret,dest_projection)
polar_poly &amp;lt;- as(polar,"SpatialPolygons") #Here the coastlines are indeed lines, but we want them as polygons so that we can color-fill them.

limitANT &amp;lt;- Lines(list(Line(cbind(-180:180,-45))),ID="a")
limitANT &amp;lt;- SpatialLines(list(limitANT),proj4string=CRS("+proj=longlat"))
limitANT &amp;lt;- spTransform(limitANT,dest_projection)

png("turonian.png",h=400,w=400)
par(mar=c(0,0,0,0))
plot(limitANT,col="white")
plot(polar_poly,col="grey80",border="grey80",add=TRUE)
plot(polar_379,col="red",pch=19,add=TRUE,cex=0.5)
box(lwd=3)
dev.off()</code></pre>

<img class="current" src="img/turonian.png" width="400"/><p class="caption">I know, it's a bit underwhelming since the Antarctic plate didn't move that much in the last 100Myr, but you get the gesture :)</p>

			</div></description><category>Programming</category><category>R</category></item>
<item><title>Addendum to the previous post on paleogeographic maps</title><link>http://plannapus.github.io/blog/2020-11-03.html</link><pubDate>2020-11-03</pubDate><description><div class="blog-content">
				As it turned out, I realized the way I have been making maps in R since 10 years is more or less deprecated! Package <code class="inline">sp</code> was superseded by package <code class="inline">sf</code>, and indeed the coordinate system transformation works so much better and do not need as much <a href="https://github.com/plannapus/Unsorted-functions/blob/master/myTransform.R">weird hacking</a> to work on complex projections.
				The code from <a href="2020-10-15.html">last time</a> thus become as follows:
				<pre><code class="r">library(sf)
system("gplates reconstruct -l /Applications/GPlates-1.5.0/SampleData/FeatureCollections/Coastlines/Shapefile/Seton_etal_ESR2012_Coastlines_2012.1_Polygon.shp \\
-r /Applications/GPlates-1.5.0/SampleData/FeatureCollections/Rotations/Seton_etal_ESR2012_2012.1.rot \\
-e shapefile -t 33.5 -o ~/Desktop/reconstructed")
coast &amp;lt;- read_sf("~/Desktop/reconstructed.shp") #This, already, is an improvement on the previous cumbersome readOGR

library(NSBcompanion)
nsb &amp;lt;- nsbConnect("guest","arm_aber_sexy")
exp113 &amp;lt;- dbGetQuery(nsb,"SELECT hole_id, paleo_latitude, paleo_longitude,rotation_source FROM neptune_paleogeography WHERE hole_id LIKE '113_%' AND reconstructed_age_ma=33.5;")
dbDisconnect(nsb)

dest_projection &amp;lt;- CRS("+proj=laea +lat_0=-90 +lon_0=0")
exp113pts &amp;lt;- st_multipoint(cbind(exp113$paleo_longitude, exp113$paleo_latitude),dim="XY") #Two-step approach, similar to what we had in "sp"
exp113sfc &amp;lt;- st_sfc(exp113pts,crs="+proj=longlat")
polar_113 &amp;lt;- st_transform(exp113sfc,dest_projection)
polar &amp;lt;- st_transform(coast,dest_projection)

limitANT &amp;lt;- st_sfc(st_linestring(cbind(-180:180,-45)),crs="+proj=longlat") #Way simpler here.
limitANT &amp;lt;- st_transform(limitANT,dest_projection)

png("eocene2.png",h=400,w=400)
par(mar=c(0,0,0,0))
plot(limitANT,col="white")
plot(polar$geometry,col="grey80",border="grey80", add=TRUE)
plot(polar_113,col="red",pch=19,cex=0.5,add=TRUE)
box(lwd=3)
dev.off()</code></pre>

With the following result (identical to the one in the previous post obviously):
<img class="current" src="img/eocene2.png" width="400"/>

Trying more complex, better projections is clearly seamless (though not trivial) but still have a few glitches that will need to taken care of:

<pre><code class="r">dest_projection2 &amp;lt;- CRS("+proj=ortho +lat_0=-90 +lon_0=0") #Projection orthogonal, more natural
polar2_113 &amp;lt;- st_transform(exp113sfc,dest_projection2)
polar2 &amp;lt;- st_transform(coast,dest_projection2)

valids &amp;lt;- st_is_valid(polar2) # Some geometries fail with that projection unfortunately

png("eocene3.png",h=400,w=400)
par(mar=c(0,0,0,0))

#Draw a circle around earth
r &amp;lt;- 6400000 #earth radius
t &amp;lt;- seq(0,2*pi,by=0.01)
plot(r*cos(t),r*sin(t),col="black",lty=1,lwd=1.5,type="l")

plot(polar2[valids,]$geometry,col="grey80",border="grey80",add=TRUE)
plot(st_make_valid(polar2[!valids,]),col="grey80",border="grey80",add=T) #Yes, as simple as "make_valid" :)
plot(polar2_113,col="red",pch=19,cex=0.5,add=TRUE)

dev.off()</code></pre>

As one can see South America and Africa suffer from the projection:
<img class="current" src="img/eocene3.png" width="400"/>

Hopefully I ll find a solution for that, at some point.

			</div></description><category>Programming</category><category>R</category></item>
<item><title>A few thoughts on stratigraphy</title><link>http://plannapus.github.io/blog/2020-10-28.html</link><pubDate>2020-10-28</pubDate><description><div class="blog-content">
				This summer Dave and I wrote a <a href="https://nfdi4earth.de/images/nfdi4earth/documents/pilots/proposals/132-CHRONOSTRATIGRAPHIC_API_RDC_SERVICES.pdf">pre-proposal</a> for a pilote project (whose goal was to create a data standard for stratigraphy) in the context of the <a href="https://www.nfdi4earth.de/">NFDI4Earth</a>, a german-wide initiative to create a common infrastructure for geosciences-related research data. 
				My preproposal was not selected, which, given its <span class="tooltip">niche<span class="tooltiptext">broader than it seems but maybe it wasn't made clear in my proposal</span></span> subject, was more or less expected.

				<p>It was nonetheless a very good occasion to put in writings some thoughts I had about stratigraphy as a science, and I kept pondering about it since then. I do think there are a significant number of issues in stratigraphy that prevent it from going forward.</p>

				<p>The first issue, that I wanted to address with that proposal, is the issue of reproducibility. Instead of re-explaining it, here's the main point I made in a preliminary version of the proposal:</p>

				<blockquote cite="https://nfdi4earth.de/images/nfdi4earth/documents/pilots/proposals/132-CHRONOSTRATIGRAPHIC_API_RDC_SERVICES.pdf"><p>Current exchange standards reflect the fact that <b>chronostratigraphic information</b> in many databases <b>is reduced to a single text or numerical age tag for an object. However this value is not raw data but interpreted data, which depends on how the age was inferred; what biostratigraphic, magnetostratigraphic or isotopic data were used, how these data were calibrated and to which standard timescale</b> (e.g. a geomagnetic polarity time scale - GPTS) <b>the calibrations were made</b>. As calibrations and standard scales evolve with time, those static age assignments attached to samples are frequently out of date and incorrect. The relevant chronostratigraphic data needed to correct these ages, despite often being in existence, is typically stored in another system and is not linked to the sample records. The absence of linked chronostratigraphic metadata accompanying these age assignments, or a standard for creating the linkage, is a major hinderance to correct data synthesis.</p></blockquote>

				<p>The second issue is that of error quantification. It's a fairly complex issue that I am not sure to be able to verbalize correctly here but to make it simple: age models (in deep-sea stratigraphy at least) are made based on biostratigraphic, magnetostratigraphic, isotopic data;  all those data come with a margin of error, first in depth as the cores are sampled discretely, and more importantly in time; that error, in the current state of affair, is not propagated to the age model. It should be, but we simply do not have the mathematical framework to do so correctly (as far as I can judge) currently. More generally, statistics in stratigraphy is widely underdeveloped as a discipline but is necessary for the field to go forward. I dream of seeing one day an IODP site being dated with an age model with a confidence interval!</p>

				<p>Another, more minor issue, that may be unique to deep-sea stratigraphy, is the assumption of linearity in the sedimentation rate, i. e. what makes age models a series of connected segments rather than a smooth curve (see for instance my age-model making software <a href="http://github.com/plannapus/nsb_adp_wx">NSB_ADP_wx</a> as main culprit). There are plenty of evidence for a dynamically changing sedimentation rate. Of course implementing that to routinely create non-broken age models is frankly very complex, but it is most probably the way to go forward.</p>

				<p>Unfortunately, apart from the first issue mentioned, for which I had a solution (creating a metadata standard), the other issues are more open-ended, and I, unfortunately, do not have a solution so far. But I am working on it!</p>

				<img class="current" width="800" src="img/crappy_am.png"/><p class="caption">Quite a textbook scenario for everything I mentioned here!</p>
			</div></description><category>Science</category></item>
<item><title>Sarah's paper in Nature Communications</title><link>http://plannapus.github.io/blog/2020-10-22.html</link><pubDate>2020-10-22</pubDate><description><div class="blog-content">
			At last, our paper, lead by <a href="https://www.researchgate.net/profile/Sarah_Trubovitz">Sarah Trubovitz</a>, came out today in <a href="http://doi.org/10.1038/s41467-020-18879-7"><i>Nature Communications</i></a> ! Sarah came to visit us in Berlin twice in 2017 to learn/figure out with us the taxonomy of Neogene radiolarians from the equatorial Pacific. We had an inkling that the actual amount of species in that region/timeframe would be way higher that what was published but I must admit we did't expect that much: she found 990 species just in the last 10 million years of a single site!
			<p>Later, Dave, Paula and her got the idea to compare the species richness and the species content of her work with the Southern Ocean dataset that <a href="http://dx.doi.org/10.1666/12016">I counted during my thesis</a>, eons ago, and figure out if the extinction event I saw was global, regional or in fact just a migration out of the Antarctic circle, i. e. range shrinking due to the global cooling of the late Miocene (find out more about it by reading the <a href="https://www.museumfuernaturkunde.berlin/en/press/press-releases/largest-biome-earth-peril">press release</a>... or naturally the paper itself). I didn't have that much time to spent contributing to the writing itself but as the concept was to directly compare the species ranges in both our datasets, I was naturally involved, in particular to match her species concepts with mine. It was quite the nice surprise to see some of the species I described in the Southern Ocean based on a few specimens popping up in the equatorial Pacific!</p>

			<table class="img_current"><tr><td><img class="current" src="img/kozoi.png" title="Lithomelissa kozoi" width="240"/></td>
				<td><img class="current" src="img/sugiyamai.png" title="Clathrocorys sugyiamai" width="315"/></td>
			</tr></table><p class="caption"><i>Lithomelissa</i>? <i>kozoi</i> and <i>Clathrocorys sugyiamai</i> (both described in <a href="http://dx.doi.org/10.1144/jmpaleo2011-025">Renaudie &amp;amp; Lazarus 2013</a>) were among the species also found in the Equatorial Pacific.</p>

			<p>Despite the fact that radiolarians from these two regions are basically studied since close to 200 years, it might surprise some to know that <a href="https://doi.org/10.5281/zenodo.4014322">Sarah's dataset and mine</a> are, to date, the only two datasets that <i>tried</i> to actually record exhaustively the full diversity of those faunas! In part because radiolarian are so insanely diverse, but also simply because studying biodiversity was/is not a (fundable) goal of micropaleontology.</p>
			<p>Later during reviews, I also contributed a little piece of statistical analysis that I am actually quite proud of – despite it being a very, very small part of the whole paper naturally :) It uses a nice trick I learned on <a href="https://stats.stackexchange.com/">CrossValidated</a> (sadly I can't seem to find the post itself back) to test the significance of an event in a time serie: building an AR model for the time serie, and adding as explaining variable to the model a categorical variable stating whether the data points belong to the interval before or after the event, and then test if that variable explains a significant proportion of the variance (using a Wald test). It's fairly simple but it nicely goes around the limitation a t-test would have (as the data in a timeseries would most likely not be independent).</p>
			<p>Sarah also wrote a <a href="https://natureecoevocommunity.nature.com/posts/marine-plankton-show-threshold-extinction-response-to-neogene-climate-change">"Behind The Paper"</a> story that you will find on the <a href="https://natureecoevocommunity.nature.com/">Nature Ecology &amp;amp; Evolution Community blog</a>.</p>
			<h4>Reference:</h4>
			<p class="publications">Trubovitz S., Lazarus D., Renaudie J., Noble P.J., 2020. <a href="http://doi.org/10.1038/s41467-020-18879-7">Marine plankton show threshold extinction response to Neogene climate change</a>. <i>Nature Communications</i>, 11:5069.</p>
			</div></description><category>Science</category><category>MS-related</category></item>
<item><title>Miassou</title><link>http://plannapus.github.io/blog/2020-10-21.html</link><pubDate>2020-10-21</pubDate><description><div class="blog-content">
				<a href="https://en.wikipedia.org/wiki/P%C3%A9rigord_noir">The region I come from</a> is known for its duck/goose-based cooking, not so much for its desserts. And indeed we do not have that much of a sweet tooth in Perigord Noir. But we do have a few desserts. My favourite and maybe the least known of them is the miassou, and I happen to make quite a good one, if i may say so. The name unfortunately refers to a dozen different dishes, ranging from potato pancakes to any cornmeal-based cake, so what follows is a recipe of miassou, <i>as done in my family</i>.

				<p>The basis is quite unconventional for a french cake but very seasonal (and as a consequence I tend to cook one every year in Fall): pumpkin, cornmeal and apples.</p>

				<p>The idea is to boil the chopped-up pumpkin, mash it and mix in some cornmeal. Separately split 2 to 4 eggs, whiten the yolks with a bit of sugar (brown sugar if possible), and beat the egg whites until ferm. Incorporate both preparations to the pumpkin/cornmeal mix until it is homogenous. Then place the mixture in a buttered dish, and cover with slices of apples. Add a dash of sugar on top of the apples and cook in the oven at 220°C for nearly 45 min.</p>

				<img class="current" src="img/miassou.jpg" width="400"/><p class="caption">The result, from above. When cut into slices, it should stand on its own in one piece. If not, then put more cornmeal the next time around :)</p>
			</div></description><category>Cooking</category></item>
<item><title>First step in mixing gplates and R to make paleogeographic maps</title><link>http://plannapus.github.io/blog/2020-10-15.html</link><pubDate>2020-10-15</pubDate><description><div class="blog-content">I make <a href="http://www.biogeosciences.net/13/6003/2016/">a lot of paleogeographic maps</a>, and I always do so in R. Whichever programming language you use, <a href="https://www.gplates.org/">Gplates</a> is an invaluable tool to achieve this. What follows is a <span class="tooltip">short example<span class="tooltiptext">as mentioned in the previous post I am still processing some bad news so it's all I am capable of conjuring right now</span></span> of how to insert it in your workflow to map your paleodata.
		
			<p>OK as the idea here is really just to give a first glance of the tools I will start straight ahead with converting a standard coastline map to a late Eocene base map using <a href="https://linkinghub.elsevier.com/retrieve/pii/S0012825212000311">Seton et al. 2012 rotation model</a>. Why? Because then all you need is given with Gplates, and this is the one we have been using in <a href="http://nsb-mfn-berlin.de">Neptune</a>.</p>

<pre><code class="R">library(rgdal)
system("gplates reconstruct -l /Applications/GPlates-1.5.0/SampleData/FeatureCollections/Coastlines/Shapefile/Seton_etal_ESR2012_Coastlines_2012.1_Polygon.shp \\
-r /Applications/GPlates-1.5.0/SampleData/FeatureCollections/Rotations/Seton_etal_ESR2012_2012.1.rot \\
-e shapefile -t 33.5 -o ~/Desktop/reconstructed")
#A lot to unpack: here I use the "feature collections" given with Gplates. 
# '-l'  argument is the feature to modify
# '-r' the rotation model
# '-e' the format of the output
# '-t' the desired time in Million years
# '-o' the basename of the output
# Here I have gplates on my sys.path, but you can just point at the binary file directly
coast &amp;lt;- readOGR("/Users/johan.renaudie/Desktop","reconstructed") #Read it in in R. For some reason, readOGR does not do tilde expansion.
</code></pre>

			<p>Using <a href="https://github.com/plannapus/NSBcompanion">my package NSBcompanion</a>, grab some data in the <a href="http://nsb-mfn-berlin.de">NSB database</a>, here, to simplify, the paleocoordinates of the sites from ODP leg 113 (though I will show in a future post how to calculate them as well using gplates CLI tool, though it is a couple of steps more complicated than this). Again, the paleogeography (as of 2020) in Neptune is based on the same rotation model mentioned above, so it is compatible.</p>

<pre><code class="R">library(NSBcompanion)
nsb &amp;lt;- nsbConnect("guest","arm_aber_sexy") #Connect to Neptune
exp113 &amp;lt;- dbGetQuery(nsb,"SELECT hole_id, paleo_latitude, paleo_longitude,rotation_source FROM neptune_paleogeography WHERE hole_id LIKE '113_%' AND reconstructed_age_ma=33.5;") #Grabs the Leg 113 paleocoordinates
dbDisconnect(nsb) #Disconnect
</code></pre>

			<p>The next step is to transform from the standard longitude-latitude projection to something more suited to the Southern Ocean (since expedition 113 went around the Weddell Sea), here the Lambert Azimuthal Equal Area Projection.</p>

<pre><code class="R">orig_projection &amp;lt;- CRS("+proj=longlat")
dest_projection &amp;lt;- CRS("+proj=laea +lat_0=-90 +lon_0=0") #This is the proj4 string for said projection
# It is fairly straight-forward: 
# +proj=laea means Lambert Azimuthal Equal Area Projection
# +lat_0=-90 : we take latitude -90 as the center of the projection
# +lon_0=0 : and the Greenwhich meridian is kept at the center (top).
exp113pts &amp;lt;- SpatialPointsDataFrame(exp113[,c("paleo_longitude","paleo_latitude")],data=exp113, proj4string=orig_projection) #Make the sites coordinates into a "Spatial-" object, that can be transformed.
polar &amp;lt;- spTransform(coast,dest_projection) #Transform the base map
polar_113 &amp;lt;- spTransform(exp113pts,dest_projection) #Transform the data points</code></pre>

			<p>To make the plot a bit cleaner, I use this dumb trick to zoom in: I create a fake parallel (here 45˚ South) and plot it first to force the plot to fit around it.</p>

<pre><code class="R">limitANT &amp;lt;- Lines(list(Line(cbind(-180:180,-45))),ID="a") #Make the object into a Line object (literally, a line from -180 to 180 longitude at -45 latitude)
limitANT &amp;lt;- SpatialLines(list(limitANT),proj4string=orig_projection) #Make the object into a SpatialLines object. Yes it is quite tedious.
limitANT &amp;lt;- spTransform(limitANT,dest_projection) #Transform</code></pre>

			<p>And then plot everything as wanted:</p>

<pre><code class="R">png("eocene.png",h=400,w=400)
par(mar=c(0,0,0,0))
plot(limitANT,col="white")
plot(polar,col="grey80",border="grey80",add=TRUE)
plot(polar_113,col="red",pch=19,add=TRUE,cex=0.5)
box(lwd=3)
dev.off()</code></pre>

			<p>The resulting picture: a map of the Southern Ocean at the Eocene-Oligocene boundary, highlighting the varous sites from ODP expedition 113 to the Weddell Sea.</p>
			<img class="current" src="img/eocene.png" width="400"/></div></description><category>Programming</category><category>R</category></item>
<item><title>Guinea Fowl in Cider</title><link>http://plannapus.github.io/blog/2020-10-10.html</link><pubDate>2020-10-10</pubDate><description><div class="blog-content">
				As every time I receive bad news, I cope by cooking. This weekend, I wanted to try something new so I did the lazy variation of inventing a new dish: combining two dishes.
				In that case I mixed a recipe I had for a guinea fowl cooked in white wine, and a recipe of rabbit in cider I made two winters ago, and thus cooked a guinea fowl in cider, and it was delicious.<br/><br/><a href="img/livre_recette.jpg"><img class="current" src="img/livre_recette.jpg" title="Behold the Foodonomicon" width="300"/></a>
				<p class="caption">The page in my very own <i>encyclopedia culinaria</i> for the original guinea fowl recipe I modified here.</p>

				<p>The idea is fairly simple. First cut the guinea fowl in pieces and sauté them in a pot in olive oil, with salt and pepper. When golden, one can take them out of the pot and add oignons, carrots and garlic, with a bit of thyme. After a couple minutes (when the oignons start softening), add back the meat, and cover the whole thing with cider. Because I am in Berlin, I went with one of the OK ciders sold in supermarkets here, but naturally in France I would have gone for a cider from Brittany (a bit drier and less sweet than the ones here) but they are quite difficult to find here for a reasonnable price. Then add a laurel leave or two and cover, reduce the heat and let cook a good 45 minutes. When cooked, take the meat out of the pot, mix the rest together (or press everything with a potato masher as I did here). Thicken the sauce with a bit of cornstarch, put back the meat in and that's basically it. We ate it with some leftover boiled potatoes and some long beans, but rice, pasta or polenta are also possibilities.</p>
				<img class="current" src="img/guineafowl_2.jpg" title="The dish after 45 minutes of cooking" width="400"/><img class="current" src="img/guineafowl_3.jpg" title="The dish once served" width="400"/><p class="caption">Dish at the end of the simmering process and once served. Turned out great!</p>
			</div></description><category>Cooking</category></item>
<item><title>A walk through my beamer theme</title><link>http://plannapus.github.io/blog/2020-10-05.html</link><pubDate>2020-10-05</pubDate><description><div class="blog-content">A few years ago I started converting to LaTeX: first for proposals, manuscripts and then eventually for presentations. I am not going to lie: the first one took me way longer than usual but since then, now that I have a basic template, it became really worth it. I didn't go for a flashy, fancy theme but made a very simple one based on <a href="https://www.museumfuernaturkunde.berlin/">my institution</a> official template. And since it is simple, it makes for a nice introduction to people willing to learn how to make their own beamer template.<br/>
			First: your theme will reside in a separate file, called by your main presentation file in a way similar to a package. A weird quirk of beamer is that your filename has to be of the form "<b>beamertheme</b><i>themename</i>.sty", i.e. <code class="inline">beamerthemeexample.sty</code> for a theme called "example". In my case the file is called <code class="inline">beamerthememfn_green.sty</code>, and reside in the same folder as the presentation file, it is thus imported in it as follows:

				<pre><code class="Latex">\usetheme{mfn_green}</code></pre>

			The theme file (downloadable <a href="../data/beamer_template.zip">here</a>) itself starts like this:

				<pre><code class="Latex">\mode&amp;lt;presentation&amp;gt;
\usepackage{pgfcomp-version-0-65}
\usepackage{color}                %Used to define colors
\usepackage{fontspec}             %Used to import a new font</code></pre>

			After importing a few packages, it starts by defining colors and the main font (here the one used in all official documentation by my institution: <span class="tooltip">Trade Gothic LT Std<span class="tooltiptext">Beware however that the presentation file thus needs to be compiled with xelatex or lualatex, not standard pdflatex/latex!</span></span>):

<pre><code class="Latex">\definecolor{black}{RGB}{0, 0, 0}
\definecolor{mfngreen}{RGB}{161, 191, 36}
\usefonttheme{professionalfonts}
\setmainfont{Trade Gothic LT Std}</code></pre>

			Then set the appearence of normal slides:

<pre><code class="Latex">\useinnertheme{rounded} %Base minimalist template
\setbeamercolor{frametitle}{fg = mfngreen} %Titles are green...
\setbeamerfont{frametitle}{size = \large}  %... large ...
\setbeamertemplate{frametitle}[default][center] %... and centered
\setbeamercolor{normal text}{fg = black}   %Rest of text is black
\setbeamertemplate{navigation symbols}{}   %And i don't want any navigation symbols</code></pre>

			Then the title slide:

<pre><code class="Latex">\setbeamertemplate{title page}{
	\begin{center}
	\huge{\textcolor{mfngreen}\inserttitle}  %The title of the talk, called in the file with \title{}

	\vspace{1cm}
	\Large{\insertauthor}                    %The author(s) of the talk, called in the file with \author{}

	\small{\insertinstitute}                 %Their institute, called in the file with \institute{}

	\small{\insertdate}						 %And the date (called with \date{})

	\end{center}
}</code></pre>

			The "outline" slide, together with the look of bullet point lists (not that I use a lot of them):

<pre><code class="Latex">\setbeamercolor{section in toc}{fg= black}
\setbeamertemplate{itemize item}{\color{mfngreen}$\textbullet$} %All lists have a green bullet for each item
\setbeamertemplate{itemize subitem}{\color{mfngreen}$\textbullet$} %...and subitem
\setbeamertemplate{section in toc}{\color{mfngreen}$\textbullet$~\color{black}\inserttocsection} %Needs to be specified separately for the TOC</code></pre>

			And then I created a new environment for the "thank you" frame:

<pre><code class="Latex">\newenvironment{thankframe}{
	\setbeamercolor{frametitle}{fg = black}
	\setbeamercolor{background canvas}{bg=mfngreen} %Here the background is green and the title black
	\begin{frame}
}
{\end{frame}}</code></pre>

			The file then terminates with this statement:

			<pre><code class="Latex">\mode
&amp;lt;all&amp;gt;</code></pre>

			And it's basically it! In the real file I also upload a couple of logos to add to the first and last slide footers but it could be as well done in the presentation file itself with the usual <code class="inline">\includegraphics</code> bit.<br/>
			Here is an example of what the output looks like (in this case a seminar talk I gave in Potsdam last year when visiting my colleagues at the GFZ):<br/><object class="blog" data="../data/20190723Potsdam.pdf" width="500" height="425" type="application/pdf"><p><a href="../data/20190723Potsdam.pdf">Download.</a></p></object>
			</div></description><category>Programming</category><category>LaTeX</category></item>
<item><title>A backbone for radiolarian phylogeny</title><link>http://plannapus.github.io/blog/2020-10-02.html</link><pubDate>2020-10-02</pubDate><description><div class="blog-content">
			It's been a couple of good years for radiolarian research! After almost 200 years of studying them, and failing to understand their classification fully (in part thanks to Haeckel honestly), three papers came out in the last 2 years that present molecular data to establish a proper backbone to living radiolarian phylogeny.
			The first one from last year by <a href="&amp;#10;&amp;#9;&amp;#9;&amp;#9;https://doi.org/10.1016/j.protis.2019.02.002">Sandin et al.</a> presented a morpho-molecular classification of Nassellarians, then this year a paper by <a href="&amp;#10;&amp;#9;&amp;#9;&amp;#9;https://doi.org/10.1016/j.protis.2019.125712">Nakemura et al.</a> sorted out the various groups considered by some to be modern Entactinarians (Entactinaria being an order of polycystines radiolarians known in the Paleozoic and the Mesozoic, primarily), and a preprint by <a href="https://doi.org/10.1101/2020.06.29.176917">Sandin et al.</a>, again, offered a morpho-molecular classification of Spumellarians. Patching them together and adding to that the work from a few years ago by <a href="https://doi.org/10.1016/j.revmic.2004.06.002">Osamu Takahashi et al. in 2004</a> which showed that the haeckelian order of Collodaria was in fact highly derived Nassellarians and the work by <a href="http://dx.doi.org/10.1016/j.protis.2015.05.002">Biard et al. 2015</a> which sorted them out, and you get that combined tree (as long as i didn t make any mistakes): <br/><pre><code>((Eucyrtidoidea, ((Acropyramioidea,((Carpocaniidae, Artostrobioidea),(Acanthodesmoidea,(Orosphaeridae,(Sphaerozoidae,(Collophidiidae,Collosphaeridae))Collodaria)))), (((Archipiloidea,Theopilidae),Plagiacanthoidea), (Cycladophoridae, (Lychnocanoidea,Pterocorythoidea)))))Nassellaria, ((Hexastyloidea, (Spongosphaeroidea,(Lithocyclioidea, Spongodiscoidea))), (Liosphaeroidea,((Rhizosphaeroidea,(Centrocubidae,Excentroconchidae)Centrocuboidea), ((Stylodictyoidea,(Actinommoidea,Spongopyloidea)), (Tholoniidae,Pyloniidae)Pylonioidea))))Spumellaria);
			</code></pre>

			<img class="current" src="img/radtree.png" title="Tree of polycystine radiolarians" width="600"/><p class="caption">Tree of living polycystine radiolarians; after Takahashi et al. 2004, Biard et al. 2015, Sandin et al. 2019, 2020, Nakemura et al. 2020.</p>

			The biggest surprises for me in these studies and this tree are:
			<ul><li> the fact that Orosphaeridae are the sister group of the collodarians, and in fact belong to Nassellaria: though honestly, after the fact, it makes so much sense, as an intermediary step! </li>
			<li> the placement of Cannobotryidae inside Plagiacanthoidea: again, make sense, but how is it we never thought about it before? I seem to remember a paper by Maria Petrushevskaya in which she considered them as sister taxa though.</li>
			</ul>

			Anyway, now that we have that, we can actually start testing skeletal homology hypothesis and start filling the blanks (given naturally the molecular studies used a very limited number of species/specimens for each group). Very exciting!

			<h4>References:</h4>
			<p class="publications">Biard, T., Pillet, L., Decelle, J., Poirier, C., Suzuki, N., and Not, F. (2015). <a href="http://dx.doi.org/10.1016/j.protis.2015.05.002">Towards an integrative morpho-molecular classification of the Collodaria (Polycystinea, Radiolaria)</a>. Protist, 166(3):374– 388.</p>
			<p class="publications">Nakamura, Y., Sandin, M. M., Suzuki, N., Tuji, A., and Not, F. (2020). <a href="&amp;#10;&amp;#9;&amp;#9;&amp;#9;https://doi.org/10.1016/j.protis.2019.125712">Phylogenetic revision of the order Entactinaria—Paleozoic relict Radiolaria (Rhizaria, SAR)</a>. Protist, 171(1):125712.</p>
			<p class="publications">Sandin, M. M., Pillet, L., Biard, T., Poirier, C., Bigeard, E., Romac, S., Suzuki, N., and Not, F. (2019). <a href="&amp;#10;&amp;#9;&amp;#9;&amp;#9;https://doi.org/10.1016/j.protis.2019.02.002">Time calibrated morpho-molecular classification of Nassellaria (Radiolaria)</a>. Protist, 170(2):187–208.</p>
			<p class="publications">Sandin, M. M., Biard, T., Romac, S., O’Dogherty, L., Suzuki, N., and Not, F. (2020). <a href="https://doi.org/10.1101/2020.06.29.176917">Morpho-molecular diversity and evolutionary analyses suggest hidden life styles in Spumellaria (Radiolaria)</a>. bioRxiv.</p>
			<p class="publications">Takahashi, O., Yuasa, T., Honda, D., and Mayama, S. (2004). <a href="https://doi.org/10.1016/j.revmic.2004.06.002">Molecular phylogeny of solitary shell-bearing Polycystinea (Radiolaria)</a>. Revue de Micropaléontologie, 47(3):111-118.</p>
			</div></description><category>Science</category></item>
<item><title>Scraping rendered HTML</title><link>http://plannapus.github.io/blog/2020-09-28.html</link><pubDate>2020-09-28</pubDate><description><div class="blog-content">
				Once in a while (or, you know, every day), when you're doing 'big' data analysis you need to do some web scraping – grab directly data from public websites – particularly if they do not have an API to deliver the data you are interested in. Most of the time, all it takes is to read in the HTML page in R (thanks to <a href="https://cran.r-project.org/web/packages/XML/index.html">package XML</a>), and pick up the data, which in the best case scenario already lies there as a table. 

				<p>But thanks to web 2.0, most of the time nowadays it does not work. Why? Because the data is actually not contained in the HTML itself but put there by a script (usually Javascript) that is run by your browser when you go on the page itself. Meaning that if you try to get it from R, which is not a browser, you'll see squat. Luckily, enter headless browsers. What they are, are command-line tools that mimic browsers by executing this kind of scripts. Personally I use <a href="https://phantomjs.org/">phantomJS</a> because it is the easiest for me to use given i don't really know Javascript that much. So, once you installed phantomJS, all you need to do is write a little script that just reads in the page, processes it and outputs the result as a pure HTML page. Here, as an example, a script I wrote a year or two ago, for a colleague, that grabs data from the website of the IUCN red list:</p>

				<pre><code class="R"># The page you want to process
url &amp;lt;- "https://www.iucnredlist.org/species/1301/511335"
# Name of the intermediary file
out &amp;lt;- "species.html"
# A little bit of metaprogramming here:
j &amp;lt;- sprintf("var url ='%s';
var page = new WebPage() 
var fs = require('fs'); 

page.open(url, function (status) { 
  just_wait(); 
}); 

function just_wait() { 
  setTimeout(function() { 
    fs.write('%s', page.content, 'w'); 
    phantom.exit(); 
  }, 2500); 
}", url, out)
# Save that to a file
cat(j, file="scrape.js")
# Asks your shell to run phantomJS on that script
system("phantomjs scrape.js")
# Read in the resulting file
h &amp;lt;- htmlParse("species.html")
# and pick whatever info you wanted in the first place (here using xpath)
xpathSApply(h, "//div[@id='threats-details']//div[@class='text-body']", xmlValue)</code></pre>
			</div></description><category>Programming</category><category>R</category></item>
<item><title>A little bit of everything</title><link>http://plannapus.github.io/blog/2020-09-27.html</link><pubDate>2020-09-27</pubDate><description><div class="blog-content">
			Since a few months, I felt the need to create a blog. I have literally no idea why. Nor have I any idea of what I will put in it, though if I had to guess I would say a little bit of science — most probably the day-to-day somewhat boring details of it —, a little bit of programming — I have been contributing to <a href="https://stackoverflow.com/users/1451109">StackOverflow</a> for years, and I have been <a href="https://github.com/plannapus/MfN-Code-Clinic">helping my colleagues</a> a lot with their programming, but this way I might actually talk about the issues I want to talk about and not the ones I am asked about —, a little bit of cooking — as it is a significant part of my life and the one that is the easiest to talk about and the most likely to actually interest someone else than myself :) — and maybe a little bit about other subjects that interest me – literature, board games, mythology, ...<br/>
			So stay tuned for a little bit of everything I guess.<br/><br/><img class="current" src="img/eqpac.png" title="Assemblage of radiolarians" width="400"/><img class="current" src="img/bento.jpg" title="Not the most appetizing picture but still" width="400"/><img class="current" src="img/some_code.png" title="Example of code" width="400"/><p class="caption">A little bit of everything: actinommid radiolarians from site 846B; my first attempt at making Onigirazu from before the pandemic; some of my code (here Python).</p>
			</div></description></item>
</channel>
</rss>
